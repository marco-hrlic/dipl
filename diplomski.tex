% Predlozak za pisanje diplomskog rada na PMF-MO
% Opcenita uputstva za LaTeX se mogu npr. naci na 
% http://web.math.hr/nastava/rp3, http://web.math.hr/nastava/s4-prof/latex.pdf
% NE PREPORUCA se "Ne ba≈° tako kratak uvod u TEX", buduci se radi o vrlo starom prirucniku
% koji nije pogodan za moderne verzije LaTEXa.
% Originalna verzija "The not so short..." na http://tobi.oetiker.ch/lshort/lshort.pdf 
% je obnovljena i daje bolji uvid u moderne verzije LaTeXa

% Stil je optimiziran za kreiranje pdf dokumenta (npr. pomocu pdflatex-a, XeLaTeX-a)

\documentclass[a4paper,twoside,12pt]{memoir} % jednostrano: promijeniti twoside u oneside

% Paket inputenc omogucava direktno unosenje hrvatskih dijakritickih znakova 
% opcija utf8 za unicode (unix, linux, mac)
% opcija cp1250 za windowse
\usepackage[utf8]{inputenc}  % ukoliko se koristi XeLaTeX onda je \usepackage{xunicode}\usepackage{xltxtra}
\usepackage{mathrsfs} 
% Stil za diplomski, unutra je ukljucena podrska za hrvatski jezik
\usepackage{diplomski}
% bibliografija na hrvatskom
\usepackage[languagenames,fixlanguage,croatian]{babelbib} % zahtijeva datoteku croatian.bdf
% hiperlinkovi 
\usepackage[pdftex]{hyperref} 
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}
\usepackage{enumitem}
\usepackage{multicol}


%\newcommand{\vect}[1]{\boldsymbol{\mathrm{#1}}}
\newcommand{\vect}[1]{\mathbf{#1}}
\renewcommand{\vec}{\vect}
\newcommand{\card}{\text{\normalfont{card}}}
\newcommand{\supp}{\text{\normalfont{supp}}}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\norms}[1]{\left\lVert#1\right\rVert}
\newcommand{\rank}{\text\normalfont{r}}
%\newcommand{\argmin}{\text{\normalfont{arg}}\min}
%\newcommand{\argmax}{\text{\normalfont{arg}}\max}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\sgn}{\text{\normalfont{sgn}}}
\renewcommand{\Re}{\text{\normalfont{Re}}}
\renewcommand{\Im}{\text{\normalfont{Im}}}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\cone}{cone}
\DeclareMathOperator{\tr}{tr}

\newenvironment{alg}[1]
{
    \bigskip
    \begin{tcolorbox}[arc=0mm,boxrule=1.2pt,colframe=black,colback=white,detach title, before upper={\medskip\begin{center}\textbf{#1}\end{center}\hline\newline\medskip},frame hidden]
    \medskip
}
{
    \medskip
\end{tcolorbox}
    \bigskip
}


% Odabir familije fontova:
% koristenjem XeLaTeX-a mogu se koristiti svi fontovi instalirani na racunalu, npr
% \defaultfontfeatures{Mapping=tex-text}
% \setmainfont[Ligatures={Common}]{Hoefler Text}
% ili
% \newcommand{\nas}[1]{\fontspec{Adobe Garamond Pro}\fontsize{24pt}{24pt}\color{Chocolate}\selectfont #1}
% i onda \nas{Naslov ...}
%\usepackage{lmodern} % times new roman 
\usepackage[T1]{fontenc}
%\usepackage{newtxtext,newtxmath}
\usepackage{lmodern}
\usepackage{mathtools}

\usepackage{lineno}
%\linenumbers

% Paket graphicx sluzi za manipuliranje grafikom 
\usepackage[pdftex]{graphicx} % ukoliko se koristi XeLaTeX onda je \usepackage[xetex]{graphicx}
\usepackage[most]{tcolorbox}
% Paket amsmath je vec ukljucen
% Dodatno definirane matematicke okoline:
% teorem (okolina: thm), lema (okolina: lem), korolar (okolina: cor),
% propozicija (okolina: prop), definicija (okolina: defn), napomena (okolina: rem),
% slutnja (okolina: conj), primjer (okolina: exa), dokaz (okolina: proof)
% Definirane su naredbe za ispisivanje skupova N, Z, Q, R i C
% Definirane su naredbe za funkcije koje se u hrvatskoj notaciji oznacavaju drukcije 
% nego u americkoj: tg, ctg, ... (\tgh za tangens hiperbolni)
% Takodjer su definirane naredbe za Ker i Im (da bi se razlikovala od naredbe za imaginarni dio kompleksnog
% broja, naredba se zove \slika).

\pagestyle{headings}
% uz paket fancyhdr mogu se lako kreirati fancy zaglavlja i podnozja

% Podaci koje treba unijeti
\title{Sa\v{z}eto uzorkovanje}
\author{Marco Hrli\'c}
\advisor{Prof. dr. sc. Damir Baki\'c}  % obavezno s titulom (prof. dr. sc ili doc. dr. sc.)
\date{2019.}  % oblika mjesec, godina

% Moguce je unijeti i posvetu
% Ukoliko nema posvete, dovoljno je iskomentirati/izbrisati sljedeci redak 
\dedication{Albini}

\begin{document}
\setlength\abovedisplayskip{10pt}
\setlength\belowdisplayskip{10pt}
\setlength\abovedisplayshortskip{10pt}
\setlength\belowdisplayshortskip{10pt}

\nocite{*}

% Naredna frontmatter generira naslovnu stranicu, stranicu za potpise povjerenstva, eventualnu posvetu i sadrzaj
% Moze se iskomentirati ukoliko nije u pitanju konacna verzija
\frontmatter

% Tekst diplomskog ...

% Diplomski rad treba poceti s uvodnim poglavljem  
\begin{intro}
    Prikupljanje korisnih informacija iz mjerenja \v{c}esti je problem u primjeni. Ilustrativan primjer je proces uzorkovanja radio valova u kontekstu telekomunikacijskih tehnologija. Radio valove mo\v{z}emo shvatiti kao odre\dj ene promjene elektromagnetnog polja. Niz takvih fluktuacija koje nose neku informaciju zove se \textit{signal}. Signal putuje do antenskog sustava koji (uglavnom) periodi\v{c}ki uzima uzorke, tj. vr\v{s}i mjerenja nad elektromagnetnim poljem. Najjednostvniji matemati\v{c}ki model ovakvog procesa je linearni problem uzorkovanja. Neka $\vec x \in \C^N$ reprezentira informaciju, tj. signal koji nose radio valovi. Antenski sustav modeliramo kao matricu mjerenja $\vec A \in \C^{m \times N}$ a o\v{c}itana informacija neka je $\vec y \in \C^m$. Veza izme\dj u $\vec x$ i $\vec y$ dana je sa
    \begin{equation}\label{1.1}
        \vec{Ax} = \vec{y}. 
    \end{equation}
    Jasno, da bi rekonstruirali informaciju $\vec x$ potrebno je rije\v{s}iti sustav linearnih jednad\v{z}bi \eqref{1.1}. Nadalje, prirodno je zahtijevati da je $m \geq N$, tj. ako je signal $\vec x$ duljine $N$, onda je za o\v{c}ekivati da moramo uzeti barem $N$ uzoraka ako \v{z}elimo taj signal uspje\v{s}no rekonstruirati. Ako je $m < N$, klasi\v{c}na teorija linearne algebre nam ka\v{z}e da ako postoji barem jedno rje\v{s}enje tada postoji beskona\v{c}no mnogo rje\v{s}enja. O\v{c}ito da takav sustav nije od prakti\v{c}ne koristi. Nadalje, postoji jo\v{s} restrikcija na najmanji broj potrebnih mjerenja. To je takozvani Nyquist-Shannonov teorem (vidi \cite{shannon1949}) koji ka\v{z}e da frekvencija uzorkovanja vremenski neprekidnog signala mora biti dvostruko ve\'ca od najve\'ce frekvencije koju taj signal sadr\v{z}i ukoliko \v{z}elimo uspje\v{s}nu rekonstrukciju. \\
    \indent
    Uz odre\dj ene dodatne uvjete mogu\'ce je posti\'ci uspje\v{s}nu rekonstrukciju signala i u situaciji kada je broj mjerenja $m$ puno manji od $N$, \v{c}ak manji od broja mjerenja koji proizlazi iz Nyquist-Shannonovog teorema (vidi \cite{radar}). Nadalje, postoje i efikasni prakti\v{c}ni algoritmi za rekonstrukciju. Uvjet koji to omogu\'{c}uje je \textit{sa\v{z}etost} ili \textit{rijetkost}. Za signal ka\v{z}emo da je rijedak ako je ve\'cina njegovih komponenti nula. Takva je pretpostavka opravdana. Naime, empirijski znamo da su veliki broj signala u primjeni kompresibilni, a takve signale mo\v{z}emo dobro aprokimirati rijetkim signalima. Uzmimo za primjer slike pohranjene u JPEG formatu. Slika se prebaci u wavelet bazu u kojoj zadr\v{z}imo samo najve\'ce koeficijente dok ostale postavimo na nulu.\\
    \indent
    Intuitivno je jasno da nema potrebe za uzimanjem svih $N$ uzoraka ako je signal rijedak, tj. ve\'cina uzoraka \'ce biti nula. Problem le\v{z}i u \v{c}injenici da ne znamo koji su elementi signala nula a koji ne, te to uvodi nelinearnost u problem. To lako vidimo po\v{s}to skup $s$-rijetkih vektora (najvi\v{s}e $s$ ne-nul komponenti) ne formira linearan skup. Naime, zbroj dva $s$-rijetka vektora je generalno $2s$-rijedak vektor. Pokazati \'cemo da je metoda za rekonstrukciju koja se prirodno name\'ce NP-te\v{z}ak problem. \\
    \indent
    Dva osnovna pitanja sa\v{z}etog uzorkovanja na koja \'cemo poku\v{s}ati dati odgovoriti u ovom radu su:
    \begin{enumerate}
        \item Kako odabrati matricu mjerenja $\vec A \in \C^{m \times N}$? 
        \item Kako rekonstruirati vektor $\vec x$ iz mjerenja $ \vec y = \vec{Ax}$?
    \end{enumerate}
    Ispostavlja se da je konstrukcija eksplicitnih matrica mjerenja $\vec A$ zahtjevan problem, koji jo\v{s} uvijek nije razrije\v{s}en. Napredak je ostvaren upotrebom teorije slu\v{c}jnih matrica i teorije vjerojatnosti. Mi se ne\'cemo baviti takvim stohasti\v{c}kim konstrukcijama, ve\'c \'cemo istra\v{z}iti koja su to svojstva matrice mjerenja dovoljna za uspje\v{s}nu rekonstrukciju. Nadalje, prou\v{c}it \'cemo nekoliko najpopularnijih algoritama rekonstrukcije koji se koriste u sa\v{z}etom uzorkovanju. Zapo\v{c}et \'cemo s osnovnom teorijom rijetkih vektora, pokazati \'cemo NP-slo\v{z}enost prirodne rekostrukcijske metode, dati pregleda ostalih prakti\v{c}nih algoritama rekonstrukcije te pregled uvjeta koji garantiraju rekonstrukciju. Sadr\v{z}aj ovog rada prati knjigu \textit{A Mathematical Introduction to Compressive Sensing} \cite{foucart13} te je naglasak na matemati\v{c}koj teoriji. Za dobar pregled primjene sa\v{z}etog uzorkovanja vidi \cite{primjene} i \cite{tao}.

\end{intro}

\chapter[Rijetka rje\v{s}enja][Rijetka rje\v{s}enja]{Rijetka rje\v{s}enja}\label{chapter_rijetka_rijesenja}	
% ukoliko naslov nije jako dugacak dovoljno je samo \chapter{Naslov poglavlja} 

\section[Rijetkost i sa\v{z}etost vektora][Rijetkost i sa\v{z}etost vektora]{Rijetkost i sa\v{z}etost vektora}
%\subsection{Naslov podsekcije}
Uvedimo potrebnu notaciju. Neka je $[N]$ oznaka za skup $\{1,2,...,N\}$ gdje je $N\in\N$. Sa $\card(S)$ ozna\v{c}ujemo kardinalitet skupa $S$. Nadalje, $\bar{S}$ je komplement od $S$ u $[N]$, tj. $\bar{S}=[N]\backslash S$.

\begin{defn}
    Nosa\v{c} vektora $\vec{x} \in \C^{N}$ je skup indeksa njegovih ne-nul elemenata, tj.
    $$\supp(\vec{x}):=\{j\in[N]:x_j \neq 0 \}.$$
\end{defn}

\noindent Za vektor $\vec{x}\in\C^{N}$ ka\v{z}emo da je $s$-rijedak ako vrijedi $$\|\vec{x}\|_0 := \card(\supp(\vec{x})) \leq s$$
Primijetimo,
$$\|\vec{x}\|_p^p := \sum_{j=1}^N|x_j|^p \xrightarrow{p\rightarrow 0} \sum_{j=1}^N\mathbf{1}_{\{x_j \neq 0\}} = \card(\{j \in [N]:x_j \neq 0\}) = \|\vec{x}\|_0$$
Gdje smo koristili da je $\mathbf{1}_{\{x_j \neq 0\}} = 1$  ako je $x_j \neq 0$ te $\mathbf{1}_{\{x_j \neq 0\}} = 0$  ako je $x_j = 0$. Drugim rije\v{c}ima, $\|\vec{x}\|_0$ je limes $p$-te potencije $\ell_p$-kvazinorme vektora $\vec{x}$ kada $p$ te\v{z}i k nuli. Kvazinorma definira se jednako kao standardna $\ell_p$-norma, jedino \v{s}to nejednakost trokuta oslabimo, tj. zahtijevamo
$$\|\vec{x}+\vec{y}\|\leq C(\|\vec{x}\|+\|\vec{y}\|)$$ 
za neku konstantu $C \geq 1$.
Funkciju $\|\cdot\|_0$ \v{c}esto nazivamo $\ell_0$-norma vektora $\vec x$, iako  ona nije niti norma niti kvazinorma. U praksi, te\v{s}ko je tra\v{z}iti rijetkost vektora, pa je stoga prirodno zahtijevati slabiji uvjet \textit{kompresibilnosti}.  
\begin{defn}\label{greska_naj_s_aprox}
    $\ell_p$-gre\v{s}ku najbolje $s$-rijetke aproksimacije vektora $\vec{x}\in\C^{N}$ definiramo sa 
    $$\sigma_s(\vec{x})_p := \inf\big\{\|\vec{x}-\vec{z}\|_p,\ \vec{z} \in \C^{N} \ \text{je s-rijedak}\big\}$$
\end{defn}
\indent Primijetimo da se infimum posti\v{z}e za svaki $s$-rijedak vektor $\vec{z} \in \C^{N}$ koji ima ne-nul elemente koji su jednaki s $s$ najve\'cih komponenti vektora $\vec{x}$. Iako takav $\vec{z} \in \C^{N}$ nije jedinstven, on posti\v{z}e infimum za svaki $p > 0$. Neformalno, mogli bi re\'ci da je vektor $\vec{x} \in \C^{N}$ \textit{kompresibilan} ako gre\v{s}ka njegove najbolje $s$-rijetke aproksimacije brzo konvergira u $0$ kada $s$ te\v{z}i u $N$. Da bi to formalno iskazali, od koristi \'ce biti ocjena za $\sigma_s(\cdot)_p$. Po\v{s}to nam za to ne\'ce biti va\v{z}an poredak elemenata vektora $\vec{x}$, uvodimo sljede\'cu definiciju koja \'ce nam olak\v{s}ati ra\v{c}un.

\begin{defn}
    Nerastu\'ci poredak vektora $\vec{x} \in \C^{N}$ je vektor $\vec{x}^* \in \R^{\N}$ takav da je
    $$x^*_1 \geq x^*_2 \geq x^*_3 \geq \dots \geq 0$$
    te postoji permutacije $\pi : [N]\rightarrow[N]$ takva da $x^*_j=|x_{\pi(j)}|$ za sve $j\in [N]$.
\end{defn}
\begin{prop}\label{osnovna_ocjena_lp_greske}
    Za sve $q > p > 0$ i za svaki $\vec{x}\in \C^{N}$ vrijedi
    $$\sigma_s(\vec{x})_q \leq \frac{1}{s^{1/p - 1/q}}\|\vec{x}\|_p.$$
\end{prop}
\begin{proof}
    Neka je $\vec{x}^* \in \R^N$ nerastu\'ci poredak vektora $\vec{x}\in\C^N$. Tada slijedi
    \begin{equation*}
    \begin{split} 
        \sigma_s(\vec{x})_q^q &= \sum_{j=s+1}^{N}(x_j^*)^q=\sum_{j=s+1}^{N}(x_j^*)^p(x_j^*)^{q-p} \leq (x_s^*)^{q-p} \sum_{j=s+1}^{N}(x_j^*)^p \\ & \leq \bigg(\frac{1}{s}\sum_{j=1}^{s}(x_j^*)^p\bigg)^{\frac{q-p}{p}}\bigg( \sum_{j=s+1}^N(x_j^*)^p\bigg) \leq \bigg( \frac{1}{s} \|\vec{x}\|_p^p \bigg)^{\frac{q-p}{p}}\|\vec{x}\|_p^p \\ & = \frac{1}{s^{q/p-1}}\|\vec{x}\|_p^q
    \end{split}
    \end{equation*}
    Prva nejednakost slijedi iz \v{c}injenice da je $x_j^* \leq x_s^*$ za svaki $j \geq s+1$. Druga nejednakost je tako\dj er posljedica nerasta komponenti od $\vec{x}^*$. Potenciranjem obje strane s $1/q$ slijedi tvrdnja.
\end{proof}
Primijetimo da ako je $\vec{x}$ iz jedini\v{c}ne $\ell_p$-kugle za neki mali $p>0$, onda prethodna propozicija garantira kovergenciju od $\sigma_s(\vec{x})_q$ kada $s$ te\v{z}i u $N$, gdje $\ell_p$-kuglu definiramo kao
$$B_p^N := \big\{ \vec{z} \in \C^N : \|\vec{z}\|_p \leq 1\big\}$$
Vratimo se sada ocjeni iz propozicije \ref{osnovna_ocjena_lp_greske}. Sljede\'ci teorem daje najmanju konstantu $c_{p,q}$ takvu da vrijedi $\sigma_s(\vec{x})_q\leq c_{p,q}s^{-1/p+1/q}\|\vec{x}\|_p$ te zapravo predstavlja ja\v{c}u tvrdnju.
\begin{thm}\label{tm:2:5}
    Za svaki $q > p > 0$ i za svaki $\vec{x}\in\C^N$ vrijedi
    \begin{equation*}
    \sigma_s(\vec{x})_q \leq \frac{c_{p,q}}{s^{1/p - 1/q}}\|\vec{x}\|_p
    \end{equation*}
    gdje je
    $$
    c_{p,q} := \bigg[ \bigg(\frac{p}{q}\bigg)^{p/q}\bigg( 1-\frac{p}{q}^{1-p/q}\bigg) \bigg]^{1/p}\leq1.
    $$
\end{thm}
Istaknimo za \v{c}esti odabir $p=1$ i $q=2$
\begin{equation*}
    \sigma_s(\vec{x})_2 \leq \frac{1}{2\sqrt{s}}\|\vec{x}\|_1
\end{equation*}
\begin{proof}
    Neka je $\vec{x}^*$ nerastu\'ci poredak vektora $\vec{x}\in\C^N$ i $\alpha_j := (x_j^*)^p$. Dokazat \'cemo ekvivalentnu tvrdnju
    \begin{equation}\label{ocjena_ekv_tvrdnja}
    \begin{rcases}
{\alpha_1 \geq \alpha_2 \geq \cdots \geq \alpha_N \geq 0} \\
{\alpha_1 + \alpha_2 + \cdots + \alpha_N \leq 1} 
\end{rcases}\implies \alpha_{s+1}^{q/p} + \alpha_{s+2}^{q/p} + \cdots + \alpha_{s+N}^{q/p} \leq \frac{c^q_{q}}{s^{q/p-1}}.
    \end{equation}
    Stoga, za $r:=q/p>1$, problem se svodi na maksimizaciju konveksne funkcije
    $$
    f(\alpha_1, \alpha_2, \dots, \alpha_N) := \alpha_{s+1}^r + \alpha_{s+2}^r + \cdots +\alpha_{N}^r
    $$
    na konveksnom mnogokutu
    $$
    \mathcal{C} := \big\{ (\alpha_1, \cdots, \alpha_N)\in \R^N :  \alpha_1 \geq \alpha_2 \geq \cdots \geq \alpha_N \geq 0 \text{ i }  \alpha_1 + \alpha_2 + \cdots + \alpha_N \leq 1\big\}.
    $$
    Prema teoremu B.16 u \cite{foucart13} $f$ posti\v{z}e maksimum na nekom od vrhova mnogokuta $\mathcal{C}$, a vrhovi od $\mathcal{C}$ su dani kao sjeci\v{s}ta  $N$ hiperplohi koje dobijemo tako da u \eqref{ocjena_ekv_tvrdnja} $N$ nejednakosti pretvorimo u jednakosti. Mogu\'cnosti su:
    \begin{enumerate}
        \item $\alpha_1=\cdots=\alpha_N \ \implies\  f(\alpha_1, \alpha_2, \dots, \alpha_N) = 0$
        \item $\alpha_1+\cdots+\alpha_N=1$ i $\alpha_1=\cdots=\alpha_k>\alpha_{k+1}=\cdots=\alpha_N=0$ za neki \\ $1\leq k \leq s \  \implies \  f(\alpha_1, \alpha_2, \dots, \alpha_N) = 0$
        \item $\alpha_1+\cdots+\alpha_N=1$ i $\alpha_1=\cdots=\alpha_k>\alpha_{k+1}=\cdots=\alpha_N=0$ za neki\\ $s+1\leq k \leq N \  \implies \  \alpha_1=\cdots\alpha_k=1/k$ te $f(\alpha_1, \alpha_2, \dots, \alpha_N) = (k-s)/k^r$
    \end{enumerate}
    Dakle, slijedi da je
    $$
    \max\limits_{(\alpha_1,\dots,\alpha_N)\in\mathcal{C}} f(\alpha_1, \alpha_2, \dots, \alpha_N) = \max\limits_{s+1\leq k \leq N} \frac{k-s}{k^r}.
    $$
    Shvatimo sada $k$ kao realnu varijablu i zamijetimo da $g(k):=(k-s)/k^r$ raste do kriti\v{c}ne to\v{c}ke $k^*=(r/(r-1))s$ nakon koje opada.
    $$
    \max\limits_{(\alpha_1,\dots,\alpha_N)\in\mathcal{C}} f(\alpha_1, \alpha_2, \dots, \alpha_N) \leq g(k^*) = \frac{1}{r}\bigg( 1- \frac{1}{r}\bigg)^{r-1}\frac{1}{s^r-1}=c^q_{p,q}\frac{1}{s^{q/p}-1}.
    $$
\end{proof}
\indent Alternativni na\v{c}in na koji bi mogli definirati pojam \textit{kompresibilnosti} za vektor $\vec{x}\in\C^N$ je da zahtijevamo da je broj
$$\card(\{j\in[N]:|x_j|\geq t\})$$
tj. broj njegovih zna\v{c}ajnih ne-nul komponenti dovoljno mali. Ovaj pristup vodi na definiciju slabih $\ell_p$-prostora.
\begin{defn}
Za $p>0$, slabi $\ell_p$-prostor s oznakom $w\ell_p^N$ definiramo kao prostor $\C^N$ s kvazinormom
\begin{equation}\label{slaba_kvazinorma}
    \|\vec{x}\|_{p, \infty}:=\inf\bigg\{ M \geq 0: \card (\{j\in [N]: |x_j|\geq t \})\leq \frac{M^P}{t^p},\ \forall t>0    \bigg\}.
\end{equation}
\end{defn}
\noindent
Da bi pokazali da je \eqref{slaba_kvazinorma} zapravo kvazinorma, potreban nam je sljede\'ci rezultat.
\begin{prop}
    Neka su $\vec{x}^1,\dots\vec{x}^k\in\C^N$. Tada za svaki $p>0$ vrijedi 
    \begin{equation*}
    \|\vec{x}^1+\dots+\vec{x}^k\|_{p,\infty} \leq k^{\max\{1, 1/p\}}(\|\vec{x}^1\|_{p, \infty} + \cdots + \|\vec{x}^k\|_{p, \infty}).
    \end{equation*}
\end{prop}
\begin{proof}
    Neka je $t>0$. Ako je $|x_j^1+\cdots+x_j^k|\geq t$ za neki $j\in [N]$, tada imamo da je $|x_j^i|\geq t/k$ za neki $i \in [k]$. Dakle, vrijedi
    \begin{equation*}
        \big\{ j\in [N]:|x_j^1+\cdots+x_j^k| \geq t \big\} \subseteq \bigcup\limits_{i\in [k]} \big \{ j \in [N] : |x_j^i| \geq t/k \big \}
    \end{equation*}
    pa je stoga
    \begin{align*}
        \card\big( \big\{ j\in [N] : |x_j^1+\cdots+k_j^k| \geq t \big\}\big)&\leq\sum\limits_{i\in [k]}\frac{\|\vec{x}^i\|^p_{p, \infty}}{(t/k)^p} \\ 
                                                                            &= \frac{k^p(\|\vec{x}^1\|^p_{p, \infty}+\cdots + \|\vec{x}^k\|^p_{p, \infty})}{t^p}.
    \end{align*}
    Prema definiciji slabe $\ell_p$-kvazinorme \eqref{slaba_kvazinorma} vektora $\vec{x}^1+\cdots+\vec{x}^k$ dobivamo
    \begin{equation*}
        \|\vec{x}^1+\cdots+\vec{x}^k\|_{p, \infty}\leq k\big(\|\vec{x}^1\|^p_{p,\infty}+ \cdots +\|\vec{x}^k\|^p_{p,\infty}\big).
    \end{equation*}
    Ako je $p \leq 1$, uspore\dj uju\'ci $\ell_p$ i $\ell_1$ norme na $\R^k$ slijedi
    \begin{equation*}
        \big(\|\vec{x}^1\|^p_{p,\infty}+ \cdots +\|\vec{x}^k\|^p_{p,\infty}\big)^{1/p} \leq k^{1/p-1}\big(\|\vec{x}^1\|_{p,\infty}+ \cdots +\|\vec{x}^k\|_{p,\infty}\big)
    \end{equation*}
    te ako je $p \geq 1$ slijedi
    \begin{equation*}
        \big(\|\vec{x}^1\|^p_{p,\infty}+ \cdots +\|\vec{x}^k\|^p_{p,\infty}\big)^{1/p} \leq \|\vec{x}^1\|_{p,\infty}+ \cdots +\|\vec{x}^k\|_{p,\infty}.
    \end{equation*}
    Tvrdnja slijedi kombiniranjem dobivenih ocjena.
\end{proof}

\noindent
Poka\v{z}imo sada da je $\norm{\cdot}_{p, \infty}$ kvazinorma. Uzmimo $\vec{x}, \vec{y} \in \C^N$ i neka je $\lambda \in \C$ proizvoljan.

\begin{enumerate}
    \item Neka je $\|\vec{x}\|_{p, \infty}=0$. Iz \eqref{slaba_kvazinorma} slijedi $ \card(\{j \in [N]: |x_j| \geq t\}) = 0$ za svaki $t > 0$ pa je stoga broj ne-nul komponenti on $\vec{x}$ jednak nuli, tj. $\vec{x}=0$.
    \item Ako je $\lambda$ nula, $\|\lambda \vec{x}\| = | \lambda | \| \vec{x} \|$ vrijedi trivijalno. Za $\lambda \neq 0$, imamo \\
        $\card(\{ j \in [N]: |\alpha x_j| \geq t \}) = \card(\{ j \in [N]: |x_j| \geq t/|\alpha| \})\leq (\alpha M)^p/t^p$ za svaki $t>0$. Dakle, opet $\|\lambda \vec{x}\| = | \lambda | \| \vec{x} \|$.
    \item $\|\vec{x}+\vec{y}\|\leq C(\|\vec{x}\|+\|\vec{y}\|)$ je sada direktna posljedica prethodne propozicije.
\end{enumerate}

\noindent Sljede\'ca propozicija daje alternativni izraz za slabu $\ell_p$-kvazinormu.
\begin{prop}\label{slaba_kvazinorma_2}
    Za $p>0$, vrijedi
    \begin{equation*}
        \|\vec{x}\|_{p, \infty} = \max \limits_{k \in [N]}k^{1/p}x_k^{*}
    \end{equation*}
    gdje je $\vec{x}^* \in \R^N$ nerastu\'ci poredak vektora $\vec{x}\in \C^N$.
\end{prop}
\begin{proof}
    Primijetimo prvo da iz \eqref{slaba_kvazinorma} slijedi da je $\|\vec{x}\|_{p, \infty}=\|\vec{x}^*\|_{p, \infty}$, pa zapravo pokazujemo da je $\|\vec{x}\|:= \max_{k \in [N]}k^{1/p}x_k^* = \|\vec{x}^*\|$. Nadalje, za $t>0$ vrijedi da je $\{j \in [N]: x^*_j \geq t\}=[k]$ za neki $k \in [N]$ ili je $\{j \in [N]: x^*_j \geq t\}=\emptyset$. U prvom slu\v{c}aju $t \leq x^*_k \leq \|\vec{x}\|/k^{1/p}$ pa je $\card(\{j \in [N]:x_j^* \geq t \}) = k \leq \|\vec{x}\|/k^{1/p}$. U drugom slu\v{c}aju ista nejednakost vrijedi trivijalno. Iz definicije slabe $\ell_p$-kvazinorme \eqref{slaba_kvazinorma} sada dobivamo $\|\vec{x}^*\|_{p, \infty} \leq \|\vec{x}\|$. Pretpostavimo da je $\|\vec{x}^*\|_{p, \infty} < \|\vec{x}\|$. Tada postoji $\varepsilon > 0$ takav da je $(1+ \varepsilon)\|\vec{x}^*\|_{p, \infty} \leq \|\vec{x}\|$. Slijedi da je $(1 + \varepsilon)\|\vec{x}^*\| \leq  k^{1/p}x^*_k$ za neki $k \in [N]$ pa stoga
    \begin{equation*}
        [k] \subseteq \big\{ j \in [N] : (1 + \varepsilon)\|\vec{x}^*\|_{p, \infty}/k^{1/p} \leq x_j^* \big\}.
    \end{equation*}
    Ponovo iz \eqref{slaba_kvazinorma} imamo
    \begin{equation*}
        k \leq \frac{\|\vec{x}^*\|^p_{p, \infty}}{\big( (1 + \varepsilon)\|\vec{x}^*\|_{p, \infty}k^{1/p}\big)^p}=\frac{k}{(1 + \varepsilon)^p}.
    \end{equation*}
    Kontradikcija, dakle mora vrijediti $\|\vec{x}\| = \|\vec{x}^*\|_{p, \infty}$.
\end{proof}
\noindent Sada lagano mo\v{z}emo usporediti slabu i jaku $\ell_p$ normu,
\begin{prop}
    Za svaki $p > 0$ i za svaki $\vec{x} \in \C^N$,
    \begin{equation*}
        \|\vec{x}\|_{p, \infty} \leq \|\vec{x}\|_p.
    \end{equation*}
\end{prop}
\begin{proof}
    Neka je $k \in [N]$,
    \begin{equation*}
        \|\vec{x}\|_p^p = \sum_{j=1}^{N}(x_j^*)^p \geq \sum_{j=1}^{k}(x_j^*)^p \geq k(x_k^*)^p.
    \end{equation*}
    Tvrdnja slijedi potenciranjem na $1/p$, uzimaju\'ci maksimum po $k$ i primjenom prethodne propozicije.
\end{proof}
Koriste\'ci propoziciju \eqref{slaba_kvazinorma_2} mo\v{z}emo dobiti verziju ocjene iz propozicije \eqref{osnovna_ocjena_lp_greske} sa slabom $\ell_p$ normom.
\begin{prop}
    Za svaki $q>p>0$ i $\vec{x} \in \C^N$, vrijedi
    \begin{equation*}
        \sigma_s(\vec{x})_q \leq \frac{d_{p,q}}{s^{1/p-1/q}}\|\vec{x}\|_{p, \infty}
    \end{equation*}
    gdje je
    \begin{equation*}
        d_{p,q} := \big( \frac{p}{q-p} \big)^{1/q}.
    \end{equation*}
\end{prop}
\begin{proof}
    Bez smanjenja op\'cenitosti mo\v{z}emo pretpostaviti da je $\norm{\vec{x}}_{p,\infty} \leq 1$, pa je $x_k^* \leq 1/k^{1/p}$ za svaki $k \in [N]$. Tada vrijedi,
    \begin{equation*}
    \sigma_s(\vec{x})^q_q = \sum_{k=s+1}^{N} (x_k^*)^q \leq \sum_{k=s+1}^N \frac{1}{k^{q/p}} \leq \int_s^N \frac{1}{t^{q/p}} dt = - \frac{1}{q/p-1} \frac{1}{t^{q/p-1}}\bigg\rvert^{t=N}_{t=s} \leq \frac{p}{q-p} \frac{1}{s^{q/p-1}}.
    \end{equation*}
    Potenciranjem s $1/q$ slijedi tvrdnja.
\end{proof}
Prethodna propozicija daje da su vektori $\vec{x} \in \C^N$ koji su kompresibilni u smislu $\norm{\vec{x}}_{p, \infty} \leq 1$ za mali $p>0$, tako\dj er kompresibilni u smislu da gre\v{s}ka njihove najbolje $s$-rijetke aproksimacije brzo konvergira u 0 kada $s$ te\v{z}i u $N$. Iska\v{z}imo jo\v{s} jedan tehni\v{c}ki rezultat.
\begin{lem}
    Neka su $\vec{x}, \vec{y} \in \C^N$. Tada vrijedi,
    \begin{equation} \label{nerastuci_poredak_ocjena_1}
        \norm{\vec{x}^* - \vec{y}^*}_{\infty} \leq \norm{\vec{x} - \vec{y}}_{\infty}.
    \end{equation}
    Nadalje, za $s \in [N]$,
    \begin{equation}\label{nerastuci_poredak_ocjena_2}
        |\sigma_s(\vec{x})_1 - \sigma(\vec{y})_1| \leq \norm{\vec{x} - \vec{y}}_1
    \end{equation}
    i za $k>s$,
    \begin{equation}\label{nerastuci_poredak_ocjena_3}
        (k-s)x_k^* \leq \norm{\vec{x} - \vec{y}}_1 + \sigma_s(\vec{y})_1.
    \end{equation}
\end{lem}
\begin{proof}
    Za $j \in [N]$, skup indeksa $j$ najve\'cih komponenti vektora $\vec{x}$ ima ne-trivijalni presjek sa skupom od $N-j+1$ najmanjih komponenti vektora $\vec{y}$. Izaberimo indeks $l$ iz tog presjeka. Tada vrijedi, 
    \begin{equation*}
        x_j^* \leq |x_l| \leq |y_l| + \norm{\vec{x} - \vec{y}}_{\infty} \leq z_j^* + \norm{\vec{x} - \vec{y}}_{\infty}.
    \end{equation*}
    Zamjenom uloga od $\vec{x}$ i $\vec{y}$ slijedi \eqref{nerastuci_poredak_ocjena_1}.
    Neka je $\vec{v} \in \C^N$ najbolja $s$-rijetka aproksimacija vektora $\vec{y}$. Tada
    \begin{equation*}
        \sigma_s(\vec{x})_1 \leq \norm{\vec{x} - \vec{v}}_1 \leq \norm{\vec{x} - \vec{y}}_1 + \norm{\vec{y} - \vec{v}}_1 = \norm{\vec{x} - \vec{y}}_1 + \sigma_s(\vec{y})_1 .
    \end{equation*}
    Ponovno, zbog simetrije slijedi \eqref{nerastuci_poredak_ocjena_2}. Napokon, ocjena \eqref{nerastuci_poredak_ocjena_3} slijedi iz \eqref{nerastuci_poredak_ocjena_2} te iz \v{c}injenice
    \begin{equation*}
        (k-s)x_k^* \leq \sum_{j=s+1}^{k}x_j^* \leq \sum_{j \geq s+1} x_j^* = \sigma_s(\vec{x})_1.
    \end{equation*}
\end{proof}

\section[Minimalni broj mjerenja][Minimalni broj mjerenja]{Minimalni broj mjerenja}
Problem sa\v{z}etog uzorkovanja sastoji se od rekonstrukcije $s$-rijetkog vektora $\vec{x} \in \C^N$ iz sustava
\[\vec{y} = \vec{A}\vec{x}.\]
Matricu $\vec{A} \in \C^{m\times N}$ nazivamo \textit{matrica mjerenja}. Ako je $m < N$, za ovakav sustav linearnih jednad\v{z}bi ka\v{z}emo da je \textit{neodre\dj en}. Iako iz klasi\v{c}ne teorije linearne algebre ovakvi sustavi imaju beskona\v{c}no mnogo rije\v{s}enja, pokazat \'ce se da je dodatna pretpostavka rijetkosti vektora $x$ dovoljno za jedinstvenost rje\v{s}enja. U ovom poglavlju istra\v{z}it \'cemo koji je minimalni broj mjerenja, tj. $m$ broj redaka matrice $\vec{A}$, koji garantira rekonstrukciju $s$-rijetkog vektora $\vec{x}$. Zapravo, postoje dva pristupa ovom problemu. Mo\v{z}emo zahtijevati da problem mjerenja rekonstruira sve $s$-rijetke vektore $\vec{x} \in \C^N$ istodobno ili mo\v{z}emo tra\v{z}iti rekonstrukciju specifi\v{c}nog, tj. predodre\dj enog vektora $\vec{x} \in \C^N$. Taj pristup \v{c}ini se neprirodan, no pokazuje se da je on va\v{z}an u prou\v{c}avanju problema gdje matricu $\vec{A}$ biramo na slu\v{c}ajan na\v{c}in. \\ 
\indent Poka\v{z}imo da su za danu rijetkost $s$, matricu $\vec{A} \in \C^{m \times N}$ i $s$-rijedak vektor $\vec{x} \in \C^N$, naredne tvrdnje ekvivaltentne: 
\begin{enumerate}
    \item Vektor $\vec{x}$ je jedinstveno $s$-rijetko rje\v{s}enje sustava $\vec{A}\vec{z}=\vec{y}$ gdje je $\vec{y} = \vec{Ax}$, tj. $\{\vec{z} \in \C^N : \vec{A}\vec{z}= \vec{A}\vec{x},\ \norm{\vec{z}}_0 \leq s\} = \{\vec{x}\}$
    \item Vektor $\vec{x}$ je jedinstveno rje\v{s}enje problema minimizacije
        \begin{equation}\label{problem_minimizacije}
            \min\limits_{\vec{z} \in \C^N} \norm{\vec{z}}_0\quad \text{uz uvjet}\ \vec{Az} = \vec y \tag{$P_{0}$}
        \end{equation}
\end{enumerate}
Ako je $\vec{x} \in \C^N$ jedinstveno $s$-rijetko rje\v{s}enje od $\vec{Az} = \vec y$ takvo da je $\vec y = \vec{Ax}$, onda rje\v{s}enje $\vec x^{\sharp}$ od \eqref{problem_minimizacije} je $s$-rijetko i zadovoljava $\vec{Ax} = \vec y$ pa je $\vec x^\sharp = \vec x$. Drugi smjer slijedi trivijalno.


\subsection[Rekonstrukcija svih rijetkih vektora][Rekonstrukcija svih rijetkih vektora]{Rekonstrukcija svih rijetkih vektora}
Neka je $\vec{A} \in \C^{m \times N}$ i $S \subseteq [N]$, s $\vec A_S$ ozna\v{c}ujemo matricu formiranu od stupaca od $\vec A$ indeksiranih sa $S$. Sli\v{c}no, s $\vec x_S$ ozna\v{c}ujemo ili vektor iz $\C^{S}$ koji se sastoji od komponenti vektora $\vec x$ indeksiranih po $S$, tj. $(\vec x_S)_l = x_l$ za sve $l \in S$, ili vektor iz $\C^N$ koji se podudara s $\vec x$ na komponentama indeksiranim u $S$ i jednak je nula na indeksima koji nisu u $S$, tj. $(\vec x_S)_l = x_l$ za $l \in S$ i $(\vec x_S)_l =0$ za $ l \notin S$. Iz konteksta \'ce uvijek biti jasno na koju definiciju se misli.

\begin{thm} \label{rekonstrukcija_tm1}
    Neka je $\vec A \in \C^{m \times N}$. Ekvivalentno je:
    \begin{enumerate}[label=(\alph*)]
        \item Postoji samo jedan $s$-rijedak vektor $\vec x \in \C^N$ koji zadovoljava $\vec{Ax} = \vec{Az}$, tj. ako je $\vec{Ax}=\vec{Az}$ i ako su $\vec x$, $\vec z$ oba $s$-rijetki tada je $\vec x = \vec z$.
        \item Jezgra od $\vec A$ ne sadr\v{z}i niti jedan $2s$-rijedak vektor osim nul-vektora, tj. $\ker \vec A \cap \{\vec z \in \C^N: \norm{\vec z}_0 \leq 2s\} = \{\vec 0\}$.
        \item Za svaki $S \subseteq [N]$ takav da je $\card(S) \leq 2s$, podmatrica $\vec A_S$ je injektivna kao preslikavanje s $\C^S$ u $\C^m$.
        \item Svaki skup od $2s$ stupaca matrice $\vec A$ je linearno nezavisan skup.
    \end{enumerate}
\end{thm}
\begin{proof}
    \begin{itemize}
        \item[]$(b)\implies(a)$. Neka su $\vec x$ i $\vec z$ $s$-rijetki vektori takvi da $\vec{Ax} = \vec{Az}$. Tada je $\vec x - \vec z$ $2s$-rijedak i $\vec A(\vec x - \vec z) = \vec 0$. Po\v{s}to $\ker \vec A$ ne sadr\v{z}i $2s$-rijetke vektore osim nul-vektora, mora vrijediti $\vec x = \vec z$.
        \item[] $(a)\implies(b)$. Obratno, pretpostavimo da za svaki $s$-rijetki vektor $\vec x \in \C^N$ vrijedi $\{\vec z \in \C^N : \vec{Az} = \vec{Ax}, \norm{\vec z}_0 \leq s\} = \{\vec x\}$. Neka je $\vec v \in \ker \vec A$, $2s$-rijedak. Tada $\vec v$ mo\v{z}emo rastaviti kao $\vec v = \vec x - \vec z$ gdje su $\vec x$ i $\vec z$ $s$-rijetki takvi da $\supp(\vec x)\cap \supp(\vec z) = \emptyset$. Imamo da je $\vec{Ax}=\vec{Az}$ pa prema pretpostavci vrijedi $\vec{x}=\vec{z}$. Po\v{s}to su nosa\v{c}i od $\vec x$ i $\vec z$ disjunktni, mora vrijediti $\vec x = \vec z = \vec 0$ pa je stoga i $\vec v = 0$.
        \item[] $(b)\implies(c)$. Pretpostavimo suprotno, $\ker \vec A \cap \{\vec z \in \C^N: \norm{\vec z}_0 \leq 2s\} = \{\vec 0\}$ i postoji $S \in [N]$ takav da je $\card(S) \leq 2s$ te da $\vec A_s$ nije injektivna. To zna\v{c}i da postoji vektor $\vec x \in \C^{\card(S)} \backslash \{\vec 0\}$ takav da je $\vec A_S \vec x = \vec 0$. Definiramo vektor $\tilde{\vec{x}}\in \C^N$ s 
            \begin{equation*}
                \tilde{x}_j = 
                \begin{cases}
                    x_j \quad & \text{za}\ j \in S \\
                    0 \quad & \text{za}\  j \in \bar S \\
                \end{cases}
            \end{equation*}
            Dakle, imamo $\vec x \neq \vec 0$, $\norm{\vec x}_0 \leq 2s$ i vrijedi $\vec{Ax}=0$, tj. $\vec x \in \ker \vec A$. Kontradikcija s $(b)$.
        \item[]$(c)\implies(d)$. Odaberimo $2s$  stupaca od $\vec A$. Skup indeksa tih stupaca ozna\v{c}imo sa $S$. Prema $(c)$, matrica $\vec A_S$ je injektivna, a to zna\v{c}i da su njeni stupci linearno nezavisni, pa su stoga i $2s$ odabranih stupaca matrice $\vec A$ linearno nezavisni.
        \item[]$(d)\implies(b)$. Pretpostavimo da jezgra od $\vec A$ sadr\v{z}i $2s$-rijedak ne-nul vektor $\vec x \in \C^N$. Neka je $S$ skup indeksa ne-nul elemenata vektora $\vec x$. To zna\v{c}i da je $\vec A_S \vec x_S = 0$, i $\vec x_S \neq \vec 0$. Dakle $\vec A_S$ nije injektivna, pa stoga i skup stupaca od $\vec A$ indeksiranih sa $S$ nije linearno nezavisan, \v{s}to je kontradikcija s $(d)$.
\end{itemize}
\end{proof}

Uo\v{c}imo da ako je mogu\'ce rekonstruirati svaki $s$-rijedak vektor $\vec x \in \C^N$ iz vektora mjerenja $\vec y = \vec{Ax} \in \C^m$, tada vrijedi $(a)$. Prema pro\v{s}lom teoremu tada vrijedi i tvrdnja $(d)$ pa je stoga $\rank(\vec A) \geq 2s$. Tako\dj er vrijedi da je $\rank(\vec A) \leq m$ pa imamo 
\begin{equation*}
    m \geq 2s.    
\end{equation*}
To zna\v{c}i da je potrebno barem $2s$ mjerenja da bi rekonstruirali svaki $s$-rijedak vektor. Pokazati \'cemo da je, makar u teoriji, dovoljno to\v{c}no $2s$ mjerenja.

\begin{thm}
    Za svaki $N \geq 2s$, postoji matrica mjerenja $\vec A \in \C^{2s \times N}$ takva da se svaki $s$-rijedak vektor $\vec x \in \C^N$ mo\v{z}e rekonstruirati iz vektora mjerenja $\vec y = \vec{Ax} \in \C^m$ kao rje\v{s}enje problema minimizacije \eqref{problem_minimizacije}.
\end{thm}
\begin{proof}
    Fiksirajmo $t_N>\cdots t_2 > t_1 > 0$ i neka je $\vec A \in \C^{2s \times N}$ dana s
    \begin{equation}\label{vandermont_matrica}
        \vec A = 
        \begin{bmatrix}
            1 & 1 & \cdots & 1 \\ 
            t_1 & t_2 & \cdots & t_N \\
            \vdots & \vdots & \cdots & \vdots \\
            t_1^{2s-1} & t_2^{2s-1} & \cdots & t_N^{2s-1} \\
        \end{bmatrix}
    \end{equation}
    Nadalje, neka je $S=\{j_1 < \cdots < j_{2s}\}$ skup indeksa. Matrica $\vec A_S \in \C^{2s \times 2s}$ je transponirana \textit{Vandermontova matrica}. Lako se provjeri da
    \begin{equation*}
        \det(\vec{A}_S) = \prod_{k < l} (t_{j_l} - t_{j_k})>0.
    \end{equation*}
    To zna\v{c}i da je matrica $\vec A$ invertibilna, pa posebno i injektivna. Tada je zadovoljena tvrdnja $(c)$ teorema \ref{rekonstrukcija_tm1}, pa je po istom teoremu zadovoljena i tvrdnja $(a)$, tj. svaki $s$-rijedak vektor $\vec x \in \C^N$ zadovoljava $\vec{Az}=\vec{Ax}$. Stoga je taj vektor mogu\'ce jedinstveno rekonstruirati putem minimizacije \eqref{problem_minimizacije}.
\end{proof}
\indent Zapravo, mnogo matrica zadovoljava uvjet $(c)$ iz teorema \ref{rekonstrukcija_tm1}. Na primjer, potencije od $t_1,\dots,t_N$ u \eqref{vandermont_matrica} ne moraju biti uzastopne. Nadalje, brojevi $t_1,\dots,t_N$ ne moraju biti pozitivni, niti realni sve dok vrijedi $\det(\vec A_S) \neq 0$. Posebno, mo\v{z}emo uzeti $t_l = e^{2\pi i (l-1)/N}$ za $l \in [N]$ te argumentom iz pro\v{s}log teorema vidimo da parcijalna Fourierova matrica
\begin{equation*}
   \vec A = 
   \begin{bmatrix*}
       1 & 1 & 1 & \cdots & 1 \\
       1 & e^{2 \pi i/ N} & e^{2 \pi i2/ N} & \cdots & e^{2 \pi i(N-1)/ N} \\ 
       \vdots & \vdots & \vdots & \vdots & \vdots \\ 
       1 & e^{2 \pi i(2s-1)/ N} & e^{2 \pi i(2s-1)2/ N} & \cdots & e^{2 \pi i(2s-1)(N-1)/ N} \\ 
   \end{bmatrix*}
\end{equation*}
rekonstruira svaki $s$-rijedak vektor $\vec x \in \C^N$ iz $\vec y = \vec{Ax} \in \C^{2s}$.
Zapravo mo\v{z}e se pokazati da skup $(2s) \times N$ matrica takvih da $\det(\vec A_S) = 0$ za neki $S \subseteq [N]$ i $\card(S) \leq 2s$ ima Lebesgueovu mjeru nula, pa stoga gotovo sve $(2s) \times N$ matrice rekonstruiraju svaki $s$-rijedak vektor $\vec x \in \C^N$ iz $\vec y = \vec{Ax} \in \C^{2s}$. Me\dj utim u praksi nije isplativo rje\v{s}avati problem minimizacije \eqref{problem_minimizacije}, \v{s}to \'cemo kasnije i pokazati.




\subsection[Rekonstrukcija zadanog rijetkog vektora][Rekonstrukcija zadanog rijetkog vektora]{Rekonstrukcija zadanog rijetkog vektora}
Promatramo problem gdje je $s$-rijedak vektor $\vec x \in \C^N$ unaprijed zadan i poznat, a matricu $\vec A \in \C^{m \times N}$ \v{z}elimo odabrati tako da ona garantira rekonstrukciju vektora $\vec x$ iz mjerenja $\vec y = \vec{Ax} \in \C^m$. Isprva, ovakav pristup izgleda neprirodan zbog \v{c}injenice da je vektor $\vec x$ apriorno poznat. Ideja je da \'ce uvjeti rekonstrukcije vrijediti za gotovo sve $(s+1) \times N$ matrice, \v{s}to podupire \v{c}injenicu da se u praksi matrice mjerenja \v{c}esto odabiru na slu\v{c}ajan na\v{c}in.
\begin{thm}
Za svaki $N \geq s + 1$ i za dani $s$-rijedak vektor $\vec x \in \C^N$, postoji matrica mjerenja $\vec A \in \C^{(s+1) \times N}$, takva da se vektor $\vec x$ mo\v{z}e rekonstruirati iz mjerenja $\vec y = \vec{Ax} \in \C^m$ kao rje\v{s}enje minimizacije \eqref{problem_minimizacije}.
\end{thm}
\begin{proof}
    Neka je $\vec A \in \C^{(s+1) \times N}$ matrica za koju se $s$-rijedak vektor $\vec x$ ne mo\v{z}e rekonstruirati iz $\vec y = \vec{Ax}$ putem minimizacije \eqref{problem_minimizacije}. To zna\v{c}i da postoji vektor $\vec z \in \C^N$ razli\v{c}it od $\vec x$, takav da je $S=\supp(\vec z)=\{j_1, \dots, j_s\}$, $\card(S) \leq s$ (ako je $\norm{\vec z}_0 < s$, u $S$ dodamo proizvoljne elemente $j_l \in [N]$) i $\vec{Az}=\vec{Ax}$. Ako je $\supp(\vec x) \subseteq S$, tada iz $\big( \vec A (\vec z - \vec x)  \big)_{[s]}=0$ slijedi da $\vec A_{[s], S}$ nije invertibilna, tj.
    \begin{equation*}
        f(a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N}) := \det(\vec A_{[s], S}) = 0.
    \end{equation*}
    Ako $\supp(\vec x) \not\subseteq S$ tada je dimenzija prostora $V:=\{ \vec u \in \C^N: \supp(\vec u ) \subseteq S \} + \C \vec x$ jednaka $s+1$, i linearno preslikavanje $G:V \rightarrow \C^{s+1}$, $\vec v \mapsto \vec{Av}$ nije invertibilno, po\v{s}to je $G(\vec z - \vec x)=0$. Matrica linearnog preslikavanja $G$ u bazi $(\vec e_{j_1}, \dots, \vec e_{j_s}, \vec x)$ prostora V, je oblika
    \begin{equation*}
        B_{\vec x, S}:=
        \begin{bmatrix*}
            a_{1, j_1} & \cdots & a_{1,j_s} & \sum_{j \in \supp(\vec x)}x_j a_{1,j} \\
            \vdots & \ddots & \vdots & \vdots \\
            a_{s+1, j_1} & \cdots & a_{s+1,j_s} & \sum_{j \in \supp(\vec x)}x_j a_{s+1,j}
        \end{bmatrix*}
    \end{equation*}
    i imamo
    \begin{equation*}
        g_S(a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N}) := \det (B_{\vec x, S})=0.
    \end{equation*}
    Dakle, vrijedi
    \begin{equation*}
        (a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N}) \in f^{-1}(\{0\}) \cup \bigcup \limits_{\card(S)=s}g_S^{-1}(\{0\}).
    \end{equation*}
    Primijetimo da su skupovi $f^{-1}(\{0\})$ i $g^{-1}_S(\{0\})$ Lebesgueove mjere nula iz razloga \v{s}to su $f$ i $g_S$ polinomi u varijablama $(a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N})$. Dakle, elemente matrice $\vec A$ moramo izabrati izvan skupa mjere nula, da bi osigurali rekonstrukciju vekotora $\vec x$ iz $\vec y = \vec{Ax}$.
\end{proof}

\section[NP-slo\v{z}enost $\mathbf{\ell_0}$-minimizacije][NP-slo\v{z}enost $\ell_0$-minimizacije]{NP-slo\v{z}enost $\ell_0$-minimizacije}
Kao \v{s}to smo najavili, pokazati \'cemo da je u praksi neisplativo rje\v{s}avati problem $\ell_0$-minimizacije u svrhu rekonstrukcije vektora $\vec x$ iz mjerenja $\vec y = \vec{Ax}$. Prisjetimo se, problem koji rje\v{s}avamo je oblika, 
\begin{equation*}
    \min_{\vec z \in \C^N} \norm{\vec z}_0 \quad \text{uz uvjet } \vec{Az}=\vec y. 
\end{equation*}
Po\v{s}to je minimizator najvi\v{s}e $s$-rijedak, najjednostavniji algoritam za rje\v{s}avanje ovog problema je rje\v{s}iti sve pravokutne sustave $\vec A_S \vec u = \vec y$ ili sve kvadratne sustave oblika $\vec A_S^* \vec A_S \vec u = \vec A_S^* \vec y$ za svaki $\vec u \in \C^S$ gdje S ide po svim poskupovima od [N], veli\v{c}ine $s$. No ispada da je broj podskupova $N \choose s$, \v{s}to za male probleme s $N = 1000$ i $s=10$, iznosi ${1000 \choose 10} \geq (\frac{1000}{10})^{10}=10^{20}$. Kada bi jedan $10 \times 10$ sustav mogli rje\v{s}iti u $10^{-10}$ sekundi, trebalo bi nam vi\v{s}e od 300 godina da sve rje\v{s}imo. Sada \'cemo pokazati za\v{s}to je zapravo op\'cenitiji problem
\begin{equation}
\min_{\vec z \in \C^N}\ \norm{\vec z}_0 \quad \text{uz uvjet }\norm{\vec{Az}- \vec{y}}_2 \leq \eta \tag{$P_{0, \eta}$}\label{problem_minimizacije_generalni}
\end{equation}
NP-te\v{z}ak.\\
\indent Uvedimo prvo potrebne pojmove iz kompleksnosti algoritama. Za algoritam ka\v{z}emo da je \textit{polinomijalnog-vremena} ako je broj koraka do rje\v{s}enja ograni\v{c}en polinomom u varijabli veli\v{c}ine ulaza. Nadalje, uvedimo neformalne definicije klasa problema odlu\v{c}ivanja:
\begin{itemize}
    \item $\mathfrak{P}$: Svi problemi odlu\v{c}ivanja za koje postoji algoritam polinomijalnog vremena koji daje rje\v{s}enje.
    \item $\mathfrak{NP}$: Svi problemi odlu\v{c}ivanja za koje postoji algoritam polinomijalnog vremena koji provjerava to\v{c}nost rje\v{s}enja.
    \item $\mathfrak{NP}$-te\v{s}ki: Svi problemi (ne nu\v{z}no problemi odlu\v{c}ivanja) za koje se algoritam za rje\v{s}enje mo\v{z}e u polinomijalnom vremenu transformirati u algoritam rje\v{s}enja za bilo koji $\mathfrak{NP}$ problem.
    \item $\mathfrak{NP}$-potpuni: Svi problemi koji su istovremeno $\mathfrak{NP}$ i $\mathfrak{NP}$-te\v{s}ki.
\end{itemize}
Pitanje je li $\mathfrak{P}$ strogo sadr\v{z}ano u  $\mathfrak{NP}$ do dan danas nije odgovoreno. No, vjeruje se da postoje problemi za koje ne postoji algoritam rje\v{s}enja polinomijalnog vremena, ali postoji algoritam koji \'ce provjeriti to\v{c}nost rje\v{s}enja u polinomijalnom vremenu.
Najpoznatiji $\mathfrak{NP}$-potpun problem je problem putuju\'ceg prodava\v{c}a. No, iskoristiti \'cemo  problem egzaktnog pokriva\v{c}a tro\v{c}lanim skupovima da bi pokazali da je problem \eqref{problem_minimizacije_generalni} $\mathfrak{NP}$-te\v{z}ak.

\subsection[Egzaktni pokriva\v{c} tro\v{c}lanim skupovima][Egzaktni pokriva\v{c} tro\v{c}lanim skupovima]{Egzaktni pokriva\v{c} tro\v{c}lanim skupovima}
Za danu kolekciju $\{\mathcal{C}_i;\ i \in [N]\}$ tro\v{c}lanih podskupova od $[m]$, postoji li egzaktni pokriva\v{c} skupa $[m]$, tj. postoji li $J \subseteq [N]$ takav da je $\cup_{j \in J}\mathcal{C}_j=[m]$, gdje je $\mathcal{C}_j \cap \mathcal{C}_k = \emptyset$ za sve me\dj usobno razli\v{c}ite $j,k \in J$? Poznato je da je taj problem $\mathfrak{NP}$-potpun (vidi \cite{Karp72}, \cite{garey1979computers}).
\begin{thm}
    Za svaki $\eta \geq 0,\ \vec A \in \C^{m \times N}$ i $\vec y \in \C^m$, problem minimizacije \eqref{problem_minimizacije_generalni} je $\mathfrak{NP}$-potpun.
\end{thm}
\begin{proof}
    Zbog linearnosti problema \eqref{problem_minimizacije_generalni}, mo\v{z}emo uzeti da je $\eta < 1$. Pokazat \'cemo da se problem egzaktnog pokriva\v{c}a mo\v{z}e u polinomijalnom vremenu reducirati na problem $\ell_0$-minimizacije. Neka je $\{\mathcal{C}_i;\ i \in [N]\}$ kolekcija tro\v{c}anih podskupova $[m]$. Definirajmo vektora $\vec a_1, \vec a_2,\dots \vec a_N \in \C^m$
    \begin{equation*}
        (\vec a_i)_j = 
        \begin{cases*}
            1\ \text{za } j \in \mathcal{C}_i,\\
            0\ \text{za } j \not\in \mathcal{C}_i
        \end{cases*}
    \end{equation*}
    Definiramo matricu $\vec A \in \C^{m \times N}$ i vektor $\vec y \in \C^m$ s
    \begin{equation*}
        \vec A = [\vec a_1\ \vec a_2\ \cdots \ \vec a_N], \qquad \vec y = [1,1, \dots, 1]^T.
    \end{equation*}
    Po\v{s}to je $N \leq {m \choose 3}$, to mo\v{z}emo napraviti u polinomijalnom vremenu. Ako $\vec z \in \C^N$ zadovoljava $\norm{\vec{Az}-\vec y}_2 \leq \eta$, tada su svih $m$ komponenti od $\vec{Az}$ udaljeljene od $1$ za najvi\v{s}e $\eta$, pa su te komponente razli\v{c}ite od nula, jer smo $\eta$ uzeli manji od $1$. Dakle, vrijedi $\norm{\vec{Az}}_0 = m$. Ali po\v{s}to svaki od vektora $\vec a_i$ ima to\v{c}no tri ne-nul komponente, vektor $\vec{Az}=\sum_{j=1}^N z_j \vec a_j$ ima najvi\v{s}e $3 \norm{\vec z}_0$ ne-nul elemenata, tj. $\norm{\vec{Az}}_0 \leq 3 \norm{\vec{z}}_0$. Dakle, za svaki vektor $\vec z \in \C^N$ koji zadovoljava $\norm{\vec{Az}-\vec y}_2 \leq \eta$ vrijedi $\norm{\vec z}_0 \geq m/3$. Neka je sada $\vec x \in \C^N$ rje\v{s}enje $\ell_0$-minimizacije \eqref{problem_minimizacije_generalni}. Imamo dva slu\v{c}aja za normu vektora $\vec x$:
    \begin{enumerate}
        \item Ako je $\norm{\vec{x}}_0 = m/3$ tada je $\{\mathcal{C}_j;\ j \in \supp(\vec x)\}$ egzaktni pokriva\v{c} skupa $[m]$ jer ina\v{c}e bi neke od $m$ komponenti od $\vec{Ax}$ bile jednake nuli.
        \item Ako je $\norm{\vec{x}}_0 > m/3$ tada ne mo\v{z}e postojati egzaktni pokriva\v{c} $\{\mathcal{C}_j;\ j \in J\}$ jer bi u suprotnom vektor $\vec z \in \C^N$ definiran tako da je $z_j = 1$ ako je $j \in J$ i $z_j = 0$ ako je $j \not \in J$, zadovoljavao $\vec{Az}=\vec y$ i $\norm{\vec z}_0=m/3$, \v{s}to je kontradikcija s minimalnosti vektora $\vec x$.
    \end{enumerate}
    Dakle, rje\v{s}avanjem problem $\ell_0$-minimizacije, mo\v{z}emo rje\v{s}iti problem egzaktnog pokriva\v{c}a tro\v{c}lanim skupovima, pa je stoga i sam problem $\ell_0$-minimizacije $\mathfrak{NP}$-potpun.
\end{proof}
\v{C}ini se da prethodni teorem predstavlja ozbiljnu zapreku u prakti\v{c}nom rje\v{s}avanju problema sa\v{z}etog uzorkovanja. No primijetimo, teorem tvrdi da je algoritam koji rje\v{s}ava problem $\ell_0$-minimizacije, za sve mogu\'ce matrie $\vec A$ i vektore $\vec y$ barem klase $\mathfrak{NP}$. Naravno, u samoj praksi nije nu\v{z}no zahtijevati rekonstrukciju za sve takve matrice i vektore. Naime, pokazat \'cemo da postoje algoritmi koji uspje\v{s}no rekonstruiraju $\vec x$ iz $\vec y$ za posebno dizajnirane matrice $\vec A$.




\chapter[Osnovni algoritmi sa\v{z}etog uzorkovanja][Osnovni algoritmi sa\v{z}etog uzorkovanja]{Osnovni algoritmi sa\v{z}etog uzorkovanja}\label{chapter_algoritmi}
Algoritmi za rje\v{s}avanje problema sa\v{z}etog uzorkovanja, koje \'cemo predstaviti, podijeljeni su u tri kategorije: optimizacije, greedy metode i grani\v{c}ne metode. U ovom poglavlju dati \'cemo samo pregled najpopularnijih algoritama, dok \'cemo formalnu analizu nekih od njih ostaviti za kasnije, nakon \v{s}to razvijemo potrebne teorijske alate.
\section[Optimizacijske metode][Optimizacijske metode]{Optimizacijske metode}
Op\'ceniti problem optimizacije je oblika
\begin{equation*}
    \min_{\vec x \in \R^N} F_0(\vec x)\quad\text{uz uvjet }F_i(\vec x) \leq b_i,\ i \in [n]
\end{equation*}
gdje $F_0:\R^N \rightarrow \R$ zovemo \textit{funkcija cilja}, a funkcije $F_1, \dots, F_n: \R^N \rightarrow \R$ zovu se \textit{funkcije ograni\v{c}enja}. Ako su $F_0, F_1, \dots, F_n$ konveksne funkcije, tada ovaj problem zovemo \textit{problem konveksne optimizacije}. Ako su te funkcije linearne, tada je to \textit{problem linearnog programiranja} (vidi \cite{chong2013introduction}). Primijetimo da je problem rekonstrukcije rijetkog vektora \eqref{problem_minimizacije}, zapravo problem minimizacije. No, na\v{z}alost taj problem nije konveksan i kao \v{s}to smo u prethodnom poglavlju pokazali, op\'cenito je $\mathfrak{NP}$-te\v{z}ak. Prisjetimo se da $\norm{\vec z}_q^q$ konvergira k $\norm{\vec z}_0$ za $q \rightarrow 0^+$, pa je prirodno  \eqref{problem_minimizacije} aproksimirati problemom
\begin{equation}
    \min \norm{\vec z}_q\quad\text{uz uvjet }\vec{Az}=\vec y\tag{$P_{q}$}\label{problem_minimizacije_aprox}.
\end{equation}
Poka\v{z}e se da za $q > 1$, \v{c}ak $1$-rijetki vektori nisu rje\v{s}enja od \eqref{problem_minimizacije_aprox}. Dok za $0 < q < 1$, \eqref{problem_minimizacije_aprox} ponovno nije konveksan i dalje je op\'cenito $\mathfrak{NP}$-te\v{z}ak. Za $q=1$, problem postaje konveksan
\begin{equation}
    \min \norm{\vec z}_1 \quad \text{uz uvjet }\vec{Az}=\vec y.\tag{$P_{1}$}\label{problem_minimizacije_l1}
\end{equation}
To je zapravo konveksna relaksacija problema \eqref{problem_minimizacije} i zovemo ga $\ell_1$\textit{-minimizacija} ili \textit{BP} algoritam (eng. \textit{basis pursuit}).

\begin{alg}{$\ell_1$-minimizacija (BP)}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$. \\
    \textit{Problem:}
        \begin{equation}
            \vec x^{\sharp} = \argmin \norm{\vec z}_1 \quad \text{uz uvjet }\vec{Az}=\vec y\tag{$\ell_1-min$}\label{algoritam_l1_minimizacija}
        \end{equation} \\
        \textit{Izlaz:} vektor $\vec x^{\sharp}$
\end{alg}

\noindent Poka\v{z}imo sada da su $\ell_1$-minimizatori rijetki vektori u realnom slu\v{c}aju.
\begin{thm}
    Neka je $\vec A \in \R^{m \times N}$ matrica mjerenja sa stupcima $\vec a_1, \dots, \vec a_N$. Ako je $\vec x^{\sharp}$ minimizator od
    \begin{equation*}
        \min_{\vec z \in \R^N} \norm{\vec z}_1\quad \text{uz uvjet } \vec{Az}=\vec y,
    \end{equation*}
    tada je skup $\{\vec a_j,\ j \in \supp(\vec x^{\sharp})\}$ linearno nezavisan i vrijedi
    \begin{equation*}
        \norm{\vec{x}^{\sharp}}_0 = \card(\supp(\vec x^{\sharp})) \leq m. 
    \end{equation*}
\end{thm}
\begin{proof}
    Pretpostavimo suprotno, tj. da je skup $\{\vec a_j,\ j \in \supp(\vec x^{\sharp})\}$ linearno zavisan. Neka je $S= \supp(\vec x^{\sharp})$. To zna\v{c}i da postoji ne-nul vektor $\vec v \in \R^N$ s nosa\v{c}em na $S$ takav da je $\vec{Av} = \vec 0$. Tada za svaki $t \not= 0$ imamo
    \begin{equation*}
        \norm{\vec x^{\sharp}}_1 < \norm{\vec x^{\sharp} + t \vec v}_1 = \sum_{j \in S}|x_j^{\sharp} + tv_j| = \sum_{j \in S}\sgn(x_j^{\sharp}+tv_j)(x_j^{\sharp}+tv_j).
    \end{equation*}
    Ako je $|t|$ dovoljno mali, tj. $|t| < \min_{j \in S}|x_j^{\sharp}|/ \norm{\vec v}_{\infty}$, onda vrijedi
    \begin{equation*}
        \sgn(x_j^{\sharp}+tv_j) = \sgn(x_j^{\sharp})\quad \text{za svaki }j \in S.
    \end{equation*}
    Dakle, za $0<|t|<\min_{j \in S}|x_j^{\sharp}|/ \norm{\vec v}_{\infty}$ slijedi
    \begin{equation*}
    \begin{split}
        \norm{\vec x^{\sharp}}_1 & <  \sum_{j \in S} \sgn(x_j^{\sharp})(x_j^{\sharp}+tv_j)=\sum_{j \in S} \sgn(x_j^{\sharp})(x_j^{\sharp})+t \sum_{j \in S}\sgn(x_j^{\sharp})v_j \\ &= \norm{\vec x^{\sharp}}_1 + t \sum_{j \in S}\sgn(x_j^{\sharp})v_j.
    \end{split}
    \end{equation*}
    No, to je kontradikcija jer $t \not = 0$ mo\v{z}emo odabrati dovoljno mali tako da je \\ $t \sum_{j \in S}\sgn(x_j^{\sharp})v_j \leq 0$.
\end{proof}

\indent
U realnom slu\v{c}aju, \eqref{problem_minimizacije_l1} mo\v{z}emo reinterpretirati kao problem linearnog programiranja, tako da uvedemo pomo\'cne varijable $\vec z^+,\ \vec z^- \in \R^N$ definirane sa
\begin{multicols}{2}
    \noindent
    \begin{equation*} 
        z_j^+ = 
        \begin{cases}
            z_j\ & \text{za } z_j > 0, \\
            0\ & \text{za } z_j \leq 0
        \end{cases}
    \end{equation*}
    \begin{equation*} 
        z_j^- = 
        \begin{cases}
            0\ & \text{za } z_j > 0, \\
            -z_j\ & \text{za } z_j \leq 0
        \end{cases}
    \end{equation*}
\end{multicols}
\noindent
za svaki $j \in [N]$. Tada je problem \eqref{problem_minimizacije_l1} ekvivalentan problemu
\begin{equation}
    \min_{\vec z^+,\vec z^- \\ \in \R^N} \sum_{j=1}^{N}(z_j^+ + z_j^-) \quad \text{uz uvjet }
    \begin{bmatrix*}
        \vec A & -\vec A
    \end{bmatrix*}
    \begin{bmatrix*}
        \vec z^+ \\ \vec z^-
    \end{bmatrix*}
    = \vec y, \quad
    \begin{bmatrix*}
        \vec z^+ \\ \vec z^-
    \end{bmatrix*}
    \geq 0.\tag{$P_1'$}
\end{equation}
Isto ne vrijedi za kompleksan slu\v{c}aj. Tu \v{c}injenicu pokazat \'cemo na op\'{c}enitijem problemu,
\begin{equation}\label{problem_minimizacije_l1_kvadraticni}
    \min \norm{\vec z}_1 \quad \text{uz uvjet } \norm{\vec{Az}-y}_2 \leq \eta.\tag{$P_{1,\eta}$}
\end{equation}
Taj problem je zapravo pogodniji za praksu, po\v{s}to vektor $\vec y \in \C^m$ ne mo\v{z}emo izmjeriti s beskona\v{c}nom to\v{c}no\v{s}\'cu, ve\'c uz neku gre\v{s}ku $\vec e \in \C^m$ pa je stoga
\begin{equation*}
    \vec y = \vec{Ax} + \vec e. 
\end{equation*}
U praksi takvoj gre\v{s}ci \v{c}esto mo\v{z}emo ocijeniti $\ell_2$-normu
\begin{equation*}
    \norm{\vec e}_2 \leq \eta, \quad \text{za neki } \eta > 0.
\end{equation*}
Za dani vektor $\vec z \in \C^N$, neka su $\vec u,\ \vec v \in \R^N$ njegovi realni i imaginarni dijelovi te neka je $\vec c \in \R^N$ takav d je $c_j \geq |z_j| = \sqrt{u_j^2+v_j^2}$ za sve $j \in [N]$. Problem \eqref{problem_minimizacije_l1_kvadraticni} je tada ekvivalentan problemu
\begin{equation}
\begin{split}
    \min_{\vec c, \vec u, \vec v \in \R^N}\sum_{j=1}^N c_j\quad \text{uz uvjete }& 
    \norms{
        \begin{bmatrix*}
            \Re(\vec A) & -\Im(\vec A) \\
            \Im(\vec A) & \Re(\vec A)
        \end{bmatrix*}
        \begin{bmatrix*}
           \vec u \\ \vec v 
        \end{bmatrix*}
        -
        \begin{bmatrix*}
            \Re(\vec y) \\ \Im(\vec y) 
    \end{bmatrix*} }_2 \leq \eta \\
    &\sqrt{u_j^2 + v_j^2} \leq c_j,\quad \forall j \in [N].
\end{split}\tag{$P_{1, \eta}'$}\label{problem_minimizacije_l1_kvadraticni_drugi_oblik}
\end{equation}
Ovo je \textit{problem konike drugog reda}. Primijetimo da za $\eta=0$ dobivamo formulaciju problema \eqref{problem_minimizacije_l1} za kompleksni slu\v{c}aj u takvom obliku.
\\\indent Princip rje\v{s}avanja \eqref{problem_minimizacije_l1_kvadraticni} zove se \textit{kvadrati\v{c}no ograni\v{c}ena $\ell_1$-minimizacija} ili \textit{$\ell$-minimizacija osjetljiva na \v{s}um} (eng. \textit{quadratically constrainted basis pursuit}).
\begin{alg}{Kvadrati\v{c}no ograni\v{c}ena $\ell_1$-minimizacija}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, razina \v{s}uma $\eta$. \\
    \textit{Problem:}
        \begin{equation}
            \vec x^{\sharp} = \argmin \norm{\vec z}_1 \quad \text{uz uvjet }\norm{\vec{Az}-y}_2 \leq \eta\tag{$\ell_1-min_{\eta}$}\label{algoritam_l1_minimizacija_kvadraticna}
        \end{equation} \\
        \textit{Izlaz:} vektor $\vec x^{\sharp}$
\end{alg}
Rje\v{s}enje $\vec x^{\sharp}$ povezano je s rje\v{s}enjem problema $\ell_1$-minimizacije s ugra\dj enim uklanjanjem \v{s}uma
\begin{equation}\label{problem_minimizacije_l1_sum}
    \min_{\vec z \in \C^N} \lambda\norm{\vec z}_1 + \norm{\vec{Az}-\vec y}_2^2
\end{equation}
za neki $\lambda \geq 0$. Tako\dj er povezano je s rje\v{s}enjem \textit{LASSO} problema (vidi \cite{tibshirani96regression}), za neki $\tau \geq 0$,
\begin{equation}\label{lasso}
    \min_{\vec z \in \C^N} \norm{\vec{Az}-\vec y}_2\quad \text{uz uvjet } \norm{\vec z}_1\leq \tau
\end{equation}
To upravo tvrdi naredna propozicija.
\begin{prop}
    \begin{enumerate}[label=(\alph*)]
        \item Ako je $\vec x$ minimizator problema \eqref{problem_minimizacije_l1_sum} s $\lambda > 0$, onda postoji $\eta = \eta_{\vec x} \geq 0$ takav da je $\vec x$ minizator kvadrati\v{c}no ograni\v{c}ene $\ell_1$-minimizacije \eqref{problem_minimizacije_l1_kvadraticni}.
        \item Ako je $\vec x$ jedinstveni minimizator problema \eqref{problem_minimizacije_l1_kvadraticni} s $\eta \geq 0$, onda postoji $\tau = \tau_{\vec x} \geq 0$ takav da je $\vec x$ minimizator LASSO problema \eqref{lasso}.
        \item Ako je $\vec x$ minimizator LASSO problema \eqref{lasso}, onda postoji $\lambda = \lambda_{\vec x} \geq  0$ takav da je $\vec x$ minimizator problema \eqref{problem_minimizacije_l1_sum}.
    \end{enumerate}
\end{prop}
\begin{proof}
    \begin{enumerate}[label=(\alph*)]
        \item Neka je $\eta := \norm{\vec{Ax}- \vec y}_2$ i $\vec z \in \C^N$ takav da je $\norm{\vec{Az-y}}_2 \leq \eta$. Po\v{s}to je prema pretpostavci $\vec x$ minimizator od \eqref{problem_minimizacije_l1_sum} slijedi
            \begin{equation*}
                \lambda \norm{\vec x}_1 + \norm{\vec{Ax} -\vec{y}}_2^2 \leq \lambda \norm{\vec z}_1 + \norm{\vec{Az} - \vec{y}}_2^2 \leq \lambda \norm{\vec z}_1 + \norm{\vec{Ax}-\vec y}_2^2.
            \end{equation*}
            Dakle slijedi da je $\norm{\vec x}_1 \leq \norm{\vec{y}}_1$, pa je $\vec x$ minimizator problema \eqref{problem_minimizacije_l1_kvadraticni}

        \item Neka je $\eta := \norm{\vec x}_1$ i neka je $\vec z \in \C^N\backslash\{\vec x\}$ takav da je $\norm{\vec z}_1 \leq \tau$. Po\v{s}to je $\vec x$ jedinstveni minimizator od \eqref{problem_minimizacije_l1_kvadraticni} to zna\v{c}i da $\vec z$ ne mo\v{z}e zadovoljavati uvjet iz \eqref{problem_minimizacije_l1_kvadraticni}, pa stoga $\norm{\vec{Az}- \vec{y}}_2 > \eta \geq \norm{\vec{Ax}-\vec y}_2$. Dakle, $\vec x$ je jedinstveni minimizator \textit{LASSO} problema.
        \item Za dokaz ove tvrdnje potrebni su alati konveksne analize, vidi teorem B.24 u \cite{foucart13}.
    \end{enumerate}
\end{proof}

\section[Greedy metode][Greedy metode]{Greedy metode}
Upoznat \'cemo se s dva iterativna greedy algoritma koji se \v{c}esto koriste u kontekstu sa\v{z}etog uzorkovanja. Prvo algoritam koji \'cemo prou\v{c}iti zove se \textit{OMP} (skra\'cenica od eng. \textit{orthogonal matching pursuit}).
\begin{alg}{OMP}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$. \\
    \textit{Inicijalizacija:} $S^0 = \emptyset$, $\vec x^0 = \vec 0$ \\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{align}
            &S^{n+1} = S^n \cup \{j_{n+1}\},\quad j_{n+1} := \argmax\limits_{j \in [N]}\{|(\vec A^*(\vec y - \vec{Ax}^n))_j|\},\tag{$OMP_1$}\label{omp_1}
        \\
            &\vec x^{n+1} = \argmin\limits_{\vec z \in \C^N}\{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subseteq S^{n+1}\}.\tag{$OMP_2$}\label{omp_2}
        \end{align} \\
        \textit{Izlaz:} $\bar{n}$-rijedak vektor $\vec x^{\sharp}=\vec{x}^{\bar{n}}$.
\end{alg}

Numeri\v{c}ki najskuplja operacija ovog algoritma je \eqref{omp_2}. Situacija se mo\v{z}e popraviti kori\v{s}tenjem $QR$ dekompozicije matrice $\vec{A}_{S_n}$. Tada se mogu iskoristiti efikasni algoritmi za a\v{z}uriranje $QR$ dekompozicije kada se u matricu doda novi stupac. Nadalje, za dodatna ubrzanja mogu se iskoristiti i algoritmi za brzo matrica-vektor mno\v{z}enje bazirani na brzoj Fourierovoj transformaciji.
\newline\indent
Indeks $j_{n+1}$ bira se tako da se reducira $\ell_2$-norma reziduala $\vec{y} - \vec{Ax}^n$ \v{s}to je vi\v{s}e mogu\'ce. Sljede\'ca lema opravdava za\v{s}to je smisleno $j$ odabrati takav da maksimizira vrijednost $|{(\vec{A}^*(\vec{y}-\vec A \vec x^n))_j}|$.
\begin{lem}\label{lem:3:3}
    Neka je $\vec A \in \C^{m \times N}$ s $\ell_2$-normaliziranim stupcima. Ako su $S \subseteq [N]$, $\vec v \in \C^N$ s nosa\v{c}em na $S$, $j \in [N]$, te ako vrijedi
    \begin{equation*}
        \vec w := \argmin_{\vec z \in \C^N} \{ \norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subseteq S \cup \{j\} \},
    \end{equation*}
    tada
    \begin{equation*}
        \norm{\vec y - \vec{Aw}}_2^2 \leq \norm{\vec y - \vec{Av}}_2^2 - |{(\vec{A}^*(\vec y - \vec{Av}))_j|^2.
    \end{equation*}
\end{lem}
\begin{proof}
    Po\v{s}to svaki vektor oblika $\vec v + t \vec e_j,\ t \in \C$, ima nosa\v{c} u $S \cup \{j\}$ vrijedi,
    \begin{equation*}
        \norm{\vec y - \vec{Aw}}_2^2 \leq \min_{t \in \C} \norm{\vec y - \vec{A}(\vec v + t \vec e_j)}_2^2.
    \end{equation*}
    Stavimo da je $t = \rho e^{i \theta}$, gdje je $\rho \geq 0$ i $\theta \in [0,2 \pi)$. Imamo,
    \begin{align*}
        \norm{\vec y - \vec{A}(\vec v + t \vec e_j)}_2^2 &= \norm{\vec y - \vec{Av} - t \vec{A}\vec{e}_j}_2^2\\
        &= \norm{\vec y - \vec{Av}}_2^2 + |t|^2 \norm{\vec{Ae}_j}_2^2 - 2 \Re(\bar{t}\langle \vec{y} - \vec{Av}, \vec{Ae}_j \rangle)\\
        &= \norm{\vec y - \vec{Av}}_2^2 + \rho^2 - 2 \Re(\rho e^{-i \theta}(\vec{A}^*(\vec y - \vec{Av}))_j)\\
        & \geq \norm{\vec y - \vec{Av}}_2^2 + \rho^2 - 2 \rho |{(\vec{A}^*(\vec y - \vec{Av}))_j}|^2
    \end{align*}
    gdje jednakost vrijedi za pogodno odabrani $\theta$. Kao kvadratni polinom u varijabli $\rho$, zadnji izraz poprima minimum za $\rho = |{(\vec{A}^*(\vec y - \vec{Av}))_j}|$.
\end{proof}

Korak \eqref{omp_2} mo\v{z}e se prikazati u obliku
\begin{equation*}
    \vec{x}_{S^{n+1}}^{n+1} = \vec{A}_{S^{n+1}}^{\dagger}\vec y,
\end{equation*}
gdje je $\vec{x}_{S^{n+1}}^{n+1}$ restrikcija od $\vec x^{n+1}$ na svoj nosa\v{c} $S^{n+1}$ i gdje je $\vec A^{\dagger}_{S^{n+1}}$ pseudo-inverz od $\vec{A}_{S^{n+1}}$ (vidi \cite{penrose_1955}). Drugim rje\v{c}ima to zna\v{c}i da je $\vec z = \vec x ^{n+1}_{S^{n+1}}$ rje\v{s}enje sustava $\vec A^*_{S^{n+1}} \vec A_{S^{n+1}} \vec z = \vec A^*_{S^{n+1}} \vec y$. Ta \v{c}injenica je korisna i u drugim algoritmima koji imaju korak sli\v{c}an s \eqref{omp_2}.
\begin{lem}\label{lem:3:4}
    Ako je $S \subseteq [N]$ i
    \begin{equation*}
        \vec v := \argmin_{\vec z \in \C^N} \{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subseteq S\},
    \end{equation*}
    tada je
    \begin{equation}\label{omp_ortogonalnost}
        (\vec A^*(\vec y - \vec{Av}))_S = \vec 0.
    \end{equation}
\end{lem}
\begin{proof}
    Prema definiciji vektora $\vec v$, vektor $\vec{Av}$ je ortogonalna projekcija vektora $\vec y$ na prostor $\{\vec{Az},\ \supp(\vec{z} \subseteq S)\}$, pa je karakteriziran relacijom ortogonalnosti
    \begin{equation*}
        \langle \vec y - \vec{Av}, \vec{Az} \rangle = 0 \quad \text{za sve }\vec z \in \C^N \ \text{takve da } \supp(\vec z) \subseteq S.
    \end{equation*}
    Dakle, imamo da vrijedi $\langle \vec A^*( \vec y - \vec{Av}), \vec{z} \rangle = 0$ za sve $\vec z \in \C^N,\ \supp(\vec z) \subseteq S$, \v{s}to vrijedi ako i samo ako vrijedi \eqref{omp_ortogonalnost}.
\end{proof}

Prirodan uvjet zaustavljanja OMP-a je kada se postigne $\norm{\vec{y} - \vec{Ax}^{\bar{n}}}_2 \leq \varepsilon$ ili $\norm{\vec A^*(\vec y - \vec{Ax}^{\bar{n}})}_{\infty} \leq \varepsilon$ za neku toleranciju $\varepsilon > 0$. Ako nam je dostupna ocjena rijetkosti $s$ rje\v{s}enja $\vec x$, tada je razumno stati kada je $\bar{n} = s$. Sljede\'ci rezultat govori o uvjetima za uspje\v{s}nu rekonstrukciju $s$-rijetkog vektora u $s$ iteracija OMP algoritma.

\begin{prop}\label{prop:3:5}
    Neka je $\vec A \in \C^{m \times N}$. Svaki ne-nul vektor $\vec x \in \C^N$ s nosa\v{c}em na skupu $S$, kardinaliteta $s$ mo\v{z}e se rekonstruirati iz $\vec y = \vec{Ax}$ u najvi\v{s}e $s$ iteracija OMP algoritma ako i samo ako je matrica $\vec A_S$ injektivna i 
    \begin{equation}\label{uvjet_rekon_omp}
        \max_{j \in S}|(\vec A^* \vec r)_j| > \max_{l \in \bar{S}}|(\vec A^* \vec r)_l|
    \end{equation}
    za sve ne-nul $\vec r \in \{\vec{Az},\ \supp(\vec z) \subseteq S\}$.
\end{prop}
% TODO nije mi bas jasno ovo
\begin{proof}
    Pretpostavimo da OMP algoritam rekonstruira sve vektore s nosa\v{c}em na skupu $S$ u najvi\v{s}e $s = \card(S)$ iteracija. Neka su $\vec v, \vec w$ s nosa\v{c}em na $S$, takvi da je $\vec{Av}=\vec{Aw}$. Zbog pretpostavke, $\vec v$ i $\vec w$ moraju biti jednaki, a to zna\v{c}i da je matrica $\vec A_S$ injektivna. Nadalje, ako je $\vec y = \vec{Ax}$ za neki $\vec x \in \C^N$ sa $\supp(\vec x)=S$, indeks $l \in \bar S$ ne mo\v{z}e biti izabran u prvoj iteraciji, po\v{s}to indeks izabran u prvoj iteraciji ostaje uvijek u nosa\v{c}u, a po pretpostavci OMP rekonstruira $\vec x$ iz $\vec y = \vec{Ax}$ u to\v{c}no $s$ iteracija. Dakle za $n=0$ iz \eqref{omp_1} imamo da je $\max_{j \in S}|(\vec A^*y)_j| > |(\vec A^*y)_l|$ za svaki $l \in \bar{S}$, pa stoga vrijedi $\max_{j \in S}|(\vec A^*y)_j| > \max_{l \in \bar{S}}|(\vec A^*y)_l|$ za sve ne-nul $\vec y \in \{\vec{Az},\ \supp(\vec z) \subseteq S\}$. \\
    \indent
    Obratno, pretpostavimo da je $\vec{Ax}^1 \neq y,\dots,\vec{Ax}^{s-1} \neq y$ jer u suprotnom nemamo \v{s}to dokazivati. Pokazat \'cemo da $S^n \subseteq S,\ \card(S^n)=n$ za $0 \leq n \leq s$. To \'ce implicirati $S^s = S$. Nadalje, \eqref{omp_2} daje $\vec{Ax}^s = \vec y$ a iz injektivnosti od $\vec{A}_S$ slijedi $\vec x_s = \vec{x}$. Dakle, neka je $0 \leq n \leq s-1$. Ako je $S^n \subseteq S$, to povla\v{c}i da je $\vec r^n := \vec y - \vec{Ax}^n \in \{\vec{Az},\ \supp(\vec z) \subseteq S\}$, pa prema \eqref{uvjet_rekon_omp} indeks $j_{n+1}$ le\v{z}i u S, pa $S^{n+1} = S \cup \{j_{n+1}\} \subseteq S$. Ovo induktivno pokazuje da je $S^n$ podskup od $S$ za svaki $0 \leq n \leq s$. Nadalje, neka je $1 \leq n \leq s-1$. Lema \eqref{lem:3:4} daje $(\vec{A}^* \vec r^n)_{S^n} = \vec 0$. Stoga, iz \eqref{omp_1} vidimo da indeks $j_{n+1}$ ne le\v{z}i u $S^{n}$, jer bi u protivnom $\vec A^* \vec r^n = \vec 0$, a po \eqref{uvjet_rekon_omp} $\vec r^n = \vec 0$. Dakle, $\card(S^n)=n$.
\end{proof}
\indent
Slabost OMP algoritma le\v{z}i u \v{c}injenici da ako krivi indeks u\dj e u nosa\v{c}, on ostaje u nosa\v{c}u u svim sljede\'cim iteracijama. Stoga $s$ iteracija algoritma nije dovoljno za rekonstrukciju vektora koji je $s$-rijedak. Mogu\'ce rje\v{s}enje je pove\'cati broj iteracija. Naredni algoritam, CoSaMP (eng. \textit{compressive sampling matching pursuit algorithm}), koristi druga\v{c}iju strategiju kada nam je dostupna ocijena rijetkosti $s$. Uvedimo oznake $H_s(\vec z)$ za najbolju $s$-rijetku aproksimaciju vekotra $\vec z \in \C^N$ i $L_s(\vec z)$ za nosa\v{c} od $H_s(\vec z)$, tj.
\begin{align}
    &L_s(\vec z) := \text{skup indeksa $s$ najve\'cih komponeneti vekora } \vec z \in \C^N \\
    &H_s(\vec z) := \vec z_{L_s(\vec z)}.
\end{align}
Nelinearni operator $H_s$ zovemo \textit{hard thresholding} operator reda $s$. Za dani vektor $\vec z \in \C^N$ on \v{c}uva $s$ apsolutno najve\'cih komponeneti a ostale postavi na nulu. Primijetimo da to nije nu\v{z}no jedinstveno definirano. Da bi zaobi\v{s}li taj problem, skup indeksa $L_s(\vec z)$ biramo iz svih mogu\'cih kandidata leksikografskim poretkom.

\begin{alg}{CoSaMP}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Inicijalizacija:} $s$-rijedak vektor $\vec x^0$ (npr. $\vec x^0 = \vec 0$).\\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{align*}
            U^{n+1} & = \supp(\vec x^n)\cup L_{2s}(\vec A^*(\vec y - \vec{Ax}^n))  \tag{$CoSaMP_1$}\label{cosamp_1}\\
            \vec u^{n+1} & = \argmin_{\vec z \in \C^N}\{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subseteq U^{n+1}\}  \tag{$CoSaMP_2$}\label{cosamp_2}\\
            \vec x^{n+1} & = H_s(\vec u^{n+1})  \tag{$CoSaMP_3$}\label{cosamp_3}
        \end{align*}
        \textit{Izlaz:} $\bar{n}$-rijedak vektor $\vec x^{\sharp}=\vec{x}^{\bar{n}}$.
\end{alg}



\section[Grani\v{c}ne metode][Grani\v{c}ne metode]{Grani\v{c}ne metode}
Algoritmi predstavljeni u ovom poglavlju tako\dj er koriste \textit{hard thresholding} operator $H_s$. Prvi algoritam, BT (eng. \textit{basic thresholding}), sastoji se od odre\dj ivanja nosa\v{c}a $s$-rijetkog vektora $\vec x \in \C^N$, koji se rekonstruira iz $\vec y = \vec{Ax} \in \C^m$ te tra\v{z}enja vektora koji najbolje aproksimira mjerenje $\vec y$.

\begin{alg}{BT}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Problem:}
        \begin{align*}
            S^{\sharp} &= L_s(\vec A^* \vec y),\tag{$BT_1$}\label{bt_1}\\
            \vec x^{\sharp} &= \argmin_{\vec z \in \C^N}\{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subseteq S^{\sharp}\}.\tag{$BT_2$}\label{bt_2}\\
        \end{align*}
        \textit{Izlaz:} $s$-rijedak vektor $\vec x^{\sharp}$.
\end{alg}

\noindent Dovoljni i nu\v{z}ni uvjeti rekonstrukcije jednostavnim BT algoritmom, sli\v{c}ni su uvjetima iz \eqref{uvjet_rekon_omp}.
\begin{prop}\label{prop:3:7}
    BT algoritam rekonstruira vektor $\vec x \in \C^N$ s nosa\v{c}em na $S$, iz $\vec y = \vec{Ax}$ ako i samo ako
    \begin{equation}\label{bt_uvjet}
        \min_{j \in S}|(\vec A^* \vec y)_j| > \max_{l \in \bar{S}} |(\vec A^* \vec y)_l| .
    \end{equation}
\end{prop}
\begin{proof}
    Vektor $\vec x$ mo\v{z}e se rekonstruirati ako i samo ako je skup indeksa $S^{\sharp}$ u \eqref{bt_1} jednak skupu $S$. To vrijedi ako i samo ako je element vektora $\vec A^* \vec y$ s indeksom iz $S$, ve\'ci od svakog elementa vektora $\vec A^* \vec y$ s indeksom u $\bar{S}$.
\end{proof}
\indent
IHT (eng. \textit{iterative hard thresholding}) algoritam rje\v{s}ava kvadratni sustav $\vec A^* \vec A \vec z= \vec A^* \vec y$ umjesto $\vec{Az}=\vec y$. To mo\v{z}emo interpretirati kao rje\v{s}avanje problema fiksne to\v{c}ke $\vec z = (\vec{I}- \vec A^* \vec A ) \vec z + \vec A^* \vec y$. Prirodno je gledati iteracije oblika $\vec x^{n+1} = (\vec{I}- \vec A^* \vec A) \vec x^n + \vec A^* \vec y$. Po\v{s}to tra\v{z}imo $s$-rijetko rje\v{s}enje u svakoj iteraciji uzimamo samo $s$ apsolutno najve\'cih komponenti od $(\vec{I} - \vec A^* \vec A ) \vec x^n + \vec A^* \vec y$.

\begin{alg}{IHT}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Inicijalizacija:} $s$-rijedak vektor $\vec x^0$ (npr. $\vec x^0 = \vec 0$).\\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{equation}
            x^{n+1} = H_s(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n).\tag{$IHT$}\label{iht}\\
        \end{equation}
        \textit{Izlaz:} $s$-rijedak vektor $\vec x^{\sharp}=\vec x^{\bar n}$.
\end{alg}

Primijetimo da IHT algoritam ne koristi ortogonalne projekcije, \v{s}to je njegova prednost. No, ako smo spremi platiti cijenu projekcija, ima smisla gledati vektor koji ima isti nosa\v{c} kao $\vec x^{n+1}$ koji najbolje aproksimira mjerenje. Upravo je to strategija HTP (eng. \textit{hard thresholding pursuit}) algoritma.

\begin{alg}{HTP}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Inicijalizacija:} $s$-rijedak vektor $\vec x^0$ (npr. $\vec x^0 = \vec 0$).\\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{align*}
            S^{n+1} &= L_s(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n),\tag{$HTP_1$}\label{htp_1}\\
            \vec x^{n+1} &= \argmin_{\vec z \in \C^N}\{ \norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subseteq S^{n+1} \}.\tag{$HTP_2$}\label{htp_2}
        \end{align*}
        \textit{Izlaz:} $s$-rijedak vektor $\vec x^{\sharp}=\vec x^{\bar n}$.
\end{alg}





\chapter[$\ell_1$-minimizacija][$\ell_1$-minimizacija]{$\ell_1$-minimizacija}
Prisjetimo se, problem sa\v{z}etog uzorkovanja sastoji se od rekonstrukcije $s$-rijetkog vektora $\vec x \in \C^N$ iz mjerenja $\vec y = \vec{Ax} \in \C^m$, gdje je $m < N$. Prirodno se name\'ce problem $\ell_0$-minimizacije,
\noindent
\begin{equation}
\min_{\vec z \in \C^N} \norm{\vec z}_0\quad \text{uz uvjet }\vec{Az} = \vec{y}\tag{$P_0$}
\end{equation}
U poglavlju \eqref{chapter_rijetka_rijesenja} vidjeli smo da je taj problem op\'cenito $\mathfrak{NP}$-te\v{z}ak. U poglavlju \eqref{chapter_algoritmi} pokazali smo nekoliko u\v{c}inkovitih strategija za rje\v{s}avanje problema sa\v{z}etog uzorkovanja. U ovom poglavlju fokusirat \'cemo se na strategiju $\ell_1$-minimizacije
\begin{equation}
    \min_{\vec z \in \C^N} \norm{\vec z}_1 \quad \text{uz uvjet }\vec{Az}=\vec y.\tag{$P_1$}
\end{equation}
Prou\v{c}it \'{c}emo uvjete na matricu $\vec A$ koji osiguravaju egzaktnu ili aproksimativnu rekonstrukciju vektora $\vec x$.

\section[Svojstvo nul-prostora][Svojstvo nul-prostora]{Svojstvo nul-prostora}
Argumenti u ovom potpoglavlje vrijede u kontekstu realnih i u konteksu kompleksnih prostora. Stoga \'cemo rezultate prvo iznijeti za polje $\K$, koje mo\v{z}e biti $\R$ ili $\C$. Nakon toga uspostavit \'cemo ekvivalenciju realnog i kompleksnog svojstva nul-prostora.

\begin{defn}
    Za matricu $\vec A \in \K^{m \times N}$ ka\v{z}emo da zadovoljava \textit{svojstvo nul-prostora} za skup $S \subseteq [N]$ ako vrijedi
    \begin{equation}\label{svojstvo_nul_prostora}
        \norm{\vec v_S}_1 < \norm{\vec v_{\bar{S}}}_1  \quad \text{za svaki }\vec v \in \ker \vec A \backslash \{\vec 0\}.
    \end{equation}
    Nadalje, ka\v{z}emo da $\vec A$ zadovoljava svojstvo nul-prostora reda $s$ ako zadovoljava gornju nejednakost za svaki $S \subseteq [N]$ takav da je $\card(S) \leq s$.
\end{defn}

Primijetimo da za vektor $\vec v \in \ker \vec A \backslash \{ \vec 0\}$ svojstvo nul-prostora vrijedi za svaki $S \subseteq [N]$ takav da je $\card(S) \leq s$, \v{c}im vrijedi za skup indeksa $s$ apsolutno najve\'cih komponenti vektora $\vec v$. \\
\indent Postoje dvije dodatne formulaciju svojsta nul-prostora. Prvu dobijemo tako da gornjoj nejednakosti dodamo $\norm{\vec v_s}_1$ s obje strane. Tada imamo
\begin{equation}\label{svojstvo_nul_prostora_form_1}
    2 \norm{\vec v_S}_1 < \norm{\vec v}_1 \quad \text{za svaki } \vec v \in \ker \vec A \backslash \{\vec 0\}.
\end{equation}
Drugu dobijemo tako da u skup $S$ stavimo $s$ apsolutno najve\'cih  komponenti vektora $\vec v$ i ovaj put nejednakosti dodamo $\norm{\vec v_{\bar S}}_1$ s obje strane. Tada imamo
\begin{equation}\label{svojstvo_nul_prostora_form_2}
    \norm{\vec v}_1 < 2 \sigma_s(\vec v)_1 \quad \text{za sve } \vec v \in \ker \vec A \backslash \{\vec 0\}.
\end{equation}
Prisjetimo se definicije \ref{greska_naj_s_aprox} $\ell_p$-gre\v{s}ke najbolje $s$-rijetke aproksimacije vektora $\vec x \in \K^N$,
\begin{equation*}
    \sigma_s(\vec x)_p = \inf_{\norm{\vec z}_0 \leq s} \norm{\vec x - \vec z}_p.
\end{equation*}
Sljede\'ci teorem govori o vezi svojstva nul-prostora i egzaktne rekonstrukcije rijetkog vektora putem $\ell_1$-minimizacije.
\begin{thm}\label{bp_tm1}
    Za $\vec A \in \K^{m \times N}$, svaki vektor $\vec x \in \K^N$ s nosa\v{c}em na $S$ je jedinstveno rje\v{s}enje od \eqref{problem_minimizacije_l1} s $\vec y = \vec {Ax}$ ako i samo ako $\vec A$ zadovoljava svojstvo nul-prostora za skup $S$.
\end{thm}
\begin{proof}
    Neka je skup indeksa $S$ fiksan. Pretpostavimo da je svaki vektor $\vec x \in \K^N$ s nosa\v{c}em na $S$ jedinstveni minimizator od $\norm{\vec z}_1$ takav da je $\vec {Az} = \vec {Ax}$. Stoga za svaki $\vec v \in \ker \vec A \backslash \{\vec 0\}$, vektor $\vec v_S$ je jedinstveni minimizator od $\norm{\vec z}_1$ takav da je $\vec{Az} = \vec{Av}_S$. Me\dj utim, imamo $\vec A (-\vec v_{\bar S}) = \vec A \vec v_S$ i $-\vec v_{\bar S} \neq \vec v_S$ jer je $\vec v \neq \vec 0$ i $ \vec 0 = \vec{Av} = \vec A (\vec v_S + \vec v_{\bar S})$. Dakle, mora vrijediti $\norm{\vec v_S}_1 < \norm{\vec v_{\bar S}}_1$.\\
\indent
Obratno, pretpostavimo da $\vec A$ zadovoljava svojstvo nul-prostora za skup $S$. Tada za vektor $\vec x \in \K^N$ s nosa\v{c}em na $S$ i za $\vec z \in \K^N$, $\vec z \neq \vec x$ takve da je $\vec{Az}=\vec{Ax}$, ozna\v{c}imo vektor $\vec v := \vec x - \vec z \in \ker \vec A \backslash \{\vec 0\}$. Imamo,
\begin{equation*}
    \norm{\vec x} \leq \norm{\vec x - \vec z_S}_1 + \norm{\vec z_S}_1 = \norm{\vec v_S}_1 + \norm{\vec z_S}_1 < \norm{\vec v_{\bar S}}_1 + \norm{\vec z_S}_1 = \norm{- \vec z_{\bar S}}_1 + \norm{\vec z_S}_1 = \norm{\vec z}_1
\end{equation*}
Dakle, vektor $\vec x$ je minimizator od \eqref{problem_minimizacije_l1}.
\end{proof}

\indent
Variranjem skupa $S$, sljede\'ci rezultat slijedi direktno iz prethodnog teorema. 

\begin{thm}\label{svojstvo_nul_prostora_tm}
    Za matricu $\vec A \in \K^{m \times N}$, svaki $s$-rijedak vektor $\vec x \in \K^N$ je jedinstveno rje\v{s}enje problema \eqref{problem_minimizacije_l1} uz $\vec y = \vec{Ax}$ ako i samo ako $\vec A$ zadovoljava svojstvo nul-prostora reda $s$.
\end{thm}

Primijetimo da prethodni teorem tvrdi da za svaki $\vec y = \vec{Ax}$, gdje je $\vec x$ $s$-rijedak, $\ell_1$-minimizacija \eqref{problem_minimizacije_l1} zapravo rje\v{s}ava problem $\ell_0$-minimizacije \eqref{problem_minimizacije} kada vrijedi svojstvo nul-prostora reda $s$. Zaista, pretpostavimo da se svaki $s$-rijedak vektor $\vec{x}$ mo\v{z}e rekonstruirati $\ell_1$-minimizacijom iz $\vec y = \vec{Ax}$. Neka je $\vec z$ minimizator $\ell_0$ problema \eqref{problem_minimizacije} s $\vec y = \vec{Ax}$, tada je $\norm{\vec z}_0 \leq \norm{\vec x}_0$ pa je $\vec z$ tako\dj er $s$-rijedak. No, po prethodnom teoremu, svaki $s$-rijedak vektor je jedinstveni $\ell_1$-minimizator pa slijedi da je $\vec x = \vec z$.
\newline \indent
Za algoritam rekonstrukcije po\v{z}eljno je da zadr\v{z}i mogu\'cnost rekonstrukcije ako su neka od mjerenja reskaliraju, permutiraju ili dodaju nova. $\ell_1$-minimizacija ima takvo svojstvo. Formalno, gore opisane promjene zapravo predstavljaju zamjenu matrice $\vec A$ matricama $\vec{\hat A}$ i $\vec{\tilde A}$
\begin{align*}
    & \vec{\hat A} := \vec{GA}, \quad \text{gdje je }\vec{G}\text{ neka invertibilna }m \times m \text{ matrica},\\
    & \vec{\tilde A} := 
    \begin{bmatrix*}
        \vec A \\ \vec B
    \end{bmatrix*}
    , \quad \text{gdje je }\vec{B}\text{ neka }m' \times N \text{ matrica}.
\end{align*}
Primijetimo da je $\ker \vec{\hat A} = \ker \vec A$ i $\ker \tilde A \subseteq \ker \vec A$, pa svojstvo nul-prostora vrijedi i za matrice $\vec{\hat A}$ i $\vec{\tilde A}$.
\newline
\newline
\indent
Za kraj prou\v{c}it \'cemo utjecaj polja $\K$. Razlika izme\dj u $\ker_{\R}\vec A$ i\\ $\ker_{\C} \vec A = \ker_{\R} \vec A + i \ker_{\R} \vec A$ vodi u slu\v{c}aju da je $\K=\R$ na realno svojstvo nul-prostora, 
\begin{equation}\label{svojstvo_nul_prostora_realno}
    \sum_{j \in S}|v_j| < \sum_{l \in \bar{S}}|v_l| \quad \text{za svaki } \vec v \in \ker_{\R} \vec A,\ \vec v \neq \vec 0,  
\end{equation}
a u slu\v{c}aju da je $\K = \C$, na kompleksno svojsto nul-prostora,
\begin{equation}\label{svojstvo_nul_prostora_kompleksno}
    \sum_{j in S}\sqrt{v_j^2 + w_j^2} < \sum_{l \in \bar{S}}\sqrt{v_j^2 + w_j^2}\quad \text{za svaki } \vec v, \vec w \in \ker_{\R} \vec A,\ \ ( \vec v, \vec w ) \neq \vec (\vec 0,\vec 0).  
\end{equation}

\noindent
Zapravo, pokazat \'cemo da su svojstva nul-prostora me\dj usobno ekvivalentna u realnom i kompleksnom slu\v{c}aju. Zato mo\v{z}emo re\'ci da realna matrica mjerenja egzaktno rekonstruira sve rijetke vektore $\ell_1$-minimizacijom. 
\begin{thm}
    Neka je $\vec A \in \R^{m \times N}$. Tada je realno svojstvo nul-prostora \eqref{svojstvo_nul_prostora_realno} za skup $S$ ekvivalentno je kompleksnom svojstvu nul-prostora \eqref{svojstvo_nul_prostora_kompleksno} za isti skup $S$.
\end{thm}
\begin{proof}
    Primijetimo \eqref{svojstvo_nul_prostora_realno} slijedi direktno iz \eqref{svojstvo_nul_prostora_kompleksno} za $\vec w = \vec 0$. Uzmimo sada $\vec v, \vec w \in \ker_{\R}\vec A$, takvi da je $(\vec v, \vec w) \neq (\vec 0, \vec 0)$. Ako su $\vec v$ i $\vec w$ linearno zavisni. tj. $\vec v = \alpha \vec w$ za neki $\alpha \in \R \backslash \{ \vec 0 \}$ onda je 
   \begin{align*}
       \sum_{j \in S} \sqrt{v_j^2+w_j^2} &=  \sum_{j \in S} \sqrt{(1+\alpha^2)w_j^2}=\sqrt{1+\alpha^2}\sum_{j \in S} \sqrt{w_j^2}\\
       &< \sqrt{1+\alpha^2}\sum_{j \in \bar S} \sqrt{w_j^2} = \sum_{j \in \bar S} \sqrt{(1+\alpha^2)w_j^2} = \sum_{j \in \bar S} \sqrt{v_j^2+w_j^2}
   \end{align*} 
   Pretpostavimo sada da su $\vec v$ i $\vec w$ linearno nezavisni i definirajmo $\vec u := \cos \theta \vec v + \cos \theta \vec v \in \ker_{\R} \vec A \backslash \{\vec 0\}$. Tada za svaki $\theta \in \R$ vrijedi
   \begin{equation}\label{svojstvo_nul_prostora_r_c_nejed1}
       \sum_{j \in S} |\cos \theta v_j + \sin \theta w_j| < \sum_{l \in \bar S} |\cos \theta v_l + \sin \theta w_l|.
   \end{equation}
   Za svaki $k \in [N]$, neka je $\theta_k \in [-\pi, \pi]$ takav da
   \begin{equation*}
       v_k = \sqrt{v_k^2 + w_k^2}\cos{\theta_k}, \quad w_k = \sqrt{v_k^2 + w_k^2}\sin{\theta_k}
   \end{equation*}
   Iz \eqref{svojstvo_nul_prostora_r_c_nejed1} slijedi
   \begin{equation*}
       \sum_{j \in S}\sqrt{v_j^2+w_j^2}|\cos(\theta - \theta_j)|<\sum_{l \in \bar S}\sqrt{v_l^2+w_l^2}|\cos(\theta - \theta_l)|
   \end{equation*}
   Integriranjem po $\theta \in [-\pi,\pi]$ dobijemo
   \begin{equation*}
       \sum_{j \in S}\sqrt{v_j^2+w_j^2}\int_{-\pi}^{\pi}  |\cos(\theta - \theta_j)| d \theta<\sum_{l \in \bar S}\sqrt{v_l^2+w_l^2}\int_{-\pi}^{\pi}  |\cos(\theta - \theta_l)|
   \end{equation*}
    No lako se provjeri da je
    \begin{equation*}
         \int_{-\pi}^{\pi}  |\cos(\theta - \theta')| d \theta = 4
    \end{equation*}
    tj. da je taj integral pozitivan i neovisan o $\theta' \in [-\pi, \pi]$.
\end{proof}

\subsection[Nekonveksna minimizacija][Nekonveksna minimizacija]{Nekonveksna minimizacija}
Prisjetimo se, $\ell_0$ norma vektora $\vec z \in \C^N$ aproksimirana je $q$-tom potencijom svoje $\ell_q$-kvazinorme,
\begin{equation*}
\|\vec{z}\|_p^p := \sum_{j=1}^N|x_j|^p \xrightarrow{p\rightarrow 0} \sum_{j=1}^N\mathbf{1}_{\{z_j \neq 0\}} = \|\vec{z}\|_0
\end{equation*}
To sugerira da $\ell_0$-minimizaciju \eqref{problem_minimizacije} zamijenimo s
\begin{equation}
    \min_{\vec z \in \C^N} \norm{\vec z}_q \quad \text{uz uvjet }\vec{Az} = \vec{y}.\tag{$P_q$}
\end{equation}
Za $0 < q < 1$ taj je problem nekonveksan i $\mathfrak{NP}$-te\v{z}ak. No, \v{z}elimo teoretski potvrditi ideju da \eqref{problem_minimizacije_aprox} dobro aproksimira \eqref{problem_minimizacije} za male $q$.
Sljede\'ci teorem daje analogon svojstva nul-prostora za $0<q<1$. Dokaz je tako\dj er analogan dokazu teorema \ref{svojstvo_nul_prostora_tm} te se koristi \v{c}injenica da za $\ell_q$-kvazinorma zadovoljava nejednakost trokuta.  
\begin{thm}\label{svojstvo_nul_prostora_tm_2}
    Za matricu $\vec A \in \C^{m \times N}$ i $0<q<1$, svaki $s$-rijedak vektor $\vec x \in \C^N$ je jedinstveno rje\v{s}enje problema \eqref{problem_minimizacije_aprox} uz $\vec y = \vec{Ax}$ ako i samo ako 
    \begin{equation*}
        \norm{\vec v_S}_q < \norm{\vec v_{\bar{S}}}_q  \quad \text{za svaki }\vec v \in \ker \vec A \backslash \{\vec 0\}.
    \end{equation*}
\end{thm}
Sada mo\v{z}emo dokazivati da rekonstrukcija $\ell_q$-minimizacijom implicira rekonstrukciju $\ell_p$-minimizacijom za $0<p<q<1$.
\begin{thm}
    Za matricu $\vec A \in \C^{m \times N}$ i $0<p<q<1$, ako je svaki $s$-rijedak vektor $\vec x \in \C^N$ jedinstveno rje\v{s}enje problema \eqref{problem_minimizacije_aprox} uz $\vec y = \vec{Ax}$  onda je $\vec x$ tako\dj er i rje\v{s}enje problema $(P_p)$ za $\vec y = \vec{Ax}$.
\end{thm}
\begin{proof}
    Prema teoremu \ref{svojstvo_nul_prostora_tm_2} dovoljno je pokazati da vrijedi
    \begin{equation}\label{svojstvo_nul_prostora_tm_3_nejed}
        \sum_{j \in S} |v_j|^p < \sum_{l \in \bar S}|v_l|^p,
    \end{equation}
    ako je $\vec v \in \ker \vec A \backslash \{\vec 0\}$, $S$ skup indeksa od $s$ apsolutno najve\'cih komponeneti od $\vec v$ i ako ista nejednakost vrijedi za $q$.
    Dakle, pretpostavimo da \eqref{svojstvo_nul_prostora_tm_3_nejed} vrijedi za $q$. Tada je nu\v{z}no $\vec v_{\bar S} \neq \vec 0$ po\v{s}to je $S$ skup indeksa od $s$ apsolutno najve\'cih komponeneti ne-nul vektora $\vec v$. Stoga \eqref{svojstvo_nul_prostora_tm_3_nejed} mo\v{z}emo napisati u obliku
    \begin{equation}\label{svojstvo_nul_prostora_tm_3_nejed_2}
        \sum_{j \in S} \frac{1}{\sum_{l \in \bar S}(|v_l|/|v_j|)^p} < 1.  
    \end{equation}
    Primijetimo da $|v_l|/|v_j| \leq 1$ za $l \in \bar S$ i $j \in S$. Stoga je lijeva strana \eqref{svojstvo_nul_prostora_tm_3_nejed_2} nepadaju\'ca funkcija u varijabli $0<p \leq 1$. Zato njena vrijednost u $p$ ne prelazi njezinu vrijednost u $q$, koja je manja od 1 po pretpostavci.
\end{proof}

\section[Stabilnost][Stabilnost]{Stabilnost}
Signali u praksi gotovo nikad nisu idealno rijetki. U najboljem slu\v{c}aju blizu su rijetkim vektorima. Stoga \v{z}elimo da metode sa\v{z}etog uzorkovanja rekonstruiraju vektor $\vec x \in \C^N$ s gre\v{s}kom koja je kontrolirana udaljenosti vektora $\vec x$ do $s$-rijetkih vektora. Za algoritme koji imaju to svojsto ka\v{z}emo da su \textit{stabilni} s obzirom na defekte rijetkosti. Pokazat \'cemo da je $\ell_1$-minimizacija \eqref{problem_minimizacije_l1} stabilna pod ja\v{c}im svojstvom nul-prostora.

\begin{defn}
    Matrica $\vec A \in \C^{m \times N}$ zadovoljava \textit{stabilno svojstvo nul-prostora} s konstantom $0<\rho<1$ za skup $S \subseteq [N]$ ako
    \begin{equation*}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 \quad \text{za svaki }\vec v \in \ker \vec A.
    \end{equation*}
    Nadalje, ka\v{z}emo da $\vec A$ zadovoljava \textit{stabilno svojstvo nul-prostora reda} $s$ sa konstantom $0<\rho<1$ ako zadovoljava zadovoljava gornju nejednakost za svaki $S \subseteq [N]$ takav da $\card(S)=s$.
\end{defn}

\begin{thm}\label{stabilnost_tm_1}
    Ako matrica $\vec A \in \C^{m \times N}$ zadovoljava stabilno svojstvo nul-prostora reda $s$ sa konstantom $0<\rho<1$, tada za svaki $\vec x \in \C^N$, rje\v{s}enje $\vec x^{\sharp}$ problema \eqref{problem_minimizacije_l1} s $\vec y = \vec{Ax}$ aproksimira vektor $\vec x$ s $\ell_1$-gre\v{s}kom
    \begin{equation}\label{stabilnost_tm_1_nejed}
        \norm{\vec x - \vec x ^{\sharp}}_1 \leq \frac{2(1+\rho)}{(1-\rho)}\sigma_s(\vec x)_1.
    \end{equation}
\end{thm}
\noindent
Sada vi\v{s}e nemamo jedinstvenost $\ell_1$-minimizatora. Prethodni teorem biti \'ce direktna posljedica ja\v{c}e tvrdnje,
\begin{thm}\label{stabilnost_tm_2}
    Ako matrica $\vec A \in \C^{m \times N}$ zadovoljava stabilno svojstvo nul-prostora s konstantom $0<\rho<1$ za skup $S$ ako i samo ako
    \begin{equation}\label{stabilnost_tm_2_nejed}
        \norm{\vec z - \vec x}_1  \leq \frac{1+\rho}{1-\rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1) 
    \end{equation}
    za sve vektore $\vec x, \vec z \in \C^N$ takve da je $\vec{Az} = \vec{Ax}$.
\end{thm}

Poka\v{z}imo kako teorem \ref{stabilnost_tm_1} slijedi iz \ref{stabilnost_tm_2}:
Neka je $S$ skup $s$ apsolutno najve\'cih komponenti vektora $\vec x$, tako da $\norm{\vec x_{\bar S}}=\sigma_s(\vec x)_1$. Ako je $\vec x^{\sharp}$ minimizator problema \eqref{problem_minimizacije_l1}, tada vrijedi $\norm{\vec x^{\sharp}}_1 \leq \norm{\vec x}_1$ i $\vec{Ax}^{\sharp}= \vec{Ax}$. Dakle, desnu strana u \eqref{stabilnost_tm_2_nejed} za $\vec z = \vec x^{\sharp}$ mo\v{z}emo ocijeniti desnom stranom \eqref{stabilnost_tm_1_nejed}.\\
\indent
Prije dokaza teorema \ref{stabilnost_tm_2} poka\v{z}imo jo\v{s} jedan koristan rezultat.
\begin{lem}\label{stabilnost_lema_1}
    Za $S \subseteq [N]$ i vektore $\vec x, \vec z \in \C^N$ vrijedi,
    \begin{equation*}
        \norm{(\vec x - \vec z)_{\bar S}}_1 \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \norm{(\vec x - \vec z)_S}_1 + 2 \norm{\vec x_{\bar S}}_1
    \end{equation*}
\end{lem}
\begin{proof}
    Imamo,
    \begin{align*}
        \norm{\vec x}_1 &= \norm{\vec x_{\bar S}}_1 + \norm{\vec{x}_S}_1 \leq \norm{\vec x_{\bar S}}_1 + \norm{(\vec x - \vec z)_S}_1 + \norm{\vec z_{S}}_1\\
        \norm{(\vec x - \vec z)_{\bar S}}_1 &\leq \norm{\vec x_{\bar S}}_1 + \norm{\vec z_{\bar S}}_1.
    \end{align*}
    Sumiranjem ove dvije nejednakosti, slijedi
    \begin{equation*}
        \norm{\vec x}_1 + \norm{(\vec x - \vec z)_{\bar S}}_1 \leq 2 \norm{\vec x_{\bar S}}_1 +  \norm{(\vec x - \vec z)_S}_1 + \norm{\vec z}_1.
    \end{equation*}
\end{proof}

\begin{proof}[Dokaz (Teorem \ref{stabilnost_tm_2})]
    Pretpostavimo da matrica $\vec A$ zadovoljava \eqref{stabilnost_tm_2_nejed} za sve vektore $\vec x, \vec z \in \C^N$ uz $\vec{Az} = \vec{Ax}$. Za dani vektor $\vec{v} \in \ker \vec A$, po\v{s}to je $\vec{Av}_{\bar S} = \vec A(-\vec v_S)$ mo\v{z}emo primjeniti \eqref{stabilnost_tm_2_nejed} s $\vec x = - \vec v_S$ i $\vec z = \vec v_{\bar S}$. Slijedi,
    \begin{equation*}
        \norm{\vec v}_1 \leq \frac{1+\rho}{1-\rho}(\norm{\vec v_{\bar S}}_1 - \norm{\vec v_S}_1). 
    \end{equation*}
    To mo\v{z}emo zapisati kao 
    \begin{equation*}
        (1-\rho)(\norm{\vec v_S}_1 + \norm{\vec v_{\bar S}}_1)  \leq  (1+\rho)(\norm{\vec v_{\bar S}}_1 + \norm{\vec v_S}_1).
    \end{equation*}
    Jednostavnom manipulacijom slijedi
    \begin{equation*}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 
    \end{equation*}
    \indent
    Obratno, neka matrica $\vec A$ zadovoljava stabilno svojstvo nul-prostora s konstantom $0<\rho<1$ za skup $S$. Neka su $\vec x, \vec z \in \C^N$ takvi da je $\vec{Az} = \vec{Ax}$, po\v{s}to je $\vec v := \vec z - \vec x \in \ker \vec A$, stabilno svojstvo nul-prostora daje
    \begin{equation}\label{stabilnost_tm_3_nejed_1}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1.
    \end{equation}
    Nadalje, iz leme \ref{stabilnost_lema_1} slijedi
    \begin{equation}\label{stabilnost_tm_3_nejed_2}
        \norm{\vec v_{\bar S}}_1  \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \norm{\vec v_S}_1 + 2 \norm{\vec x_{\bar S}}_1.
    \end{equation}
    Substituiramo \eqref{stabilnost_tm_3_nejed_1} u \eqref{stabilnost_tm_3_nejed_2},
    \begin{equation*}
        \norm{\vec v_{\bar S}}_1  \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \rho\norm{\vec v_{\bar S}}_1 + 2 \norm{\vec x_{\bar S}}_1.
    \end{equation*}
    Po\v{s}to je $\rho < 1$,
    \begin{equation*}
        \norm{\vec v_{\bar S}}_1 \leq \frac{1}{1 - \rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2\norm{\vec x_{\bar S}}_1).  
    \end{equation*}
    Ponovno iskoristimo \eqref{stabilnost_tm_3_nejed_1},
    \begin{equation*}
        \norm{\vec v}_1 = \norm{\vec v_{\bar S}}_1 +  \norm{\vec v_{S}}_1  \leq (1 + \rho) \norm{\vec v_{\bar S}}_1 \leq \frac{1+\rho}{1-\rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1).
    \end{equation*}
\end{proof}

\section[Robusnost][Robusnost]{Robusnost}
Jasno je da u realnosti signal nikad ne mo\v{z}emo mjeriti s beskona\v{c}nom to\v{c}no\v{s}\'cu. U na\v{s}em kontekstu to zna\v{c}i da je vektor mjerenja $\vec y \in \C^m$ aproksimacija vektora $\vec{Ax} \in \C^m$, tj. formalno
\begin{equation*}
    \norm{\vec{Ax} - \vec{y}} \leq \eta
\end{equation*}
za neki $\eta \leq 0$ i neku normu na $\C^m$. Od metode rekonstrukcije tra\v{z}imo da udaljenost rekonstruiranog vektora $\vec x^{\sharp}$ i orginalnog vektora $\vec x$ bude kontrolirana precizno\v{s}\'cu mjerenja $\eta$. Ako metoda zadovoljava to svojstvo ka\v{z}emo da je \textit{robusna} ili \textit{otporna} na gre\v{s}ke mjerenja. Pokazat \'cemo da je BP algoritam ($\ell_1$-minimizacija) robusna ako \eqref{problem_minimizacije_l1} zamjenimo konveksnim problemom
\begin{equation}
    \min_{\vec z \in \C^N} \norm{\vec z}_1 \quad \text{uz uvjet } \norm{\vec{Az} - \vec y} \leq \eta \tag{P_{1, \eta}}
\end{equation}
te ako vrijedi sljede\'ca ja\v{c}a varijanta svojstva nul-prostora.
\begin{defn}
    Za matricu $\vec A \in \C^{m \times N}$ ka\v{z}emo da zadovoljava \textit{robusno svojstvo nul-prostora} s konstantama $0<\rho<1$ i $\tau > 0$ za skup $S \subseteq [N]$ ako 
    \begin{equation}\label{robusnost_defn_nejed}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 + \tau \norm{\vec{Av}} \quad \text{za sve } \vec v \in \C^N.
    \end{equation}
    Nadalje, ka\v{z}emo da $\vec A$ zadovoljava robusno svojstvo nul-prostora s konstantama $0<\rho<1$ i $\tau > 0$ reda $s$ ako zadovoljava gornje svojstvo za svaki $S \subseteq [N]$ takav da $\card(S) \leq s$.
\end{defn}

\noindent
Primijetimo da definicija ne tra\v{z}i da je $\vec v \in \ker \vec A$. Kada bi to vrijedilo propao bi \v{c}lan $\norm{\vec{Ax}}$ i time bi dobili stabilno svojstvo nul-prostora. 
\begin{thm}
    Neka matrica $\vec A \in \C^{m \times N}$ zadovoljava robusno svojstvo nul-prostora reda $s$ sa konstantama $0<\rho<1$ i $\tau > 0$. Tada za svaki vektor $\vec x \in \C^N$, rje\v{s}enje problema \eqref{problem_minimizacije_l1_kvadraticni} za $\vec y = \vec{Ax}+\vec{e}$ i $\norm{\vec e} \leq \eta$ aproksimira vektor $\vec x$ s gre\v{s}kom
    \begin{equation*}
        \norm{\vec x - \vec x^{\sharp}}_1 \leq \frac{2(1+\rho)}{(1-\rho)} \sigma_s(\vec x)_1 + \frac{4 \tau}{1-\rho}\eta 
    \end{equation*}
\end{thm}
Dokazat \'cemo ja\v{c}u tvrdnju,
\begin{thm}\label{tm:4:20}
    Matrica $\vec A \in \C^{m \times N}$ zadovoljava robusno svojstvo nul-prostora s konstantama $0<\rho<1$ i $\tau > 0$ za skup $S$ ako i samo ako
    \begin{equation}\label{robusno_tm2_nejed}
        \norm{\vec z - \vec x}_1 \leq \frac{1+\rho}{1-\rho} (\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1) + \frac{2 \tau}{1 - \rho} \norm{\vec A (\vec z - \vec x)}  
    \end{equation}
    za sve vektore $\vec x, \vec z \in \C^N$.
\end{thm}
\begin{proof}
    Pretpostavimo da $\vec A$ zadovoljava \eqref{robusno_tm2_nejed}. Za $\vec v \in \C^N$, uzmimo $\vec x = - \vec v_S$ i $\vec z = \vec v_{\bar S}$. Slijedi,
    \begin{equation*}
        \norm{\vec v}_1 \leq \frac{1+\rho}{1- \rho}(\norm{\vec v_{\bar S}}_1 - \norm{\vec v_S}_1) + \frac{2 \tau}{1 - \rho} \norm{\vec{Av}}.
    \end{equation*}
    Preslagivanjem \v{c}lanova dobivamo
    \begin{equation*}
        (1-\rho)(\norm{\vec v_S}_1 + \norm{\vec v_{\bar S}}_1) \leq (1 + \rho)(\norm{\vec v_{\bar S}}_1 - \norm{\vec v_S}_1) + 2 \tau \norm{Av}\
    \end{equation*}
    tj. imamo
    \begin{equation*}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 + \tau \norm{\vec{Av}}.
    \end{equation*} 

    \indent Obratno, neka $\vec A$ zadovoljava robusno svojstva nul-prostora s konstantama $0<\rho<1$ i $\tau > 0$ za skup $S$. Za $\vec x, \vec z \in \C^N$, neka je $\vec v := \vec z - \vec x$. Iz robusnog svojstvo nul-prostora i leme \ref{stabilnost_lema_1} slijedi
   \begin{align*}
       &\norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 + \tau \norm{\vec{Av}},\\
       &\norm{\vec v_{\bar S}}_1 \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \norm{\vec v_S}_1 + 2 \norm{\vec x_{\bar S}}_1.
   \end{align*} 
   Kombiniranjem te dvije nejednakosti imamo,
   \begin{equation*}
       \norm{\vec v_{\bar S}}_1 \leq \frac{1}{1-\rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1 + \tau \norm{Av}). 
   \end{equation*}
   Ponovno iskoristimo robusno svojstvo nul-prostora
   \begin{align*}
       \norm{\vec v}_1 &= \norm{\vec v_{\bar S}}_1 + \norm{\vec v_S}_1 \leq (1 + \rho)\norm{\vec v_{\bar S}}_1 + \tau \norm{\vec{Av}}\\
       & \leq \frac{1+\rho}{1-\rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1) + \frac{2 \tau}{1-\rho}  \norm{\vec{Av}}.
   \end{align*}
\end{proof}

Sada \'cemo pobolj\v{s}ati prethodni rezultat robusnosti, tj. dati \'cemo $\ell_p$ ocjenu gre\v{s}ke za $p \geq 1$. Za to potrebna nam je jo\v{s} jedna varijantna svojstva nul-prostora,
\begin{defn}
    Za $q \geq 1$, matrica $\vec A \in \C^{m \times N}$ zadovoljava $\ell_q$-robusno svojstvo nul-prostora reda $s$ sa konstantama $0 < \rho < 1$ i $\tau > 0$, ako za svaki $S \subseteq [N]$, takav da $\card(S) \leq s$,
    \begin{equation*}
        \norm{\vec v_S}_q \leq \frac{\rho}{s^{1-1/q}}\norm{\vec v_{\bar S}}_1 + \tau \norm{\vec{Av}}\quad \text{za svaki }\vec{v} \in \C^N.
    \end{equation*}
\end{defn}
\noindent Iz $\norm{\vec v_S}_p \leq s^{1/p - 1/q}\norm{\vec v_S}_q$ za $1 \leq p \leq q$, $\ell_1$-robusno svojstvo nul-prostora implicira
\begin{equation*}
    \norm{\vec v_S}_p \leq \frac{\rho}{s^{1-1/p}}\norm{\vec v_{\bar S}}_1 + \tau s^{1/p-1/q}\norm{\vec{Av}} \quad \text{za sve } \vec v \in \C^N.
\end{equation*}
Stoga, za $1 \leq p \leq q$, $\ell_q$-robusno svojstvo nul-prostora implicira $\ell_p$-robusno svojstvo nul-prostora s jednakim konstanama, do na promjenu norme. Sljede\'ci teorem daje robusnost kvadrati\v{c}no ograni\v{c}ene $\ell_1$-minimizacije.
\begin{thm}\label{robusnost_l1_min_kvad_ogr}
    Neka matrica $\vec A \in \C^{m \times N}$  zadovoljava $\ell_2$-robusno svojstvo nul-prostora reda $s$ sa konstanama $0<\rho<1$ i $\tau >0$. Tada za svaki $\vec x \in \C^N$, rje\v{s}enje $\vec x^{\sharp}$ problema \eqref{problem_minimizacije_l1_kvadraticni} aproksimira $\vec x$ s $\ell_p$-gre\v{s}kom
    \begin{equation}
        \norm{\vec x - \vec x^{\sharp}}_p \leq \frac{C}{s^{1-1/p}} \sigma_s(\vec x)_1 + D s^{1/p - 1/2} \eta, \quad 1 \leq p \leq 2,
    \end{equation}
    za neke konstane $C,D > 0$ koje ovise samo o $\rho$ i $\tau$.
\end{thm}
Ovaj teorem je direktna posljedica narednog op\'cenitijeg teorema za $q = 2$ i $\vec z = \vec x^{\sharp}$.
\begin{thm}
    Neka je $1 \leq p \leq q$ i neka matrica $\vec A \in \C^{m \times N}$ zadovoljava $\ell_q$-robusno svojstvo nul-prostora reda $s$ sa konstantama $0 < \rho < 1$ i $\tau > 0$. Tada za svaki $\vec x, \vec z \in \C^N$,
    \begin{equation*}
        \norm{\vec z - \vec x}_p \leq \frac{C}{s^{1-1/p}}(\norm{z}_1 - \norm{\vec x}_1 + 2 \sigma_s(\vec x)_1) + D s^{1/p-1/q} \norm{\vec A (\vec z - \vec x)},
    \end{equation*}
    gdje su $C:=(1+\rho)^2/(1-\rho)$ i $D:=(3+\rho)\tau/(1-\rho)$.
\end{thm}
\begin{proof}
    Iskoristimo prvo da $\ell_q$-robusno svojstvo nul-prostora implicira $\ell_1$-robusno i $\ell_p$-robusno svojstvo nul-prostora, tj.
    \begin{equation}\label{4:18}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 + \tau s^{1-1/q} \norm{\vec{Av}},
    \end{equation}
    \begin{equation}\label{4:19}
        \norm{\vec v_S}_p \leq \frac{\rho}{s^{1-1/p}} \norm{\vec v_{\bar S}}_1 + \tau s^{1/p - 1/q} \norm{\vec{Av}},
    \end{equation}
    za svaki $\vec v \in \C^N$ i za sve $S \subseteq [N]$, takve da je $\card(S) \leq s$. Uva\v{z}avaju\'ci \eqref{4:19} i primjenom teorema \ref{tm:4:20} sa skupom $S$ koji je jednak skupu $s$ apsolutno najve\'cih komponenti vektora $\vec x$, imamo
    \begin{equation}\label{4:20}
        \norm{\vec z - \vec x}_1 \leq \frac{1+\rho}{1-\rho}(\norm{\vec z }_1 - \norm{\vec x}_1 + 2 \sigma_s(\vec x)_1)+ \frac{2 \tau}{1 - \rho}s^{1-1/q} \norm{\vec A (\vec z - \vec x)}.
    \end{equation}
    Nadalje, odabirom skupa S kao skupa $s$ apsolutno najve\'cih komponenti vektora $\vec z - \vec x$, iz teorema \ref{tm:2:5} slijedi
    \begin{equation*}
        \norm{\vec z - \vec x}_p \leq \norm{(\vec z - \vec x)_{\bar S}}_p + \norm{(\vec z - \vec x)_S}_p \leq \frac{1}{s^{1-1/p}}\norm{\vec z - \vec x}_1 + \norm{(\vec z - \vec x)_S}_p.
    \end{equation*}
    Iz \eqref{4:19} imamo,
    \begin{align}\label{4:21}
        \norm{\vec z - \vec x}_p &\leq \frac{1}{s^{1-1/p}} \norm{\vec z - \vec x}_1 + \frac{2}{s^{1-1/p}} \norm{(\vec z - \vec x)_{\bar S}}_1 + \tau s^{1/p - 1/q} \norm{\vec A (\vec z - \vec x)}\nonumber \\
        &\leq  \frac{1+\rho}{s^{1-1/p}}  \norm{\vec z - \vec x}_1 + \tau s^{1/p - 1/q} \norm{ \vec A (\vec z - \vec x)}.
    \end{align}
    Preostaje \eqref{4:20} ubaciti u \eqref{4:21}.
\end{proof}

\section[Rekonstrukcija predodre\dj enog vektora][Rekonstrukcija predodre\dj enog vektora]{Rekonstrukcija predodre\dj enog vektora}
Ukoliko \v{z}elimo rekonstruirati predore\dj eni rijetki vektor $\vec x$ umjesto svih rijetkih vektora s nosa\v{c}em u nekom skupu $S$, potrebno nam je finije svojstvo rekonstrukcije od svojstva nul-prostora. Naglasimo da se \'ce ovdje biti sitna razlika izme\dj u realnog i kompleksnog slu\v{c}aja, \v{s}to je posljedica definicije predznaka broja $z$,
\begin{equation*}
    \sgn(z):= 
    \begin{cases}
        \frac{z}{|z|} \quad &\text{ako } z \neq 0,\\
        0 &\text{ako } z = 0
    \end{cases}
\end{equation*}
i \v{c}injenice da je u realnom slu\v{c}aju to diskretna vrijednost, dok u kompleksnom nije. Za vektor $\vec x \in C^N$, $\sgn(\vec x)\in \C^N$ definiramo kao vektor s komponentama $\sgn(x_j),\ j \in [N]$.
\begin{thm}
    Za danu matricu $\vec A \in \C^{m \times N}$, vektor $\vec x \in \C^N$ s nosa\v{c}em $S$ je jedinstveni minimizator od $\norm{\vec z}_1$ uz uvjet $\vec{Az} = \vec{Ax}$ ako je jedna od narednih, ekvivalentnih tvrdnji zadovoljena:
    \begin{enumerate}[label=(\alph*)]
        \item $|\sum_{j \in S} \overline{\sgn(x_j)}v_j| < \norm{\vec v_{\bar S}}$ za sve $\vec v \in \ker \vec A \backslash \{ \vec 0 \}$, 
            \newpage
        \item $\vec A_S$ je injektivna i postoji vektor $\vec h \in \C^m$ takav da
            \begin{multicols}{2}
                \noindent
                \begin{equation*}
                    (\vec A^* \vec h)_j = \sgn(x_j),\ j \in S,
                \end{equation*}
                \begin{equation*}
                    |(\vec A^* \vec h)_l| < 1,\ l \in \bar S.
                \end{equation*}
            \end{multicols}
    \end{enumerate}
\end{thm}
\begin{proof}
    Doka\v{z}imo prvo da $(a)$ implicira da je $\vec x$ jedinstveni minimizator od $\norm{\vec z}_1$ takav da $\vec{Az} = \vec{Ax}$. Za $\vec z \neq \vec x$ takav da $\vec{Az} = \vec {Ax}$ uzmimo $\vec v := \vec x - \vec z \in \ker \vec A \backslash \{\vec 0\}$
    \begin{align*}
        \norm{\vec z}_1 &= \norm{\vec z_S}_1 + \norm{\vec z_{\bar S} }_1 = \norm{(\vec x - \vec v)_S}_1 + \norm{\vec v_{\bar S}}_1  \\
        &> |\langle \vec x - \vec z, \sgn(\vec x)_S \rangle | + |\langle \vec v, \sgn(\vec x)_S \rangle| \geq |\langle \vec x, \sgn(\vec x)_S  \rangle| = \norm{\vec x}_1.
    \end{align*}
    \indent Poka\v{z}imo sada $(b)\implies(a)$. Koriste\'ci \v{c}injenicu da $\vec{Av}_S = - \vec {Av}_{\bar S}$ za $\vec v \in \ker \vec A \backslash \{\vec 0\}$ slijedi
    \begin{align*}
        |\sum_{j \in S} \overline{\sgn(x_j)v_j}| &= |\langle \vec v_S, \vec A^* \vec h \rangle|  = |\langle \vec{Av}_S, \vec h \rangle| = |\langle \vec{Av}_{\bar S}, \vec h\rangle| \\ 
        &= |\langle \vec v_{\bar S}, \vec{A}^*\vec h \rangle| \leq \max_{l \in \bar S} |(\vec A^* \vec h)_l| \norm{\vec v_{\bar S}}_1 < \norm{\vec v_{\bar S}}_1.
    \end{align*}
    Striktna nejednakost vrijedi jer $\norm{\vec v_{\bar S}} > 0$. U suprotnom bi ne-nul vektor $\vec v \in \ker \vec A$ imao nosa\v{c} u $S$, \v{s}to je kontradikcija s injektivnosti od $\vec A_S$.\\
    \indent Preostaje pokazati $(a) \implies (b)$. Primijetimo da $(a)$ povla\v{c}i $\norm{\vec v_{\bar S}}_1 > 0$ za sve $\vec v \in \ker \vec A \backslash \{\vec 0\}$. Poka\v{z}imo da je $\vec A_S$ injektivna. Pretpostavimo $\vec A_S \vec v_S = \vec 0$ za neki $\vec v_S \neq \vec 0$. Nadopunimo $\vec v_S$ do vektora $\vec v \in \C^N$ tako da stavimo $\vec v_{\bar S} = \vec 0$. Tada je $\vec v \in \ker \vec A \backslash \{\vec 0\}$, \v{s}to je kontradikcija s $\norm{\vec v_{\bar S}}_1 > 0$ za svaki $\vec v \in \ker \vec A \backslash \{\vec 0\}$. Nadalje, primijetimo da je funkcija $\vec v \mapsto |\langle \vec v, \sgn(\vec x)_S \rangle|/ \norm{\vec v_{\bar S}}_1$ neprekidna i da poprima vrijednosti manje od jedan na jedini\v{c}noj kugli u $\ker \vec A$, koja je kompaktan skup. Dakle, maksimum $\eta$ zadovoljava $\eta < 1$ i vrijedi 
    \begin{equation*}
        |\langle \vec v, \sgn(\vec x)_S \rangle|  \leq \mu\norm{\vec v_{\bar S}}_1 \quad \text{za sve } \vec v \in \ker \vec A.
    \end{equation*}
    Za $\eta < \nu < 1$ definiramo konveksni skup $\mathcal{C}$ i afini skup $\mathcal{D}$,
    \begin{align*}
        \mathcal{C} &:= \{ \vec z \in \C^N : \norm{\vec z_S}_1 + \nu \norm{\vec z_{\bar S}}_1 \leq \norm{\vec x}_1 \},\\
        \mathcal{D} &:= \{ \vec z \in \C^N : \vec{Az} = \vec{Ax} \}.
    \end{align*}
    Poka\v{z}imo da je $\mathcal{C} \cap \mathcal{D} = \{\vec x\}$. Uzmimo $\vec x \in \mathcal{C} \cap \mathcal{D}$. Za $ \vec z \in \ker \vec A \backslash \{\vec 0, \vec x  \}$ kontradikcija slijedi iz
    \begin{align*}
        \norm{\vec x}_1 &\geq \norm{\vec z_S}_1 + \nu \norm{\vec z_{\bar S}}_1 = \norm{(\vec x - \vec z)_S}_1 + \nu \norm{\vec v_{\bar S}}_1\\
        &> \norm{(\vec x - \vec v)_S}_1 + \mu \norm{\vec v_{\bar S}}_1 \geq |\langle \vec x - \vec v, \sgn(\vec x)_S \rangle| + |\langle \vec v, \sgn(\vec x)_S \rangle|\\
        &\geq |\langle \vec x, \sgn(\vec x)_S \rangle| = \norm{\vec x}_1.
    \end{align*}
    Dakle, prema teoremu o separaciji konveksnih skupova hiperplohama (vidi teorem B.4 u \cite{foucart13}), postoji vektor $\vec w \in \C^N$ takav da
    \begin{align}
        \mathcal{C} \subseteq \{ \vec z \in \C^N: \Re \langle \vec z, \vec w \rangle \leq \norm{\vec x}_1 \}\label{4:22},\\
        \mathcal{D} \subseteq \{ \vec z \in \C^N: \Re \langle \vec z, \vec w \rangle = \norm{\vec x}_1 \}\label{4:23}.
    \end{align}
    Iz \eqref{4:22} slijedi
    \begin{align*}
        \norm{\vec x}_1 &\geq \max_{\norm{\vec z_S + \nu \vec z_{\bar S}}_1 \leq \norm{\vec x}_1} \Re \langle \vec z, \vec x\rangle \\
        &=\max_{\norm{\vec z_S + \nu \vec z_{\bar S}}_1 \leq \norm{\vec x}_1} \Re \bigg( \sum_{j \in S} z_j \overline{w_j} + \sum_{j \in \bar S} \nu z_j \overline{w_j}/ \nu \bigg)\\
        &= \max_{\norm{\vec z_S + \nu \vec z_{\bar S}}_1 \leq \norm{\vec x}_1} \Re \langle \vec z_S + \nu \vec z_{\bar S}, \vec w_{\bar S} + (1/\nu) \vec w_{\bar S} \rangle \\
        &= \norm{\vec x}_1 \norm{\vec w_S + (1/\nu)\vec w_{\bar S}}_{\infty} = \norm{\vec x}_1 \max\{ \norm{\vec w_S}_{\infty}, (1/\nu) \norm{\vec w_{\bar S}}_{\infty} \}.
    \end{align*}
    U slu\v{c}aju $\vec x = \vec 0$, dovoljno je uzeti vektor $\vec h = \vec 0$, stoga neka je $\vec x \neq \vec 0$. Gornja nejednakost daje $\norm{\vec w_S}_{\infty} \leq 1$ i $\norm{\vec w_{\bar S}}_{\infty} \leq \nu < 1$. Iz \eqref{4:23} slijedi $\Re \langle \vec x, \vec w\rangle = \norm{\vec x}_1$, tj. $w_j = \sgn(x_j)$ za sve $j \in S$, te $\Re \langle \vec v, \vec w \rangle = 0$ za sve $\vec v \in \ker \vec A$, tj. $\vec w \in (\ker \vec A)^{\perp}$. Po\v{s}to je $(\ker \vec A)^{\perp} = \im \vec A^*$, imamo $\vec w = \vec A^* \vec h$ za neki $\vec h \in \C^m$.
\end{proof}
U realnom slu\v{c}aju obratna tvrdnja tako\dj er vrijedi, dok op\'cenito to nije istina. Dati \'cemo jo\v{s} jednu karakteriziciju egzaktne rekonstrukcije $\ell_1$-minimizacijom u realnom slu\v{c}aju. Za vektor $\vec x \in \R^N$, \textit{konveksni konus} definiramo kao
\begin{equation}\label{4:34}
    T(\vec x) = \cone \{ \vec z - \vec x : \vec z \in \R^N,\ \norm{\vec z}_1 \leq \norm{\vec x}\} 
\end{equation}
gdje $\cone(\cdot)$ predstavlja konusnu ljusku: za $S = \{x_1, x_2, \dots, x_n\}$ 
\begin{equation*}
    \cone(S) = \big\{\sum_{i=1}^{n} \alpha_i x_i: \alpha_i \geq 0,\ i \in [n]\big\}
\end{equation*}

\begin{thm}
    Za matricu $\vec A \in \R^{m \times N}$, vektor $\vec x \in \R^N$ je jedinstveni minimizator od $\norm{\vec z}_1$ takav da $\vec{Az} = \vec{Ax}$ ako i samo ako $\ker \vec A \cap T(\vec x) = \{\vec 0\}$.
\end{thm}

\begin{proof}
    Pretpostavimo da je $\ker \vec A \cap T(x) = \{\vec 0\}$. Neka je $\vec x^{\sharp}$ $\ell_1$-minimizator. Imamo, $\norm{\vec x^{\sharp}}_1 \leq \norm{\vec x}_1$ i $\vec A \vec x^{\sharp} = \vec{Ax}$, pa je $\vec v := \vec x^{\sharp} - \vec x \in T(\vec x) \cap \ker \vec A = \{\vec 0\}$. Stoga je $\vec x^{\sharp} = \vec x$. Dakle, $\vec x$ je jedinstveni $\ell_1$-minimizator.\\
    \indent
    Obratno, neka je $\vec x$ jedinstveni $\ell_1$-minimizator. Vektor $\vec v \in T(\vec x) \backslash \{\vec 0\}$ mo\v{z}emo zapisati kao $\vec v = \sum t_j (\vec z_j - \vec x)$ gdje je $t_j \geq 0$ i $\norm{\vec z_j} \leq \norm{\vec x}_1$. Da je $\vec v \in \ker \vec A$, vrijedilo bi $\vec A (\sum t_j^{'} \vec z_j) = \vec{Ax}$ i $\norm{\sum t_j^{'} \vec z_j}_1 \leq \sum t_j^{'} \norm{\vec z_j}_1 \leq \norm{\vec x}_1$. Zbog jedinstvenosti, to bi zna\v{c}ilo da $\sum t_j^{'} \vec z_j = \vec x$ pa bi $\vec v = \vec 0$, \v{s}to je kontradikcija. Dakle, vrijedi $(T(\vec x) \backslash \{\vec 0\}) \cap \ker \vec A = \emptyset$. 
\end{proof}
Ovaj rezultat mo\v{z}emo pro\v{s}iriti i na robusnu rekonstrukciju.
\begin{thm}
    Za $\vec A \in \R^{m \times N}$, neka je $\vec x \in \R^N$ i $\vec y = \vec{Ax} + \vec e \in \R^m$ i $\norm{\vec e}_2 \leq \eta$. Ako je
    \begin{equation*}
        \inf_{\vec v \in T(x),\ \norm{\vec v}_2 = 1} \norm{\vec{Av}}_2 \geq \tau
    \end{equation*}
    za neki $\tau > 0$, tada minimizator $\vec x^{\sharp}$ od $\norm{\vec z}_1$ takav da $\norm{\vec{Az} - \vec y}_2 \leq \eta$ zadovoljava
    \begin{equation}\label{4:35}
        \norm{\vec x - \vec x^{\sharp}}_2 \leq \frac{2 \eta}{\tau}. 
    \end{equation}
\end{thm}
\begin{proof}
    Iz $\norm{ \vec x^{\sharp}}_1 \leq \norm{\vec x}_1$ slijedi da je $\vec v := (\vec x^{\sharp} - \vec x)/ \norm{\vec x^{\sharp} - \vec x}_2 \in T(x)$ pa mo\v{z}emo pretpostaviti da je $\vec x^{\sharp} - \vec{x} \neq \vec 0$. Po\v{s}to je $\norm{\vec v}_2 = 1$ imamo da je $\norm{\vec{Av}}_2 \geq \tau$, tj. $\norm{\vec A (\vec x^{\sharp} - \vec x)}_2 \geq \tau \norm{\vec x^{\sharp} - \vec x}_2$. Nadalje, vrijedi
    
    \begin{equation*}
        \norm{\vec A (\vec x^{\sharp} - \vec x)}_2 \leq \norm{\vec{Ax}^{\sharp} - \vec y}_2 + \norm{\vec{Ax} - \vec y}_2 \leq 2 \eta.
    \end{equation*}
    Tvrdnja slijedi kombiniranjem prethodne dvije nejednakosti.
\end{proof}

\chapter[Koherencija][Koherencija]{Koherencija}
Kao \v{s}to smo vidjeli, uspje\v{s}nost rekonstrukcije rijetkog vektora u kontekstu sa\v{z}etog uzorkovanja ovisi o odre\dj enim kvalitetama matrice mjerenja. Jedna od takvih mjera kvalitete je koherencija. Neformalno, \v{s}to je koherencija matrice mjerenja manja, to je rekonstrukcija uspje\v{s}nija.  

\section[Definicija i svojstva][Definicija i svojstva]{Definicija i svojstva}
U cjelom poglavlju podrazumjevamo da su stupci matrice mjerenja $\ell_2$-normalizirani.
\begin{defn}
    Neka je $\vec A \in \C^{m \times N}$ matrica s $\ell_2$-normaliziranim stupcima $\vec a_1, \vec a_2, \dots, \vec a_N$, tj. $\norm{\vec a_i}_2 = 1$ za sve $i \in [N]$. Koherenciju $\mu = \mu(\vec A)$ matrice $\vec A$ definiramo kao
    \begin{equation}\label{5:1}
        \mu := \max_{1 \leq i \neq j \leq N} |\langle \vec a_i, \vec a_j \rangle| .
    \end{equation}
\end{defn}

Nadalje, uvodimo op\'cenitiji pojam $\ell_1$-koherencije. Gornja definicija je poseban slu\v{c}aj za $s = 1$.
\begin{defn}
    Neka je matrica $\vec A \in \C^{m \times N}$ s $\ell_2$-normaliziranim stupcima  $\vec a_1, \vec a_2, \dots, \vec a_N$. Za $s \in [N-1]$, funkcija $\ell_1$-koherencije $\mu_1$ matrice $\vec A$ je definirana kao
    \begin{equation*}
        \mu_1(s) := \max_{i \in [N]} \max \big\{ \sum_{j \in S} |\langle \vec a_i, \vec a_j \rangle|,\ S \subseteq [N],\ \card(S) = s,\ i \not \in S   \big\} .
    \end{equation*}
\end{defn}
Jasno je da za $1 \leq s \leq N-1$ vrijedi
\begin{equation}\label{5:2}
    \mu \leq \mu_1(s) \leq s \mu
\end{equation}
i op\'cenitije za $1 \leq s,\ t \leq N-1$ takve da je $s+t \leq N-1$
\begin{equation}\label{5:3}
    \max \{\mu_1(s), \mu_1(t)\}  \leq \mu_1(s+t) \leq \mu_1(s) + \mu_1(t).
\end{equation}
Primijetimo da je $\ell_1$-koherencija pa stoga i koherencija invarijanta na mno\v{z}enje s lijeva unitarnom matricom $\vec U$. Zaista, stupci od $\vec{UA}$ su $\ell_2$-normalizirani vektori $\vec{Ua}_1, \dots, \vec{Ua}_N$ te zadovoljavaju $\langle \vec{Ua}_i, \vec{Ua}_j \rangle = \langle \vec a_i , \vec a_j \rangle$. Nadalje, zbog Cauchy-Schwarzove nejednakosti vrijedi
\begin{equation*}
   \mu \leq 1. 
\end{equation*}
Neka je na matrica $\vec A \in \C^{m \times N}$ takva da $m \geq N$. Tada je $\mu = 0$ ako i samo ako stupci matrice $\vec A$ formiraju ortonormirani sustav. U slu\v{c}aju da je matrica kvadratna, $\mu = 0$ ako i samo ako je $\vec A$ unitarna. U nastavku \'cemo prou\v{c}avati samo matrice kojima je $m < N$. U tom slu\v{c}aju vrijednost koherencije je odozdo ograni\v{c}ena, \v{s}to \'cemo kasnije i pokazati. 

\begin{thm}\label{tm:5:3}
    Neka je $\vec A \in \C^{m \times N}$ matrica s $\ell_2$-normaliziranim stupcima i neka je $s \in [N]$. Za sve $s$-rijetke vektore $\vec x \in \C^N$ vrijedi,
    \begin{equation*}
        \big(1-\mu_1(s-1)\big) \norm{\vec x}_2^2 \leq \norm{\vec{Ax}}_2^2 \leq \big(1+\mu_1(s-1)\big) \norm{\vec x}_2^2
    \end{equation*}
    ili ekvivalentno, za svaki skup $S \subseteq [N]$ takav da $\card(S) \leq s$, svojstvene vrijednosti matrice $\vec A^*_S \vec A_S$ le\v{z}e u segmentu $[1-\mu_1(s-1),\ 1+\mu(s-1)]$. Posebno, ako je $\mu_1(s-1) < 1$ tada je $\vec A^*_S \vec A_S$ invertibilna.
\end{thm}
\begin{proof}
    Neka je $S \subseteq [N]$. Po\v{s}to je matrica $\vec A^*_S \vec A_S$ pozitivno semidefinitna, svojstveni vektori koji odgovaraju realnim pozitivnim svojstvenim vrijednostima \v{c}ine ortonormiranu bazu. Ozna\v{c}imo s $\lambda_{min}$ najmanju i s $\lambda_{max}$ najve\'cu svojstvenu vrijednost. Po\v{s}to je $\vec{Ax} = \vec A_S \vec x_S$ za svaki $\vec x \in \C^N$ sa nosa\v{c}em na skupu $S$, slijedi da je maksimum od
    \begin{equation*}
        \norm{\vec{Ax}}_2^2 = \langle \vec A_S \vec x_S, \vec A_S \vec x_S  \rangle = \langle \vec A^*_S \vec A_S \vec x_S, \vec x_S \rangle
    \end{equation*}
    po skupu $\{\vec x \in \C^N,\ \supp (\vec x) \subseteq S,\ \norm{\vec x}_2 = 1\}$, jednak $\lambda_{max}$ i minimum jednak $\lambda_{min}$. Ovo pokazuje ekvivalenciju dvije tvrdnje u teoremu. Nadalje, po\v{s}to imamo da je $\norm{\vec a_j}_2 = 1$ za sve $j \in [N]$, svi dijagonalni elementi matrice $\vec A^*_S \vec A_S$ jednaki su jedan. Prema Gershgorinom teoremu (vidi \cite{gerschgorin31}, A.11 \cite{foucart13}), svojstvene vrijednost od $\vec A^*_S \vec A_S$ sadr\v{z}ane su u uniji diskova s centrom u 1 radijusa
    \begin{equation*}
        r_j := \sum_{l \in S,\ l \neq j} |(\vec A^*_S \vec A_S)_{j,l}| = \sum_{l \in S,\ l \neq j} |\langle \vec a_l, \vec a_j \rangle| \leq \mu_1 (s-1),\quad j \in S.
    \end{equation*}
    \newpage
    Po\v{s}to su svojstvene vrijednost realne, moraju le\v{z}ati u segmentu $[1-\mu_1(s-1), 1+ \mu_1(s-1)]$.
\end{proof}
\begin{cor}\label{kor:5:4}
    Neka je $\vec A \in \C^{m \times N}$ s $\ell_2$-normaliziranim stupcima i neka je $s \geq 1$. Ako  
    \begin{equation*}
        \mu_1(s) + \mu_1(s-1) < 1, 
    \end{equation*}
    onda je, za svaki $S \subseteq [N]$ takav da $\card(S) \leq 2s$, matrica $\vec A^*_S \vec A_S$ invertibilna i matrica $\vec A_S$ injektivna. Posebno, isti zaklju\v{c}ak vrijedi ako je
    \begin{equation*}
        \mu < \frac{1}{2s - 1}. 
    \end{equation*}
\end{cor}
\begin{proof}
    Iz \eqref{5:3}, $\mu_1(s) + \mu_1(s-1) < 1$ povla\v{c}i $\mu_1(2s-1) < 1$. Prema prethodnom teoremu, za $S \subseteq [N]$ takav da $\card(S) = 2s$, najmanja svojstvena vrijednost matrice $\vec A^*_S \vec A_S$ zadovoljava $\lambda_{min} \geq 1 - \mu_1(2s-1)>0$. Dakle, $\vec A^*_S \vec A_S$ je invertibilna. Ako je $\vec A_S \vec z = \vec 0$ tada je i $\vec A^*_S \vec A_S \vec z = \vec 0$ ali to implicira $\vec z = \vec 0$. Dakle, $\vec A_S$ je injektivna. Isti zaklju\v{c}ak slijedi iz $\mu_1(s) + \mu_1(s-1) \leq (2s-1)\mu < 1$ ako je $\mu < 1/(2s-1)$.
\end{proof}

\section[Matrice male koherencije][Matrice male koherencije]{Matrice male koherencije}
Sada \'cemo prou\v{c}iti ocjene odozdo na koherenciju i na $\ell_1$-koherenciju matrice $\vec A \in \K^{m \times N}$ takve da je $m < N$ i gdje je $\K = \R$ ili $\K = \C$.
\begin{defn}
    Za sustav $\ell_2$-normaliziranih vektora $(\vec a_1, \dots, \vec a_N)$ iz $\K^m$ ka\v{z}emo da je ekviangularan ako postoji konstana $c \leq 0$ takva da
    \begin{equation*}
        |\langle \vec a_i, \vec a_j \rangle|  = c \quad \text{za sve } i,j \in [N],\ i \neq j.
    \end{equation*}
\end{defn}
\begin{defn}
    Sustav  vektora $(\vec a_1, \dots, \vec a_N)$ iz $\K^m$ zovemo napeti bazni okvir ako postoji konstanta $\lambda > 0$ takva da vrijedi jedan od ekvivalentnih uvjeta:
    \begin{enumerate}[label=(\alph*)]
        \item $\norm{\vec x}_2^2 = \lambda \sum_{j=1}^N |\langle \vec x, \vec a_j \rangle|^2$ za sve $\vec x \in \K^m$,
        \item $\vec{x} = \lambda \sum_{j=1}^N \langle \vec x, \vec a_j\rangle \vec a_j$  za sve $\vec x \in \K^m$,
        \item $\vec{AA}^* = (1/\lambda) \vec I_m$, gdje je $\vec A$ matrica sa stupcima $\vec a_1, \dots \vec a_N$.
    \end{enumerate}
\end{defn}
Sustav $\ell_2$-normaliziranih vektora zove se ekviangularni napeti bazni okvir ako je ujedno ekviangularni sustav vektora i napeti bazni okvir. Takvi sustavi vektora posti\v{z}u takozvanu \textit{Welchovu ocjenu}.
\begin{thm}\label{tm:5:7}
    Koherencija matrice $\vec A \in \K^{m \times N}$ s $\ell_2$-normaliziranim stupcima zadovoljava 
    \begin{equation}\label{5:4}
        \mu \geq \sqrt{\frac{N-m}{m(N-1)}}. 
    \end{equation}
    Jednakost vrijedi ako i samo ako stupci $\vec a_1, \dots \vec a_N$ matrice $\vec A$ \v{c}ine ekviangularni napeti bazni okvir.
\end{thm}
\begin{proof}
    $\vec G := \vec A^* \vec A \in \K^{N \times N}$ zovemo \textit{Gramova matrica} sustava vektora $(\vec a_1, \dots, \vec a_N)$. Elementi od $G$ su obika
    \begin{equation*}
        G_{i,j} = \overline{ \langle \vec a_i, \vec a_j \rangle}  =  \langle \vec a_j, \vec a_i \rangle, \quad i,j \in [N].
    \end{equation*}
    Nadalje, definirajmo matricu $\vec H := \vec{AA}^* \in \K^{m \times m}$. Po\v{s}to su stupci od $\vec A$ $\ell_2$-normalizirani, imamo
    \begin{equation}\label{5:5}
        \tr(\vec G) = \sum_{i = 1}^{N} \norm{\vec a_i}_2^2 = N.
    \end{equation}
    Po\v{s}to skalarni produkt
    \begin{equation*}
        \langle \vec U, \vec V \rangle_F := \tr(\vec{UV}^*) = \sum_{i,j = 1}^{n} U_{i,j}\overline{V_{i,j}}
    \end{equation*}
    inducira \textit{Froebeniusovu normu} $\norm{ \cdot }_F$ na $\K^{n \times n}$ (vidi A.16 \cite{foucart13}), Cauchy-Schwarzova nejednakost daje
    \begin{equation}\label{5:6}
        \tr(\vec H) = \langle \vec H, \vec I_m \rangle_F \leq \norm{\vec H}_F \norm{\vec I_m}_F = \sqrt{m} \sqrt{\tr(\vec{HH}^*)}.
    \end{equation}
    Nadalje,
    \begin{align}\label{5:7}
        \tr(\vec{HH}^*) &= \tr(\vec{AA}^* \vec{AA}^*) = \tr(\vec A^* \vec{AA}^* \vec A) = tr(\vec{GG}^*) = \sum_{i,j = 1}^N |\langle \vec a_i, \vec a_j \rangle|^2\nonumber\\ 
        &= \sum_{i=1}^N \norm{\vec a_i}_2^2 + \sum_{i,j = 1,\ i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2 = N + \sum_{i,j = 1,\ i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2.
    \end{align}
    Iz \v{c}injenice da $\tr(\vec G) = \tr(\vec H)$, te kombiniranjem \eqref{5:5}, \eqref{5:6} i \eqref{5:7} imamo
    \begin{equation}\label{5:8}
        N^2 \leq m \big( N + \sum_{i,j = 1,\ i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2 \big)
    \end{equation}
    Napokon, uva\v{z}imo da 
    \begin{equation}\label{5:9}
        |\langle \vec a_i, \vec a_j \rangle| \leq \mu \quad \text{za sve }i,j \in [N],\ i \neq j,
    \end{equation}
    pa slijedi
    \begin{equation*}
        N^2 \leq m \big( N + (N^2 - N)\mu^2 \big), 
    \end{equation*}
    od kuda lako slijedi ocjena iz tvrdnje teorema.
    Nadalje, u \eqref{5:4} nastupa jednakost ako vrijede jednakosti u \eqref{5:6} i \eqref{5:9}. Jednakost u \eqref{5:6} daje $\vec H = \lambda \vec I_m$ za neku nenegativnu konstantu $\lambda$, tj. sustav $(\vec a_1, \dots, \vec a_N)$ je napeti bazni okvir. Iz jednakost u \eqref{5:9} slijedi da je taj sustav ekviangularan.
\end{proof}

Welchovu ocjenu mo\v{z}emo pro\v{s}iriti i na funkciju $\ell_1$-koherencije.
\begin{thm}\label{tm:5:8}
    Funkcija $\ell_1$-koherencije matrice $\vec A \in \K^{m \times N}$ s $\ell_2$-normaliziranim stupcima zadovoljava 
    \begin{equation}\label{5:10}
        \mu_1(s) \geq s \sqrt{\frac{N-m}{m(N-1)}}\quad \text{za } s < \sqrt{N - 1}.
    \end{equation}
    Jednakost se posti\v{z}e ako i samo stupci matrice $\vec A$ formiraju ekviangularni napeti bazni okvir.
\end{thm}
Za dokaz biti \'ce nam potrebna sljede\'ca lema,
\begin{lem}\label{lem:5:9}
    Za $k < \sqrt{n}$, ako kona\v{c}ni niz brojeva $(\alpha_1, \alpha_2, \dots, \alpha_n)$ zadovoljava
    \begin{equation*}
        \alpha_1 \geq \alpha_2 \geq \cdots \alpha_n \geq 0 \quad \text{i} \quad \alpha_1^2,\alpha_2^2, \cdots,\alpha_n^2 \geq \frac{n}{k^2} 
    \end{equation*}
    tada
    \begin{equation*}
        \alpha_1 + \alpha_2 + \cdots + \alpha_k \geq 1, 
    \end{equation*}
    gdje se jednakost posti\v{z}e ako i samo ako $\alpha_1 = \cdots = \alpha_n = 1/k$.
\end{lem}
Ideja dokaza je analogna dokazu teorema \ref{tm:2:5}, tj. problem se svodi na maksimizaciju konveksne funkcije (vidi lema 5.9 u \cite{foucart13}).

\begin{proof}[Dokaz (Teorem \ref{tm:5:8})]
    Iz \eqref{5:8} imamo 
    \begin{equation*}
        \sum_{i,j=1,i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2 \geq \frac{N^2}{m} - N = \frac{N(N-m)}{m},
    \end{equation*}
    odakle slijedi
    \begin{equation*}
        \max_{i \in [N]} \sum_{i,j=1,i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2 \geq \frac{1}{N} \sum_{i,j=1,i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2 \geq \frac{N-m}{m} .
    \end{equation*}
    Neka je $i^* \in [N]$ indeks za koji se posti\v{z}e maksimum. Sortirajmo niz $(|\langle \vec a_{i^*}, \vec a_j \rangle|)_{j=1}^{N}$ kao $\beta_1 \geq \beta_2 \geq \cdots \beta_{N-1} \geq 0$, tako da
    \begin{equation*}
        \beta_1^2 + \beta_2^2 + \cdots + \beta_{N-1}^2 \geq \frac{N-m}{m} .
    \end{equation*}
    Primjenom prethodne leme s $n = N-1,\ k = s$, i $\alpha_l := (\sqrt{m(N-1)/(N-m)}/s)\beta_l$ dobivamo $\alpha_1 + \cdots + \alpha_s \geq 1$. Dakle,
    \begin{equation*}
        \mu_1(s) \geq \beta_1 + \beta_2 + \cdots + \beta_s \geq s \sqrt{\frac{N-m}{m(N-1)}}.
    \end{equation*}
    Pretpostavimo sada da u \eqref{5:10} vrijedi jednakost, pa su sve nejednakosti zapravo jednakosti. Jednakost u \eqref{5:8} implicira da su stupci matrice $\vec A$ napeti bazni okvir. Jednakost u prethodnoj lemi implicira da $|\langle \vec a_{i^*}, \vec a_j \rangle| = \sqrt{(N-m)/(m(N-1))}$ za sve $j \in [N]$, takve da je $j \neq i^*$. Po\v{s}to indeks $j$ mo\v{z}emo proizvoljno odabrati iz $[N] \backslash \{i^*\}$, slijedi da je sustav stupaca matrice $\vec A$ ekviangularan. Obrat lako slijedi iz teorema \ref{tm:5:7} i \eqref{5:2}.
\end{proof}

U kontekstu sa\v{z}etog uzorkovanja zanimaju nas $m \times N$ matrice gdje je $N$ puno ve\'ci od $m$. No, pokazati \'cemo da u tom slu\v{c}aju ne mo\v{z}emo posti\'ci Welchovu ocjenu.
\begin{thm}\label{tm:5:10}
    Kardinalitet $N$ ekviangularnog sustava $(\vec a_1, \dots, \vec a_N)$ $\ell_2$-normaliziranih vektora u $\K^m$ zadovoljava
    \begin{equation*}
        &N \leq 
        \begin{cases}
            \frac{m(m+1)}{2} \quad  & \text{za } \K = \R,\\ 
            m^2 \quad & \text{za } \K = \C.
        \end{cases} 
    \end{equation*}
    Ako vrijedi jednakost onda je sustav $(\vec a_1, \dots, \vec a_N)$ tako\dj er i napeti bazni okvir.
\end{thm}
\newpage
Za dokaz teorema potrebna nam je sljede\'ca tvrdnja, koja se lagano provjeri.
\begin{lem}\label{lem:5:11}
    Neka je $z \in \C$, matrica 
    \begin{equation*}
        \begin{bmatrix*}
            1 & z & z & \cdots & z \\
            z & 1 & z & \cdots & z \\
            \vdots & \ddots & \ddots & \ddots & \vdots \\ 
            z & \cdots & z & 1 & z \\
            z & \cdots & z & z & 1 
        \end{bmatrix*}
    \end{equation*}
    ima jednostruku svojstvenu vrijednost $1+(n-1)z$ te svojstvenu vrijednost $1-z$ algebarske kratnosti $n-1$.
\end{lem}
\begin{proof}[Dokaz (Teorem \ref{tm:5:10})]
    Ideja je razmatranja s prostora $\K^m$ prebaciti na potprostor $\mathcal{S}_m$ operatora na $\K^m$. U slu\v{c}aju $\K = \R$, $\mathcal{S}_m$ je prostor simetri\v{c}nih operatora na $\R^m$, a u slu\v{c}aju $\K = \C$, $\mathcal{S}_m$ je cijeli prostor operatora na $\C^m$. Ti su prostori opremljeni Froebeniusovim skalarnim produktom
    \begin{equation}
        \langle \vec P, \vec Q \rangle_F = \tr(\vec{PQ}^*) 
    \end{equation}
    za $\vec P$, $\vec Q \in \mathcal{S}_m$.
    Ozna\v{c}imo s $\vec P_1, \dots, \vec P_N \in \mathcal{S}_m$ orthogonalne projektore na potprostore razapete sa $\{\vec a_i\}$ za $i=1,2,\dots,N$, definirane s
    \begin{equation*}
        \vec P_i(\vec v) = \langle \vec v, \vec a_i \rangle \vec a_i
    \end{equation*}
    za $\vec v \in \K^m$. Nadalje, neka je $c:=|\langle \vec a_i, \vec a_j \rangle|$ za $i \neq j$ te neka je $(\vec e_1, \dots, \vec e_N)$ kanonska baza za $\K^m$. Koriste\'ci \v{c}injenicu da je $\vec P_i^2 = \vec P_i = \vec P^*$, za $i,j \in [N],\ i \neq j$ ra\v{c}unamo
    \begin{align*}
        \langle \vec P_i, \vec P_i \rangle_F &= \tr(\vec P_i \vec P_i^*) = \tr(\vec P_i) = \sum_{k=1}^{m} \langle \vec P_i(\vec e_k), \vec e_k \rangle_F = \sum_{k=1}^{m} \langle \vec e_k, \vec a_i \rangle \langle \vec a_i, \vec e_k \rangle \\
    &= \sum_{k=1}^m |\langle \vec a_i, \vec e_k \rangle|^2 = \norm{\vec a_i}_2^2 = 1, \\
        \langle \vec P_i, \vec P_j \rangle_F &= \tr(\vec P_i \vec P_j^*) = \tr(\vec P_i \vec P_j) = \sum_{k=1}^m \langle \vec P_i \vec P_j (\vec e_k), \vec e_k \rangle = \sum_{k=1}^m \langle \vec P_j(\vec e_k), \vec P_i(\vec e_k) \rangle \\
        &= \sum_{k=1}^m \langle \vec e_k, \vec a_j \rangle \overline{\langle \vec e_k, \vec a_i \rangle}\langle \vec a_j, \vec a_i \rangle = \overline{\langle \vec a_i, \vec a_j \rangle} \big \langle \sum_{k=1}^m \langle \vec a_i, \vec e_k \rangle \vec e_k, \vec a_j  \big \rangle\\
        &= \overline{\langle \vec a_i, \vec a_j \rangle} \langle \vec a_i, \vec a_j \rangle = |\langle \vec a_i, \vec a_j \rangle|^2 = c^2.
    \end{align*}
    Dakle, Gramova matrica sustava $(\vec P_1, \dots, \vec P_N)$ je $N \times N$ matrica oblika
    \begin{equation*}
        \begin{bmatrix*}
            1 & c^2 & c^2 & \cdots & c^2 \\
            c^2 & 1 & c^2 & \cdots & c^2 \\
            \vdots & \ddots & \ddots & \ddots & \vdots \\ 
            c^2 & \cdots & c^2 & 1 & c^2 \\
            c^2 & \cdots & c^2 & c^2 & 1 
        \end{bmatrix*}
    \end{equation*}
    Iz \v{c}injenice $0 \leq c^2 < 1$ i leme \ref{lem:5:11} slijedi da je ova Gramova matrica invertibilna, \v{s}to zna\v{c}i da je sustav $(\vec P_1, \dots, \vec P_N)$ linearno nezavisan. Taj sustav le\v{z}i u prostoru $\mathcal{S}_m$ koji je dimenzije $m(m+1)/2$ za $\K = \R$ te dimenzije $m^2$ za $\K = \C$. Stoga vrijedi,

    \begin{equation*}
        N \leq \frac{m(m+1)}{2} \quad \quad \text{za } \K = \R,\quad \quad \quad \quad  N \leq m^2 \quad \quad \text{za } \K = \C.
    \end{equation*}
    Pretpostavimo sada da vrijedi jednakost. Tada je sustav $(\vec I_m, \vec P_1, \dots, \vec P_N)$ linearno zavisan, pa je stoga
    \begin{equation*}
        \begin{vmatrix*}
            1 & b & b & \cdots & b \\
            b & 1 & b & \cdots & b \\
            \vdots & \ddots & \ddots & \ddots & \vdots \\ 
            b & \cdots & b & 1 & b \\
            b & \cdots & b & b & 1 
        \end{vmatrix*} = 0, \quad \quad \text{gdje je } b:= \frac{mc^2 - 1}{m - 1}. 
    \end{equation*}
    Po\v{s}to je $1-b = m(1-c^2)/(m-1) \neq 0$, lema \ref{lem:5:11} implicira da je $1+(N-1)b = 0$. Slijedi, 
    \begin{equation*}
        c^2 = \frac{N-m}{m(N-1)}.  
    \end{equation*}
    Dakle, pokazali smo da $\ell_2$-normalizirani sustav $(\vec a_1, \dots, \vec a_N)$ posti\v{z}e Welchovu ocjenu a teorem \ref{tm:5:7} implicira da je taj sustav onda ekviangularan napeti okvir.
\end{proof}

Zanimljvo je da u kontekstu prostora $\C^m$ postoje sustavi od $m^2$ ekviangularnih vektora za sve $m$, dok u $\R^m$ sustavi od $m(m+1)/2$ ekviangularnih vektora ne postoje za sve $m$. Poznato je da postoje u slu\v{c}ajevima gdje je $m$ jednak $2,3,7$ i $23$. Pitanje ostalih slu\v{c}ajeva je i dalje otvoreno.
 
\begin{thm}
    Za $m \geq 3$, ako postoji ekviangularni sustav od $m(m+1)/2$ vektora u $\R^m$, tada je $m+2$ nu\v{z}no kvadrat nekog neparnog prirodnog broja.
\end{thm}
\begin{proof}
    Neka je $(\vec a_1, \dots \vec a_N)$ sustav od $N = m(m+1)/2$ ekviangularnih $\ell_2$-normaliziranih vektora. Prema teoremu \ref{tm:5:10} taj je sustav napeti bazni okvir, pa stoga matrica $\vec A$ sa stupcima $\vec a_1, \dots, \vec a_N$ zadovoljava $\vec{AA}^* = \lambda \vec I_m$ za neki $\lambda > 0$. Matrica $\vec G := \vec A^* \vec A$ ima iste ne-nul svojstvene vrijednosti kao i $\vec{AA}^*$, tj. svojstvenu vrijednost $\lambda$ algebarske kratnosti $m$ i svojstvenu vrijednost nula kratnosti $N-m$. Nadalje, po\v{s}to je $\vec G$ Gramova matrica sustava $(\vec a_1, \dots, \vec a_N)$, njezini dijagonalni elementi jednaki su jedinici, dok su svi vandijagonalni elementi jednaki po apsolutnoj vrijednosti nekom broju $c$. Dakle, matrica $\vec B := (\vec G - \vec I_N)/c$ je oblika
    \begin{equation*}
        \begin{bmatrix*}
            0 & b_{1,2} & b_{1,3} & \cdots & b_{1,N} \\
            b_{2,1} & 0 & b_{2,2} & \cdots & b_{2,N} \\
            \vdots & \ddots & \ddots & \ddots & \vdots \\ 
            b_{N-1,1} & \cdots & b_{N-1, N-2} & 0 & b_{N-1,N} \\
            b_{N,1} & \cdots & b_{N,N-2} & b_{N,N-1} & 0 
        \end{bmatrix*} = 0, \quad \quad \text{gdje je } b_{i,j}:= \pm 1,
    \end{equation*}
    i ima $-1/c$ kao svojstvenu vrijednost kratnosti $N-m$. Stoga je njezin karakteristi\v{c}ni polinom $p_{\vec B}(x) := \sum_{k = 0}^N \beta_k (-x)^k, \beta_N = 1$, s cjelobrojnim koeficijentima $\beta_k$ i poni\v{s}tava se za $-1/c$. Uva\v{z}avaju\'ci da je
    \begin{equation*}
        c = \sqrt{\frac{N-m}{m(N-1)}} = \sqrt{\frac{(m+1)/2-1}{m(m+1)/2 -1}} = \sqrt{\frac{m-1}{m^2 + m - 2}} = \frac{1}{\sqrt{m+2}}
    \end{equation*}
    imamo $p_{\vec B}(-\sqrt{m+2}) = 0$, tj.
    \begin{equation*}
        \bigg( \sum_{0 \leq k \leq N/2}b_{2k} (m+2)^k \bigg) + \sqrt{m+2} \bigg( \sum_{0 \leq k \leq (N-1)/2} b_{2k+1}(m+2)^k\bigg)  = 0.
    \end{equation*}
    Ozna\v{c}imo gornje cjelobrojne sume sa $\Sigma_1$ i $\Sigma_2$. Dakle, imamo $\Sigma_1^2 = (m+2) \Sigma_2^2$, \v{s}to implicira da je $(m+2)$ kvadrat. Preostaje pokazati da je $n := \sqrt{m+2}$ neparan. Definiramo $N \times N$ matricu $\vec J_N$ kojoj su svi elementi jednaki jedinici. Dimenzija njezine jezgre je $N-1$ pa je stoga u presjeku s $N-m$ dimenzionalnim svojstvenim potprostorom od $\vec B$ koji odgovara svojstvenoj vrijednosti $-1/c = -n$, po\v{s}to $N-1 + N - m > N$ za $m \geq 3$, tj. $N = m(m+1)/2 > m + 1$. Matrica $\vec C := (\vec B - \vec I_n + \vec J_N)/2$ ima $-(n+1)/2$ kao svojstvenu vrijednost. Dijagonalni elementi su joj nula, a vandijagonalni jednaki su ili nuli ili jedinici. Stoga je $p_{\vec C}(x) := \sum_{k=0}^{N} \gamma (-x)^k, \gamma_N = 1$ s cjelobrojnim koeficijentima i $p_{\vec C}(x)$ poni\v{s}tava se za $x = -(n+1)/2$. Tu zadnju \v{c}injenicu mo\v{z}emo zapisati u obliku
    \begin{equation*}
        (n+1)^N = - \sum_{k=0}^{N-1}2^{N-k}\gamma_k(n+1)^k.
    \end{equation*}
    \newpage \noindent Slijedi da je $(n+1)^N$ paran pa je stoga i $n+1$. Kona\v{c}no imamo da je $n=\sqrt{m+2}$ neparan.
\end{proof}
Naredni teorem daje eksplicitnu konstrukciju $m \times m^2$ kompleksnih matrica s koherencijom $1/\sqrt{m}$, \v{s}to je ujedno i limes Welchove ocjene kada $N$ ide u beskona\v{c}nost.

\begin{thm}\label{tm:5:13}
    Za svaki prosti broj $m \geq 5$, postoji eksplicitna $m \times m^2$ kompleksna matrica s koherencijom $\mu = 1/\sqrt{m}$.
\end{thm}
\begin{proof}
    Neka je $\Z / m \Z =: \Z_m$. Za $k,l \in \Z_m$ uvodimo operator \textit{translacije} $\vec T_k$ i operator \textit{modulacije} $\vec M_l$ definirane sa
    \begin{equation*}
        (\vec T_k \vec z)_j  = z_{j-k}, \quad \quad (\vec M_l \vec z)_j = e^{2 \pi i l j / m}z_j 
    \end{equation*}
    za $\vec z \in \C^{\Z_m}$ i $j \in \Z_m$. Ti operatori su izometrije prostora $\ell_2(\Z_m)$. Uvedimo takozvani \textit{Alltop} $\ell_2$-normalizirani vektor $\vec x \in \C^{\Z_m}$ definiran s
    \begin{equation*}
        x_j := \frac{1}{\sqrt{m}}e^{2\pi i j^3/m}, \quad j \in \Z_m .
    \end{equation*}
    Eksplicitna $m \times m^2$ matrica iz tvrdnje teorema dana je kao matrica sa stupcima $\vec M_l \vec T_k \vec x$ za $k,l \in \Z_m$, tj. matrica oblika
    \begin{equation*}
        \begin{bmatrix*}
            \vec M_1 \vec T_1 \vec x & \cdots & \vec M_1 \vec T_m \vec x & \vec M_2 \vec T_1 \vec x & \cdots & \vec M_m \vec T_m \vec x 
        \end{bmatrix*} 
    \end{equation*}
    Ra\v{c}unamo skalarni produkt dva stupca indeksirana sa $(k,l)$ i $(k', l')$
    \begin{align*}
        \langle \vec M_l \vec T_k \vec x, \vec M_{l'} \vec T_{k'} \vec x \rangle &= \sum_{j \in \Z_m} (\vec M_{l} \vec T_{k} \vec x)_j \overline{(\vec M_{l'} \vec T_{k'} \vec x)_j} \\
        &= \sum_{j \in \Z_m} e^{2 \pi ilj/m}x_{j-k} e^{-2 \pi il'j/m}\overline{x_{j-k'}}\\ 
        &= \frac{1}{m} \sum_{j \in \Z_m} e^{2 \pi i(l-l')j/m} e^{2 \pi i((j-k)^3 - (j - k')^3)/m}.
    \end{align*}
    Ozna\v{c}imo $a := l - l'$ i $b := k - k'$, $(a,b) \neq (0,0)$ i promijenimo indeks sumacije za $h = j - k'$
    \begin{align*}
        |\langle \vec M_l \vec T_k \vec x, \vec M_{l'} \vec T_{k'} \vec x \rangle| &= \frac{1}{m} \big |   e^{2 \pi iak'/m}  \sum_{h \in \Z_m} e^{2 \pi iah/m}e^{2 \pi i ((h-b)^3 - h^3)/m}  \big | \\
        &= \frac{1}{m} \big |   \sum_{h \in \Z_m} e^{2 \pi iah/m}e^{2 \pi i (-3bh^2 + 3b^2h - b^3)/m}  \big | \\
        &= \frac{1}{m} \big |   \sum_{h \in \Z_m} e^{2 \pi i (-3bh^2 + (a+3b^2)h)/m}  \big |
    \end{align*}
    Neka je $c := -3b$ i $d := a + 3b^2$,
    \begin{align*}
        |\langle \vec M_l \vec T_k \vec x, \vec M_{l'} \vec T_{k'} \vec x \rangle|^2 &= \frac{1}{m^2} \sum_{h \in \Z_m} e^{2 \pi i (ch^2 + dh)/m} \sum_{h' \in \Z_m} e^{-2 \pi i (ch'^2 + dh')/m}\\
        &= \frac{1}{m^2} \sum_{h,h'} e^{2 \pi i (h-h')(c(h+h') + d)/m} \\ 
        &= \frac{1}{m^2} \sum_{h',h'' \in \Z_m} e^{2 \pi i h''(c(h''+2h') + d)/m} \\
        &= \frac{1}{m^2} \sum_{h'' \in \Z_m} e^{2 \pi i h''(ch''+d)/m} \big ( \sum_{h' \in \Z_m} e^{4 \pi ich''h'/m} \big).
    \end{align*}
    Primijetimo, za svaki $h'' \in \Z_m$ imamo
    \begin{equation*}
          \sum_{h' \in \Z_m} e^{4 \pi ich''h'/m} =  
          \begin{cases}
              m \quad &\text{ako } 2ch'' = 0 \mod m, \\
              0 \quad &\text{ako } 2ch'' \neq 0 \mod m.
          \end{cases}
    \end{equation*}
    Pogledajmo dva slu\v{c}aja:
    \begin{enumerate}
        \item $c = 0 \mod m$: \\
            Po\v{s}to je $c = -3b$ i $3 \neq 0 \mod m$, imamo $b = 0$, pa stoga $d = a +3b^2 \neq 0 \mod m$ i 
            \begin{equation*}
                |\langle \vec M_l \vec T_k \vec x, \vec M_{l'} \vec T_{k'} \vec x \rangle|^2 = \frac{1}{m} \sum_{h'' \in \Z_m} e^{2 \pi idh''/m} = 0.
            \end{equation*}

        \item $c \neq 0 \mod m$: \\
            Po\v{s}to $2 \neq 0 \mod m$, jednakost $2ch'' = 0$ vrijedi samo kada je $h'' = 0 \mod m$, pa stoga
            \begin{equation*}
                |\langle \vec M_l \vec T_k \vec x, \vec M_{l'} \vec T_{k'} \vec x \rangle|^2 = \frac{1}{m}. 
            \end{equation*}
    \end{enumerate}
    Dakle, koherencija matrice je $1/\sqrt{m}$.
\end{proof}

\newpage
\section[Analiza OMP algoritma][Analiza OMP algoritma]{Analiza OMP algoritma}
Pokazati \'cemo da mala koherencija osigurava rekonstrukciju rijetkih vektora OMP algoritmom.
\begin{thm}\label{tm:5:14}
    Neka je $\vec A \in \C^{m \times N}$ matrica s $\ell_2$-normaliziranim stupcima. Ako je 
    \begin{equation}\label{5:11}
        \mu_1(s) + \mu_1(s-1) < 1, 
    \end{equation}
    onda se svaki $s$-rijedak vektor $\vec x \in \C^N$ mo\v{z}e rekonstruirati iz vektora mjerenja $\vec y = \vec{Ax}$ u najvi\v{s}e $s$ iteracija OMP algoritma.
\end{thm}
\begin{proof}
    Neka su $\vec a_1, \dots \vec a_N$ $\ell_2$-normalizirani stupci matrice $\vec A$. Prema propoziciji \ref{prop:3:5} dovoljno je dokazati da je za svaki $S \subseteq [N]$ takav da $\card(S) = s$ matrica $\vec A_S$ injektivna te da vrijedi
    \begin{equation}\label{5:12}
        \max_{j \in S} |\langle \vec r,\vec a_j \rangle|  > \max_{l \in \bar S} |\langle \vec r, \vec a_l \rangle|
    \end{equation}
    za sve ne-nul vektore $\vec r \in \{\vec{Az},\ \supp(\vec z) \subseteq S\}$. Neka je $\vec r := \sum_{i \in S}r_i \vec a_i$ i neka je $k \in S$ takav da $|r_k| = \max_{i \in S} |r_i| > 0$. Za $l \in \bar S$ imamo,
    \begin{align*}
        |\langle \vec r, \vec a_l \rangle| &= \big | \sum_{i \in S}r_i \langle \vec a_i, \vec a_l \rangle \big | \leq \sum_{i \in S}|r_i||\langle \vec a_i, \vec a_l \rangle| \leq |r_k| \mu_1(s) \\
        |\langle \vec r, \vec a_k \rangle| &= \big | \sum_{i \in S}r_i \langle \vec a_i, \vec a_k \rangle \big |  \geq |r_k| |\langle \vec a_k, \vec a_k \rangle| - \sum_{i \in S, i \neq k}|r_i||\langle \vec a_i, \vec a_k \rangle| \\
        &\geq |r_k| - |r_k|\mu_1(s-1).
    \end{align*}
    Dakle, \eqref{5:12} vrijedi jer \eqref{5:11} implicira $1-\mu_1(s-1) > \mu_1(s)$. Injektivnost od $\vec A_S$ slijedi iz korolara \ref{kor:5:4}.
\end{proof}

\section[Analiza $\ell_1$-minimizacije][Analiza $\ell_1$-minimizacije]{Analiza $\ell_1$-minimizacije}
Pokazat \'cemo da mala koherencija matrice mjerenja tako\dj er garantira i rekonstrukciju vektora $\ell_1$-minimizacijom (BT algoritmom).
\begin{thm}\label{tm:5:15}
    Neka je $\vec A \in \C^{m \times N}$ s $\ell_2$-normaliziranim stupcima. Ako je 
    \begin{equation}\label{5:13}
        \mu_1(s) + \mu_1(s-1) < 1, 
    \end{equation}
    onda se svaki $s$-rijedak vektor $\vec x \in \C^N$ mo\v{z}e rekonstruirati iz vektora mjerenja $\vec y = \vec{Ax}$ putem $\ell_1$-minimizacije.
\end{thm}
\begin{proof}
    Prema teoremu \ref{svojstvo_nul_prostora_tm} dovoljno i nu\v{z}no je dokazati da matrica $\vec A$ zadovoljava svojstvo nul-prostora te da vrijedi
    \begin{equation}\label{5:14}
        \norm{\vec v_S}_1 < \norm{\vec v_{\bar S}}_1 
    \end{equation}
    za svaki ne-nul vektor $\vec v \in \ker \vec A$ i za svaki skup indeksa $S \subseteq [N]$ takav da $\card(S) = s$. Neka su $\vec a_1, \dots, \vec a_N$ stupci od $\vec A$. Uvjet $\vec v \in \ker \vec A$ interpretiramo kao $\sum_{j=1}^N v_j \vec a_j = 0$. Dakle, imamo
    \begin{equation*}
        v_i = v_i \langle \vec a_i, \vec a_i \rangle = - \sum_{j=1,j \neq i}^N v_j \langle \vec a_j, \vec a_i \rangle = - \sum_{l \in \bar S}v_l \langle \vec a_l, \vec a_i \rangle - \sum_{j \in S,j \neq i} v_j \langle \vec a_j, \vec a_i \rangle.
    \end{equation*}
    Slijedi,
    \begin{equation*}
        |v_i| \leq \sum_{l \in \bar S}|v_l||\langle \vec a_l, \vec a_i\rangle| + \sum_{j \in S, j \neq i} |v_j||\langle \vec a_j, \vec a_i \rangle|.
    \end{equation*}
    Sumiranjem po $i \in S$ i zamjenom poretka sumacije imamo
    \begin{align*}
        \norm{\vec v_S}_1 &= \sum_{l \in \bar S}|v_l|\sum_{i \in S} |\langle \vec a_l, \vec a_i \rangle| + \sum_{j \in S}|v_j| \sum_{i \in S,i \neq j}|\langle \vec a_j, \vec a_i \rangle| \\
        &\leq \sum_{l \in \bar S} |v_l| \mu_1(s) + \sum_{j \in S}|v_j| \mu_1(s-1) = \mu_1(s) \norm{\vec v_{\bar S}}_1 + \mu_1(s-1) \norm{\vec v_S}_1.
    \end{align*}
    Od tuda lako slijedi tvrdnja.
\end{proof}

Prema teoremu \ref{tm:5:13} mo\v{z}emo odabrati matricu $\vec A \in \C^{m \times N}$ s koherencijom $\mu \leq c/\sqrt{m}$. Vidimo da je uvjet $(2s - 1)\mu < 1$, koji garantira rekonstrukciju OMP algoritmom i $\ell_1$-minimizacijom, zadovoljen ako
\begin{equation}\label{5:15}
    m \geq Cs^2.
\end{equation}
Dakle imamo ocjenu na minimalni broj mjerenja za specifi\v{c}nu matricu $\vec A$ i rijetkost $s$. No, primijetimo da ova ocjena nije prakti\v{c}na za $s$ razumne veli\v{c}ine po\v{s}to ono ulazi u ocjenu s kvadratom. Uvjerimo se da nije mogu\'ce pobolj\v{s}ati ovu ocjenu u kontekstu teorema \ref{tm:5:14} i \ref{tm:5:15}. Pretpostavimo da vrijedi dovoljan uvjet $\mu_1(s) + \mu_1(s-1) < 1$ s $m  \leq (2s-1)^2/2$ i $s < \sqrt{N-1}$ na primjer. Nadalje za $N \geq m$ iz teorema \ref{tm:5:8} slijedi
\begin{equation*}
    1 > \mu_1(s) + \mu_1(s-1) \geq (2s-1) \sqrt{\frac{N-m}{m(N-1)}} \geq \sqrt{\frac{2(N-m)}{N-1}} \geq \sqrt{\frac{N}{N-1}}.
\end{equation*}
\v{s}to je kontradikcija.

\section[Analiza grani\v{c}nih metoda][Analiza grani\v{c}nih metoda]{Analiza grani\v{c}nih metoda}
Uz sli\v{c}ne uvjete kao u prethodna dva teorema \v{c}ak i BT algoritam garantira rekonstrukciju.
\begin{thm}\label{tm:5:16}
    Neka je $\vec A \in \C^{m \times N}$ s $\ell_2$-normaliziranim stupcima i neka je $\vec x \in \C^N$ s nosa\v{c}em $S,\ \card(S) = s$. Ako je 
    \begin{equation}\label{5:16}
        \mu_1(s) + \mu_1(s-1) < \frac{\min_{i \in S}|x_i|}{\max_{i \in S}|x_i|}, 
    \end{equation}
    onda se vektor $\vec x$ mo\v{z}e rekonstruirati iz vektora mjerenja $\vec y = \vec{Ax}$ putem BT algoritma.
\end{thm}
\begin{proof}
    Neka su $\vec a_1, \dots \vec a_N$ $\ell_2$-normalizirani stupci matrice $\vec A$. Prema propoziciji \ref{prop:3:7} dovoljno je dokazati da za svaki $j \in S$ i $l \in \bar S$,
    \begin{equation}\label{5:17}
        |\langle \vec{Ax}, \vec a_j \rangle| >  |\langle \vec{Ax}, \vec a_l \rangle|.
    \end{equation}
    Primijetimo,
    \begin{align*}
        |\langle \vec{Ax}, \vec a_l \rangle| &= |\sum_{i \in S} x_i \langle \vec a_i, \vec a_l\rangle| \leq \sum_{i \in S} |x_i||\langle \vec a_i, \vec a_l \rangl| \leq \mu_1(s) \max_{i \in S} |x_i|,\\
        |\langle \vec{Ax}, \vec a_j \rangle| &=|\sum_{i \in S} x_i \langle \vec a_i, \vec a_j\rangle| \geq |x_j| - \sum_{i \in S, i \neq j}|x_i||\langle \vec a_i, \vec a_j \rangle| \\
        &\geq \min_{i \in S} |x_i| - \mu_1(s-1)\max_{i \in S}|x_i|.
    \end{align*}
    Iz \eqref{5:16} slijedi
    \begin{equation*}
        |\langle \vec{Ax}, \vec{a}_j \rangle| - |\langle \vec{Ax}, \vec{a}_l \rangle| \geq \min_{i \in S}|x_i| - (\mu_1(s) - \mu_1(s-1))\max_{i \in S} |x_i| > 0.
    \end{equation*}
\end{proof}

Uz iste uvjete, analogno se poka\v{z}e da IHT algoritam garantira rekonstrukciju. Sada \'cemo pokazati da HTP algoritam uz odre\dj ene uvjete, isto kao u OMP u $s$ iteracija rekonstruira $s$-rijedak vektor.
\begin{thm}\label{tm:5:17}
    Neka je $\vec A \in \C^{m \times N}$ s $\ell_2$-normaliziranim stupcima. Ako je
    \begin{equation*}
        2 \mu_1(s) + \mu_1(s-1) < 1, 
    \end{equation*}
    tada se svaki $s$-rijedak vektor $\vec x \in \C^N$ mo\v{z}e rekonstruirati iz vektora mjerenja $ \vec y = \vec{Ax}$ u najvi\v{s}e $s$ iteracija HTP algoritma.
\end{thm}
\begin{proof}
    Neka su $j_1, j_2, \dots, j_N$ takvi da
    \begin{equation*}
        |x_{j_1}| \geq |x_{j_2}| \geq \cdots \geq |x_{j_s}| > |x_{j_{s+1}}| = \cdots = |x_{j_N}| = 0.
    \end{equation*}
    Pokazat \'cemo da je za $0 \leq n \leq s - 1$, skup $\{j_1, \dots, j_{n+1}\}$ sadr\v{z}an u $S^{n+1}$ iz \eqref{htp_1}, koji je definiran kao skup $s$ apsolutno najve\'cih komponenti od 
    \begin{equation}\label{5:18}
        \vec z^{n+1} := \vec x^n + \vec A^* \vec A(\vec x - \vec x^n). 
    \end{equation}
    To \'ce implicirati $S^s = S = \supp(\vec x)$ pa prema \eqref{htp_2} $\vec x^s = \vec x$. Primijetimo dovoljno je dokazati
    \begin{equation}\label{5:19}
        \min_{1 \leq k \leq n+1} |z_{j_k}^{n+1}| > \max_{l \in \bar S}|z_{l}^{n+1}|.
    \end{equation}
    Dokazujemo indukcijom. Vrijedi
    \begin{equation*}
        z_j^{n+1} = x_j^n + \sum_{i = 1}^N (x_i - x_i^n) \langle \vec a_i, \vec a_j \rangle = x_j + \sum_{i \neq j}(x_i - x_i^n) \langle \vec a_i, \vec a_j \rangle.
    \end{equation*}
    Stoga,
    \begin{equation}\label{5:20}
        |z_j^{n+1} - x_j| \leq \sum_{i \in S^n, i \neq j} |x_i - x_i^n|| \langle \vec a_i, \vec a_j \rangle| + \sum_{i \in S \backslash S^n, i \neq j} |x_i||\langle \vec a_i, \vec a_j \rangle|.
    \end{equation}
    Za $1 \leq k \leq n + 1$ i $l \in \bar S$ imamo
    \begin{align} 
        &|z_{j_k}^{n+1}| \geq |x_{j_k}| - \mu_1(s)\norm{ ( \vec x - \vec x^n )_{S^n} }_{\infty } - \mu_1(s) \norm{\vec x_{S \backslash S^n}}_{\infty} \label{5:21} \\[1em]
        &|z_l^{n+1}| \leq \mu_1(s) \norm{(\vec x - \vec x^n)_{S^n}}_{\infty} - \mu_1(s)\norm{\vec x_{S \backslash S^n}}_{\infty}. \label{5:22}
    \end{align}
    Posebno, za $n = 0$ je $\norm{(\vec x - \vec x^n)_{S^n}}_{\infty} = 0$ pa iz \eqref{5:21}, \eqref{5:22} i \v{c}injenice da $2\mu_1(s) < 1$ slijedi
    \begin{equation*}
        |z_{j_1}^1| \geq (1-\mu_1(s))\norm{\vec x}_{\infty} > \mu_1(s) \norm{\vec x}_{\infty} \geq |z_l^1| \quad \text{za sve } l \in \bar S.
    \end{equation*}
    Dakle, tvrdnja \eqref{5:19} vrijedi za $n = 0$. Pretpostavimo da tvrdnja vrijedi za $n-1$ pri \v{c}emu je $n \geq 1$. To implicira $\{j_1, \dots j_n\} \subseteq S^n$. Iz \eqref{htp_2} i leme \ref{lem:3:4} slijedi
    \begin{equation*}
        (\vec A^* \vec A (\vec x - \vec x^n))_{S^n} = 0.
    \end{equation*}
    Stoga za svaki $j \in S^n$, definicija \eqref{5:18} implicira $z_j^{n+1} = x_j^n$, te iz \eqref{5:20} slijedi
    \begin{equation*}
        |x_j^n - x_j| \leq \mu_1(s-1) \norm{(\vec x - \vec x^n)_{S^n}}_{\infty} + \mu_1(s-1)\norm{\vec x_{S \backslash S^n}}_{\infty}.
    \end{equation*}
    Uzimaju\v{c}i maksimum po $j \in S^n$ dobivamo
    \begin{equation*}
        \norm{(\vec x - \vec x^n)_{S^n}}_{\infty}  \leq \frac{\mu_1(s-1)}{1 - \mu_1(s-1)} \norm{\vec x_{S \backslash S^n}}_{\infty}.
    \end{equation*}
    Dobiveno vratimo nazad u \eqref{5:21} i \eqref{5:22},
    \begin{align*}
        &|z_{j_k}^{n+1}|  \geq \big( 1 - \frac{\mu_1(s)}{1 - \mu_1(s-1)}\big) |x_{j_{n+1}}|,  \\
        &|z_{l}^{n+1}|  \leq \frac{\mu_1(s)}{1 - \mu_1(s-1)}|x_{j_{n+1}}|,
    \end{align*}
    za $1 \leq k \leq n+1$ i $l \in \bar S$.
    Po\v{s}to je $\mu_1(s)/(1-\mu_1(s-1)) < 1/2$, \eqref{5:19} vrijedi i za $n$. Po principu matemati\v{c}ke indukcije slijedi tvrdnja.
\end{proof}

\chapter[Svojstvo restriktivne izometri\v{c}nosti][Svojstvo restriktivne izometri\v{c}nosti]{Svojstvo restriktivne izometri\v{c}nosti}
U pro\v{s}lom poglavlju vidjeli smo da je pojam koherencije vrlo koristan kao mjera kvalitete matrice mjerenja. Pomo\'cu njega lako smo postavili i dokazali uvjete koji garantiraju rekonstrukciju rijetkih vektora raznim algoritmima. No, ocjena na koherenciju iz teorema \eqref{tm:5:7} ograni\v{c}ava analizu algoritama na male vrijednosti rijetkosti $s$. U ovom poglavlju uvesti \'{c}emo novu mjeru kvalitete matrice, \textit{svojstvo restriktivne izometri\v{c}nosti} (eng. \textit{restricted isometry property}) koje se ponekad zove i \textit{princip uniformne neodre\dj enosti} (eng. \textit{uniform uncertainty principle}).

\section[Definicija i osnovna svojstva][Definicija i osnovna svojstva]{Definicija i osnovna svojstva}
Za razliku od koherencije koja uzima u obzir parove stupaca matrice, svojstvo orestriktivne izometri\v{c}nosti uzima u obzir sve $s$-torke stupaca matrice pa je stoga prikladnija mjera kvalitete.
\begin{defn}\label{defn:6:1}
    $s$-ta konstanta restriktivne izometri\v{c}nosti $\delta_s = \delta_s(\vec A)$ matrice $\vec A \in \C^{m \times N}$ je najmanji $\delta \geq 0$ takva da
    \begin{equation}\label{6:1}
        (1-\delta) \norm{\vec x}_2^2 \leq \norm{\vec{Ax}}_2^2 \leq (1+\delta)\norm{\vec x}_2^2 
    \end{equation}
    za sve $s$-rijetke vektore $\vec x \in \C^N$. Ili ekvivalentno
    \begin{equation}\label{6:2}
        \delta_s = \max_{S \subseteq [N], \card(S) \leq s} \norm{\vec A^*_S \vec A_S - \vec I}_2.
    \end{equation}
    Neformalno, ka\v{z}emo da matrica $\vec A$ zadovoljava svojstvo restriktivne izometri\v{c}nosti ako je $\delta_s$ dovoljno mali za $s$ dovoljno velik (kasnije \'cemo to\v{c}no precizirati).
\end{defn}
Uvjerimo se da su \eqref{6:1} i \eqref{6:2} ekvivalente tvrdnje. Iz \eqref{6:1} direktno slijedi
\begin{equation*}
    | \norm{\vec A_S \vec x}_2^2 - \norm{\vec x}_2^2| \leq \delta \norm{\vec x}_2^2 \quad \text{za sve } S \subseteq [N],\ \card(S) \leq s,\ \text{i za sve } \vec x \in \C^s.
\end{equation*}
Primijetimo, za svaki $\vec x \in \C^s$
\begin{equation*}
    \norm{\vec A_S \vec x}_2^2 - \norm{\vec x}_2^2 = \langle \vec A_S \vec x, \vec A_S \vec x \rangle - \langle \vec x, \vec x \rangle = \langle (\vec A_S^* \vec A_S - \vec I )\vec x, \vec x\rangle.
\end{equation*}
Po\v{s}to je $\vec A_S^* \vec A_S - \vec I$ hermitska, imamo
\begin{equation*}
    \max_{x \in \C^s \backslash \{\vec 0\}} \frac{\langle (\vec A_S^* \vec A_S - \vec I)\vec x, \vec x \rangle}{\norm{\vec x}_2^2} = \norm{\vec A_S^* \vec A_S - \vec I}_2.
\end{equation*}
Dakle, \eqref{6:1} je ekvivalentno s
\begin{equation*}
    \max_{S \subseteq [N], \card(S) \leq s} \norm{\vec A^*_S \vec A_S - \vec I}_2 \leq \delta. 
\end{equation*}
Mogu\'ce je usporediti konstantu restriktivne izometri\v{c}nosti s koherencijom $\mu$.
\begin{prop}\label{prop:6:2}
    Neka je $\vec A$ s $\ell_2$-normaliziranim stupcima $\vec a_1, \dots \vec a_N$. Tada za svaki $j \in [N]$ vrijedi
    \begin{equation*}
        \delta_1 = 0, \quad \delta_2 = \mu \quad \delta_s \leq \mu_1(s-1) \leq (s-1)\mu, \quad s \geq 2. 
    \end{equation*}
\end{prop}
\begin{proof}
    Po\v{s}to su stupci od $\vec A$ $\ell_2$-normalizirani, vrijedi $\norm{\vec{Ae}_j}_2^2 = \norm{\vec e_j}_2^2$ za sve $j \in [N]$. Dakle, $\delta_1 = 0$. Neka su $\vec a_1, \dots, \vec a_N$ stupci od $\vec A$. Imamo,
    \begin{equation}
        \delta_2 = \max_{1 \leq i \neq j \leq N} \norm{\vec A^*_{\{i,j\}}\vec A_{\{i,j\}} - \vec I}_2, \quad \quad \vec A^*_{\{i,j\}}\vec A_{\{i,j\}} = 
        \begin{bmatrix*}
            1 & \langle \vec a_j, \vec a_i \rangle \\
            \langle \vec a_j, \vec a_i \rangle  & 1
        \end{bmatrix*}.
    \end{equation}
    Svojstvene vrijednosti od $\vec A^*_{\{i,j\}}\vec A_{\{i,j\}} - \vec I$ su $|\langle \vec a_i, \vec a_j \rangle |$ i $-|\langle \vec a_i, \vec a_j \rangle |$. Stoga je njezina operatorska norma jednaka $|\langle \vec a_i, \vec a_j \rangle |$. Uzimaju\'ci maksimum po $1 \leq i \neq j \leq N$ dobivamo $\delta_2 = \mu$. Nejednakost $\delta_s \leq \mu_1(s-1) \leq (s-1)\mu$ posljedica je teorema \ref{tm:5:3}.
\end{proof}

U pro\v{s}lom poglavlju pokazali smo egzistenciju $m \times m^2$ matrica s koherencijom $1/\sqrt{m}$ \v{s}to direktno implicira egzistenciju matrica istih dimenzije s konstantom restriktivne izometri\v{c}nosti $\delta_s < 1$ za $s \leq \sqrt{m}$.

\begin{prop}\label{prop:6:3}
    Neka su $\vec u, \vec v \in \C^N$ takvi da $\norm{\vec u}_0 \leq s$ i $\norm{\vec v}_0 \leq t$. Ako je $\supp(\vec u ) \cap \supp(\vec v) = \emptyset$ tada
    \begin{equation}\label{6:3}
        |\langle \vec{Au}, \vec{Av} \rangle| \leq \delta_{s+t} \norm{\vec u}_2 \norm{\vec v}_2.
    \end{equation}
\end{prop}
\begin{proof}
    Neka je $S:=\supp(\vec u) \cup \supp(\vec v)$. Po\v{s}to su nosa\v{c}i od $\vec v$ i $\vec u $ disjunktni, imamo $\langle \vec u_S, \vec v_S \rangle = 0$. Slijedi,
    \begin{align*}
        |\langle \vec{Au}, \vec{Av} \rangle| &= |\langle \vec A_S \vec u_S, \vec A_S \vec v_s  \rangle - \langle \vec u_S, \vec v_S \rangle| = |\langle (\vec A^*_S \vec A_S - \vec I)\vec u_S, \vec v_S \rangle| \\[1em]
        &\leq \norm{(\vec A^*_S \vec A_S - \vec I) \vec u_S}_2 \norm{\vec v_S}_2 \leq \norm{\vec A^*_S \vec A_S - \vec I}_2 \norm{\vec u_S}_2 \norm{\vec v_S}_2.
    \end{align*}
    Tvrdnja slijedi iz \eqref{6:2}, $\norm{\vec u_S}_2 = \norm{\vec u}_2$ i $\norm{\vec v_S}_2 = \norm{\vec v}_2$.
\end{proof}
\begin{defn}
    $(s,t)$-ograni\v{c}ena konstanta orthogonalnosti $\theta_{s,t} = \theta_{s,t}(\vec A)$ matrice $\vec A \in \C^{m \times N}$ je najmanji $\theta \geq 0$ takva da
    \begin{equation}\label{6:4}
        |\langle \vec{Au}, \vec{Av} \rangle| \leq \theta \norm{\vec u}_2 \norm{\vec v}_2
    \end{equation}
    za sve $s$-rijetke vektore $\vec u \in \C^N$ i $t$-rijetke vektore $\vec v \in \C^N$ s disjunktnim nosa\v{c}em ili ekvivalentno, 
    \begin{equation}\label{6:5}
        \theta_{s, t} = \max \big\{ \norm{\vec A^*_T \vec A_S}_2,\ S \cap T = \emptyset,\ \card(S) \leq s,\ \card(T) \leq t \big\} .
    \end{equation}
\end{defn}
\begin{prop}\label{prop:6:5}
    Vrijedi 
    \begin{equation*}
        \theta_{s,t} \leq \delta_{s+t} \leq \frac{1}{s+t} (s \delta_s + t\delta_t + 2 \sqrt{st} \theta_{s,t}).
    \end{equation*}
    Posebno, za $t = s$ imamo
    \begin{equation*}
        \theta_{s,s} \leq \delta_{2s} \quad \text{i} \quad \delta_{2s} \leq \delta_s + \theta_{s,s}.
    \end{equation*}
\end{prop}
\begin{proof}
    Prva nejednakost slijedi direktno iz propozicije \ref{prop:6:3}. Poka\v{z}imo i drugu nejednakost. Neka je $\vec x \in \C^N$ $(s+t)$-rijedak vektor takav da $\norm{\vec x}_2 = 1$. Moramo pokazati da
    \begin{equation*}
        \big |\norm{\vec{Ax}}_2^2 - \norm{\vec x}_2^2 \big |  \leq \frac{1}{s+t} (s \delta_s + t \delta_t + s \sqrt{st} \theta_{s,t}).
    \end{equation*}
    Neka su $\vec u, \vec v \in \C^N$ takvi da $\vec u + \vec v = \vec x$, gdje je $\vec u$ $s$-rijedak, $\vec v$ $t$-rijedak te imaju disjunktne nosa\v{c}e. Vrijedi, 
    \begin{equation*}
        \norm{\vec{Ax}}_2^2 = \langle \vec A(\vec u + \vec v) , \vec A(\vec u + \vec v) \rangle = \norm{\vec{Au}}_2^2 + \norm{\vec{Av}}_2^2 + 2 \Re \langle \vec{Au}, \vec{Av} \rangle.
    \end{equation*}
    Uvrstimo $\norm{\vec x}_2^2 = \norm{\vec u}_2^2 + \norm{\vec v}_2^2$,
    \begin{align*}
        \big |\norm{\vec{Ax}}_2^2 - \norm{\vec x}_2^2 \big | &\leq \big |\norm{\vec{Au}}_2^2 - \norm{\vec u }_2^2 \big | + \big |\norm{\vec{Av}}_2^2 - \norm{\vec v}_2^2 \big | + 2 \big |\langle  \vec{Au}, \vec{Av} \rangle \big | \\
        &\leq \delta_s \norm{\vec u }_2^2 + \delta_t \norm{\vec v }_2^2 + 2 \theta \norm{\vec u}_2^2 \norm{\vec v }_2^2 =: f(\norm{\vec u}^2_2),
    \end{align*}
    gdje je za $\alpha \in [0,1]$
    \begin{equation}\label{6:6}
        f(\alpha) := \delta_s \alpha + \delta_t(1-\alpha) + 2 \theta_{s,t} \sqrt{\alpha(1- \alpha)}.
    \end{equation}
    Lako se poka\v{z}e da postoji $\alpha^* \in [0,1]$ tako da je $f$ nepadaju\'ca na $[0, \alpha^*]$ i nerastu\'ca na $[\alpha^*, 1]$. Ovisno o poziciji od $\alpha^*$ s obzirom na $s/(s+t)$ funkcija $f$ je ili nepadaju\'ca na $[0, s/(s+t)]$ ili nerastu\'ca na $[s/(s+t), 1]$. Dobrim odabirom vektora $\vec u$, uvijek mo\v{z}emo pretpostaviti da je $\norm{\vec u}_2^2$ u jednom od ta dva intervala. Zaista, ako se $\vec u$ sastoji od $s$ apsolutno najmanjih komponenti od $\vec x$ a $\vec v$ od $t$ apsolutno najve\'cih komponenti od $\vec x$ onda imamo
    \begin{equation*}
        \frac{\norm{\vec u}_2^2}{s} \leq \frac{\norm{\vec v}_2^2}{t} = \frac{1 - \norm{\vec u}_2^2}{t}, \quad \text{tako da } \norm{\vec u}_2^2 \leq \frac{s}{s+t},     
    \end{equation*}
    U slu\v{c}aju da je $\vec u$ sa\v{c}injen od $s$ apsolutno najve\'cih komponenti od $\vec x$, tada bi vrijedilo $\norm{\vec u}_2^2 \geq s/(s+t)$. Dakle, 
    \begin{equation*}
        \big | \norm{\vec{Ax}}_2^2 - \norm{\vec x}_2^2 \big | \leq f(\frac{s}{s+t}) = \delta_s \frac{s}{s+t} + \delta_t \frac{t}{s+t} + 2 \theta_{s,t} \frac{\sqrt{st}}{s+t} .
    \end{equation*}
\end{proof}

Kao kod koherencije, zanima nas koja je donja granica za $s$-tu konstantu restriktivne izometri\v{c}nosti matrice $\vec A \in \C^{m \times N}$.

\begin{thm}
    Neka je $\vec A \in \C^{m \times N}$ i $2 \leq s \leq N$. Tada je
    \begin{equation}\label{6:9}
        m \geq c \frac{s}{\delta_s^2} , 
    \end{equation}
    za $N \geq Cm$ i $\delta_s \leq \delta_*$, gdje konstatne $c, C$ i $\delta_*$ ovise samo o sebi me\dj usobno. Na primjer, mo\v{z}emo uzeti $c = 1/162$, $C = 30$ i $\delta_* = 2/3$.
\end{thm}
\begin{proof}
    Primijetimo da tvrdnja ne vrijedi za $s = 1$ jer je $\delta_1 = 0$ ako svi stupci od $\vec A$ imaju $\ell_2$-normu jednaku jedan. Neka je $t := \lfloor s/2 \rfloor \geq 1$ i rastavimo $\vec A$ na blokove tipa $m \times t$, osim mo\v{z}da zadnjeg koji mo\v{z}e imati manje stupaca,
    \begin{equation*}
        \vec A =
        \begin{bmatrix*}
            \vec A_1 & \vec A_2 & \dots \vec A_n 
        \end{bmatrix*},
        \quad  N \leq nt.
    \end{equation*}
    Iz \eqref{6:2} i \eqref{6:5} za $i,j \in [n],\ i \neq j$ imamo
    \begin{equation*}
        \norm{\vec A^*_i \vec A_i - \vec I}_2 \leq \delta_t \leq \delta_s, \quad \norm{\vec A^*_i \vec A_j}_2 \leq \theta_{t,t} \leq \delta_{2t} \leq \delta_s,
    \end{equation*}
    pa svojstvene vrijednosti od $\vec A^*_i \vec A_i$ i singularne vrijednosti od $\vec A^*_i \vec A_j$ zadovoljavaju
    \begin{equation*}
        1 - \delta_s \leq \lambda_k(\vec A^*_i \vec A_i) \leq 1 + \delta_s, \quad \sigma_k(\vec A^*_i \vec A_j) \leq \delta_s. 
    \end{equation*}
    Uvedimo oznake za matrice
    \begin{equation*}
        \vec H := \vec {AA}^* \in \C^{m \times m}, \quad \vec G := \vec A^* \vec A = [\vec A^*_i \vec A_j]_{1 \leq i,j \leq n} \in \C^{N \times N}. 
    \end{equation*}
    Imamo
    \begin{equation}\label{6:10}
        \tr(\vec H) = \tr(\vec G) = \sum_{i = 1}^{n} \tr(\vec A^*_i \vec A_i) = \sum_{i=1}^n \sum_{k=1}^t \lambda_k(\vec A^*_i \vec A_i) \geq nt(1-\delta_s).
    \end{equation}
    Nadalje, 
    \begin{equation*}
        \tr(\vec H)^2 = \langle \vec I_m, \vec H \rangle^2_F \leq \norm{\vec I_m}^2_F \norm{\vec H}^2_F = m \tr(\vec H^* \vec H).
    \end{equation*}
    Zbog svojstva cikli\v{c}nosti traga vrijedi,
    \begin{align*}
        \tr(\vec H^* \vec H) &= \tr(\vec{AA}^*\vec{AA}^*) = \tr(\vec A^* \vec{AA}^* \vec A) = \tr(\vec{GG}^*) \\[1em]
        &= \sum_{i = 1}^n \tr \big(\sum_{j=1}^m \vec A^*_i \vec A_j \vec A^*_j \vec A_i \big ) \\[1em]
        &= \sum_{1 \leq i \neq j \leq n} \sum_{k=1}^t \sigma_k(\vec A^*_i \vec A_j)^2 + \sum_{i=1}^n \sum_{k=1}^t \lambda_k(\vec A^*_i \vec A_i)^2 \\[1em] 
        &\leq n(n-1)t \delta_s^2 + nt(1+\delta_s)^2.
    \end{align*}
    Dobivamo ocjenu
    \begin{equation}\label{6:11}
        \tr(\vec H)^2 \leq m n t \big( (n-1)\delta_s^2 + (1+\delta_s)^2 \big) .
    \end{equation}
    Kombiniranjem \eqref{6:10} i \eqref{6:11} imamo
    \begin{equation*}
        m \geq \frac{nt(1-\delta_s)^2}{(n-1)\delta_s^2 + (1+\delta_s)^2}.  
    \end{equation*}
    Pretpostavimo da je $(n-1)\delta_s^2 < (1+\delta_s)^2/5$. Za $\delta_s \leq 2/3$ slijedi
    \begin{equation*}
        m > \frac{nt(1-\delta_s)^2}{6(1+\delta_s)^2/5}  \geq \frac{5(1-\delta_s)^2}{6(1+\delta_s)^2}N \geq \frac{1}{30}N,   
    \end{equation*}
    \v{s}to je kontradikcija. Dakle, mora vrijediti $(n-1)\delta_s^2 \geq (1+\delta_s)^2/5$, \v{s}to uz $\delta_s \leq 2/3$ i $s \leq 3t$ implicira
    \begin{equation*}
        m \geq \frac{nt(1-\delta_s)^2}{6(n-1)\delta_s^2} \geq \frac{1}{54} \frac{t}{\delta_s^2} \geq \frac{1}{162} \frac{s}{\delta_s^2}. 
    \end{equation*}
\end{proof}

Usporedimo ocjene dobivene do sada. Imamo ocjenu odozdo
\begin{equation}\label{6:12}
    \delta_s \geq \sqrt{cs/m}. 
\end{equation}
Za $\vec A \in \C^{m \times N}$ s optimalnom koherencijom $\mu \leq c/\sqrt{m}$, propozicija \ref{prop:6:2} implicira
\begin{equation}\label{6:13}
    \delta_s \leq (s-1)\mu \leq cs/\sqrt{m}. 
\end{equation}
Primijetimo da je razmak izme\dj u \eqref{6:12} i \eqref{6:13} zna\v{c}ajan. Iz \eqref{6:13} imamo
\begin{equation}\label{6:14}
    m \geq c' s^2
\end{equation}
\v{s}to dozvoljava da $\delta_s$ bude malen. Ako je to zadovoljeno iz \eqref{6:12} imamo da je $m \geq c's$. Nije poznato je li takav uvjet dovoljan. Poka\v{z}e se da odre\dj ene slu\v{c}ajne matrice $\vec A \in \R^{m \times N}$ zadovoljavaju $\delta_s \leq \delta$ s velikom vjerojatno\v{s}\'cu za neki $\delta > 0$ ako je
\begin{equation}\label{6:15}
    m \geq C \delta^{-2}s \ln(eN/S).
\end{equation}
Konstrukcija deterministi\v{c}kih matrica u polinomijalnom vremenu koje zadovoljavaju $\delta_s \leq \delta$ u kontekstu \eqref{6:15} do danas otvoren je problem. Glavna zapreka je \v{s}to gotovo sve aproksimacije $\delta_s$ kombiniraju ocjenu koherencije i tvrdnju oblika propozicije \ref{prop:6:2}. To vodi na ocjene oblika \eqref{6:13} i kvadratnu ovisnost ocjene u varijabli $s$. Iznimka su radovi \cite{2010arXiv1008.4535B} i \cite{DBLP:journals/corr/Chen15n}. Bourgain et al. u \cite{2010arXiv1008.4535B} daje eksplicitnu konstrukciju deterministi\v{c}kih matrica $\vec A \in \C^{m \times N}$ s malim $\delta_s$ za $m \geq Cs^{2-\varepsilon}$ i $ s^{2-\varepsilon} \leq N \leq s^{2+\varepsilon}$ za neki $\varepsilon > 0$. Napredak je ostvaren putem novih estimacija za produkt skupova koji su suma dvaju skupa i za eksponencijalnu sumu produkta skupova s posebnom aditivnom strukturom. U \cite{DBLP:journals/corr/Chen15n} nadogra\v{d}uje se ideja iz \cite{2010arXiv1008.4535B} kori\v{s}tenjem algebarske geometrije. Nadalje u \cite{2012arXiv1205.2081T} pokazano je da izra\v{c}un $\delta_s$ $\mathfrak{NP}$-te\v{z}ak problem. Intuitivno to je jasno. Naime, svojstvo restriktivne izometri\v{c}nosti uzima u obizir sve mogu\'ce $s$-torke stupaca matrice $\vec A$.

\newpage
\section[Analiza $\ell_1$-minimizacije][Analiza $\ell_1$-minimizacije]{Analiza $\ell_1$-minimizacije}

Pokazati \'cemo da $\ell_1$-minimizacija uspje\v{s}no rekonstruira sve $s$-rijetke vektore za dovoljno male konstante restriktivne izometri\v{c}nosti, to\v{c}nije za $\delta_{2s} < 1/3$.

\begin{thm}\label{tm:6:9} 
    Neka $2s$-ta konstanta restriktivne izometri\v{c}nosti matrice $\vec A \in \C^{m \times N}$ zadovljava 
    \begin{equation}\label{6:16}
        \delta_{2s} < \frac{1}{3} .
    \end{equation}
    Tada je svaki $s$-rijedak vektor $\vec x \in \C^N$ jedinstveno rje\v{s}enje problema 
    \begin{equation*}
        \min_{\vec z \in \C^N} \norm{\vec z}_1 \quad \text{uz uvjet } \vec{Az} = \vec{Ax}. 
    \end{equation*}
\end{thm}
Za dokaz potreban je sljede\'ci argument.
\begin{lem}\label{lem:6:10}
    Neka je $q > p > 0$. Ako $\vec u \in \C^s$ i $\vec v \in \C^t$ zadovoljavaju 
    \begin{equation}\label{6:17}
        \max_{i \in [s]} |u_i| \leq \min_{j \in [t]} |v_j|, 
    \end{equation}
    onda,
    \begin{equation*}
        \norm{\vec u}_q \leq \frac{s^{1/q}}{t^{1/p}} \norm{\vec v}_p.
    \end{equation*}
    Posebno za $p = 1,\  q = 2$ i $t = s$,
    \begin{equation*}
        \norm{\vec u}_2 \leq \frac{1}{\sqrt{s}} \norm{\vec v}_1.
    \end{equation*}
\end{lem}
\begin{proof}
    Primijetimo da je
    \begin{equation*}
        \frac{\norm{\vec u}_q}{s^{1/q}} = \bigg[ \frac{1}{s} \sum_{i=1}^s |u_i|^q  \bigg]^{1/q} \leq \max_{i \in [s]} |u_i|, 
    \end{equation*}
    \begin{equation*}
        \frac{\norm{\vec v}_p}{t^{1/p}} = \bigg[ \frac{1}{t} \sum_{j=1}^t |v_j|^p  \bigg]^{1/p} \geq \min_{j \in [t]} |v_j|.
    \end{equation*}
    Sada iskoristimo \eqref{6:17} i slijedi tvrdnja.
\end{proof}

\begin{proof}[Dokaz (Teorem \ref{tm:6:9})]
    Prema teoremu \ref{svojstvo_nul_prostora_tm} dovoljno je pokazati da vrijedi svojstvo nul-prostora reda $s$, tj.
    \begin{equation*}
        \norm{\vec v_S}_1 \leq \frac{1}{2} \norm{\vec v}_1     
    \end{equation*}
    za sve $\vec v \in \ker \vec A \backslash \{\vec 0\}$ i za sve $S \subseteq [N],\  \card(S) = s$. To \'ce slijediti iz op\'cenitije tvrdnje  
    \begin{equation*}
        \norm{\vec v_S}_2 \leq \frac{\rho}{2 \sqrt{s}} \norm{\vec v}_1     
    \end{equation*}
    za sve $\vec v \in \ker \vec A \backslash \{\vec 0\}$ i za sve $S \subseteq [N],\  \card(S) = s$, gdje
    \begin{equation*}
        \rho := \frac{2 \delta_{2s}}{1 - \delta_{2s}}  
    \end{equation*}
    zadovoljava $\rho < 1$ za $\delta_{2s} < 1/3$.
    Primijetimo da je dovoljno promatrati skup $S =: S_0$, koji sadr\v{z}i indekse $s$ apsolutno najve\'cih komponenti vektora $\vec v \in \ker \vec A$. Nadalje, $\bar S_0$ particioniramo na $\bar S_0 = S_1 \cup S_2 \cup \cdots$, tako da
    \begin{align*}
       S_1:& \text{ skup indeksa $s$ apsolutno najve\'cih komponenti vektora $\vec v$ u $\bar S_0$}\\ 
       S_2:& \text{ skup indeksa $s$ apsolutno najve\'cih komponenti vektora $\vec v$ u $\overline{S_0 \cup S_1}$ }\\
       \vdots &
    \end{align*}
    Po\v{s}to je $\vec v \in \ker \vec A$, imamo $\vec A(\vec v_{S_0}) = \vec A(-\vec v_{S_1} - \vec v_{S_2} - \cdots)$ pa stoga
    \begin{align}
        \norm{\vec v_{S_0}}^2_2 &\leq \frac{1}{1- \delta_{2s}} \norm{\vec A(\vec v_{S_0})}^2_2 = \frac{1}{1-\delta_{2s}} \langle \vec A(\vec v_{S_0}), \vec A(-\vec v_{S_1}) + \vec A (- \vec v_{S_2}) + \cdots \rangle \nonumber \\
        &= \frac{1}{1-\delta_{2s}} \sum_{k \geq 1} \langle \vec A(\vec v_{S_0}), \vec A (- \vec v_{S_k}) \rangle \label{6:18}
    \end{align}
    Prema propoziciji \ref{prop:6:3} tako\dj er vrijedi
    \begin{equation}\label{6:19}
        \langle \vec A (\vec v_{S_0}), \vec A (-\vec v_{S_k})  \leq \delta_{2s} \norm{\vec v_{S_0}}_2 \norm{\vec v_{S_k}}_2.
    \end{equation}
    Uvrstimo \eqref{6:19} u \eqref{6:18} te podijelimo s $\norm{\vec v_{S_0}} > 0$, 
    \begin{equation*}
        \norm{\vec v_{S_0}}_2 \leq \frac{\delta_{2s}}{1 - \delta_{2s}} \sum_{k \geq 1} \norm{\vec v_{S_k}}_2 = \frac{\rho}{2} \sum_{k \geq 1} \norm{\vec v_{S_k}}_2.
    \end{equation*}
    Za $k \geq 1$, $s$ apsolutno najve\'cih komponenti od $\vec v_{S_k}$ nisu ve\'ce od $s$ apsolutnih komponenti od $\vec v_{S_{k-1}}$. Stoga lema \ref{lem:6:10} daje
    \begin{equation*}
        \norm{\vec v_{S_k}}_2 \leq \frac{1}{\sqrt{s}} \norm{\vec v_{S_{k-1}}}_1.
    \end{equation*}
    Napokon, 
    \begin{equation*}
        \norm{\vec v_{S_0}}_2 \leq \frac{\rho}{2 \sqrt{s}} \sum_{k \geq 1} \norm{\vec v_{S_{k-1}}}_1 \leq \frac{\rho}{2 \sqrt{s}} \norm{\vec v}_1. 
    \end{equation*}
\end{proof}

U prethodni teorem mogu\'ce je ukomponirati stabilnost i robusnost te dodatno oslabiti uvjet, tj. dovoljno  je tra\v{z}iti da $\delta_{2s} < \frac{4}{\sqrt{41}} \approx 0.6246$. No, svojstvo restriktivne izometri\v{c}nosti nosi i neke probleme kod $\ell_1$-minimizacije. Naime, pokazali smo da je $\ell_1$-minimizacija invarijanta na reskaliranje, preslagivanje te dodavanje novih mjerenja. Me\dj utim takve transformacije mogu pokvariti konstantu restriktivne izometri\v{c}nosti. Preciznije, preslagivanje mjerenja odgovora zamjeni matrice $\vec A \in \C^{m \times N}$ matricom $\vec{PA}$, gdje je $\vec P \in \C^{m \times m}$ matrica permutacije, i takva transformacija ne mijenja $\delta_s$. Dodavanje mjerenja odgovara dodavanju retka matrici $\vec A$, \v{s}to mo\v{z}e rezultirati pove\'canjem od $\delta_s$. Zaista, neka je $\delta_s(\vec A) < 1$ i uzmimo $\delta > \delta_s(\vec A)$. Neka je $\vec{\tilde A}$ matrica $\vec A$ kojoj smo dodali redak $[0 \cdots 0\ \sqrt{1+\delta}]$. Sada za $\vec x := [0 \cdots 0\ 1]^T$ vidimo da je $\norm{\vec{Ax}}^2_2 \geq 1+\delta$. To implicira da je $\delta_1(\vec{\tidle A}) \geq \delta$ pa stoga i $\delta_s(\vec{\tilde A}) > \delta_s(\vec A)$. Skaliranje dijagonalnom matricom te skaliranje konstantom tako\dj er mogu pove\'cati $\delta_s$.

\section[Analiza grani\v{c}nih metoda][Analiza grani\v{c}nih metoda]{Analiza grani\v{c}nih metoda}
Pokazati \'cemo da IHT i HTP algoritmi uspje\v{s}no rekonstruiraju rijetke vektore za matrice mjerenja s malim konstantama restriktivne izometri\v{c}nosti.
\begin{thm} \label{tm:6:15}
    Neka je matrica $\vec A \in \C^{m \times N}$ takva da 
    \begin{equation}\label{6:24}
        \delta_{3s} < \frac{1}{2}. 
    \end{equation}
    Tada za svaki $s$-rijedak vektor $\vec x \in \C^N$, niz $(\vec x^n)$ definiran s \eqref{iht} za $\vec y = \vec {Ax}$ konvergira prema $\vec x$.
\end{thm}
Za dokaz potrebna nam je sljede\'ca lema,
\begin{lem}\label{lem:6:16}
    Za $\vec u, \vec v \in \C^N$ i skup indeksa $S \subseteq [N]$ vrijedi,
    \begin{align*}
        &|\langle \vec u , (\vec I - \vec A^* \vec A)\vec v \rangle|  \leq \delta_t \norm{\vec u}_2 \norm{\vec v}_2 \quad & \text{za } \card(\supp(\vec u) \cup \supp(\vec v))\leq t,\\[0.5em]
        &\norm{((\vec I - \vec A^* \vec A)\vec v)_S}_2 \leq \delta_t \norm{\vec v}_2 \quad & \text{za } \card(S \cup \supp(\vec v)) \leq t.
    \end{align*}
\end{lem}
\begin{proof}
    Neka je $T:= \supp(\vec u) \cup \supp(\vec v)$. Imamo,
    \begin{align*}
        |\langle \vec u , (\vec I - \vec A^* \vec A)\vec v \rangle| &= |\langle \vec u, \vec v \rangle - \langle \vec{Au}, \vec{Av} \rangle| = |\langle \vec u_T, \vec v_T \rangle - \langle \vec A_T \vec u_T, \vec A_T, \vec v_T \rangle|\\[0.5em]
        &=|\langle \vec u_T, (\vec I - \vec A^*_T \vec A_T)\vec v_T \rangle| \leq \norm{\vec u_T}_2 \norm{(\vec I - \vec A^*_T \vec A_T)\vec v_T}_2\\[0.5em]
        &\leq \norm{\vec u_T}_2 \norm{\vec I - \vec A^*_T \vec A_T}_2 \norm{\vec v_T}_2 \leq \delta_t \norm{\vec u}_2 \norm{\vec v}_2.
    \end{align*}
    Druga nejednakost slijedi iz prve i \v{c}injenice
    \begin{equation*}
        \norm{((\vec I - \vec A^* \vec A)\vec v)_S}_2^2 = \langle ((\vec I - \vec A^* \vec A)\vec v)_S, (\vec I - \vec A^*\vec A)\vec v \rangle \leq \delta_t \norm{((\vec I - \vec A^*\vec A)\vec v)_S}_2 \norm{\vec v}_2.
    \end{equation*}
\end{proof}

\begin{proof}[Dokaz (Teorem \ref{tm:6:15}).]
    Primijetimo da je dovoljno prona\'ci konstantu $0 \leq \rho < 1$ takvu da 
    \begin{equation}\label{6:25}
        \norm{\vec x^{n+1} - \vec x}_2 \leq \rho \norm{\vec x^n - \vec x}_2, \quad n \geq 0
    \end{equation}
    odakle induktivno imamo
    \begin{equation*}
        \norm{\vec x^{n} - \vec x}_2 \leq \rho^n \norm{\vec x^0 - \vec x}_2 \xrightarrow{n \rightarrow \infty} 0. 
    \end{equation*}
    Prema samoj definiciji, vektor $\vec x^{n+1}$ je bolja ili barem jednako dobra aproksimacija vektora
    \begin{equation*}
        \vec u^n := \vec x^n + \vec A^*(\vec y - \vec{Ax}^n) = \vec x^n + \vec A^* \vec A (\vec x - \vec x^n) 
    \end{equation*}
    od $s$-rijetkog vektora $\vec x$. Dakle, 
    \begin{equation*}
        \norm{\vec u^n - \vec x^{n+1}}_2^2 \leq \norm{\vec u^n - \vec x}_2^2. 
    \end{equation*}
    Uvrstimo $\norm{\vec u ^n - \vec x^{n+1}}_2^2 = \norm{(\vec u ^n - \vec x) - (\vec x^{n+1} - \vec x)}_2^2$ te sre\dj ivanjem dobivamo
    \begin{equation}\label{6:26}
        \norm{\vec x^{n+1} - \vec x}_2^2 \leq 2 \Re \langle \vec u^n - \vec x, \vec x^{n+1} - \vec x \rangle .
    \end{equation}
    Lema \ref{lem:6:16} daje
    \begin{align}
        \Re \langle \vec u^n - \vec x, \vec x^{n+1} - \vec x \rangle &= \Re \langle (\vec I - \vec A^*\vec A)(\vec x^n - \vec x), \vec x^{n+1} - \vec x \rangle \nonumber \\
                                                                     &\leq \delta_{3s} \norm{\vec x^n - \vec x}_2 \norm{\vec x^{n+1} - \vec x}_2. \label{6:27}
    \end{align}
    Ako je $\norm{\vec x^{n+1} - \vec x}_2 > 0$, iz \eqref{6:26} i \eqref{6:27} slijedi
    \begin{equation*}
        \norm{\vec x^{n+1} - \vec x}_2 \leq 2 \delta_{3s} \norm{\vec x^n - \vec x}_2 
    \end{equation*}
    Stoga, tra\v{z}ena nejednost vrijedi za $ \rho  = 2 \delta_{3s} < 1$.
\end{proof}
Ponovno je mogu\'ce dobiti robusnost i stabilnost te se ocjena mo\v{z}e oslabiti. To je tvrdnja sljede\'ceg teorema koji vrijedi i za IHT, i za HTP algoritam.

\begin{thm}\label{tm:6:18}
    Neka je $\vec A \in \C^{m \times N}$ takva da
    \begin{equation}\label{6:28}
        \delta_{3s} < \frac{1}{\sqrt{3}} \approx 0.5773.
    \end{equation}
    Tada, za svaki $\vec x \in \C^{N},\ \vec e \in \C^m$ i $S \subseteq [N], \card(S) = s$, niz $(\vec x^n)$ definiran s \eqref{iht} ili s \eqref{htp_1}, \eqref{htp_2} za $\vec y = \vec{Ax}+ \vec e$ zadovoljava
    \begin{equation}\label{6:29}
        \norm{\vec x^n - \vec x_S}_2 \leq \rho^n \norm{\vec x^0 - \vec x_S}_2 + \tau \norm{\vec{Ax}_{\bar S} + \vec e}_2, 
    \end{equation}
    za svaki $n \geq 0$, gdje je $\rho = \sqrt{3} < 1$, $\tau \leq 2.18/(1-\rho)$ za \eqref{iht}, $\rho = \sqrt{2 \delta^2_{3s}/(1-\delta_{2s}^2)} < 1$, $\tau \leq 5.15 / (1- \rho)$ za \eqref{htp_1}, \eqref{htp_2}.
\end{thm}
U dokazu koristimo sljede\'cu tvrdnju,
\begin{lem}\label{lem:6:20}
    Za $\vec e \in \C^m$ i $S \in [N],\ \card(S) \leq s$ vrijedi 
    \begin{equation*}
        \norm{(\vec A^* \vec e)_S}_2^2 \leq \sqrt{1+\delta_s}\norm{\vec e}_2.
    \end{equation*}
\end{lem}
\begin{proof}
    Vrijedi,
    \begin{align*}
        \norm{(\vec A^* \vec e)_S}_2^2 &= \langle \vec A^* \vec e, (\vec A^* \vec e)_S  \rangle = \langle \vec e, \vec A ((\vec A^* \vec e)_S)\rangle \leq \norm{\vec e}_2 \norm{\vec A ((\vec A^* \vec e)_S)}_2\\
        &\leq \norm{\vec e}_2 \sqrt{1+\delta_s} \norm{(\vec A^* \vec e)_S}_2.
    \end{align*}
\end{proof}

\begin{proof}[Dokaz (Teorem \ref{tm:6:18})]
    Neka je $\vec x \in \C^N,\ \vec e \in \C^m,\ S \subseteq [N]$ takav da je $\card(S) = s$. Ako poka\v{z}emo da za svaki $n \geq 0$ vrijedi
    \begin{equation}\label{6:30}
        \norm{\vec x^{n+1} - \vec x_S}_2 \leq \rho \norm{\vec x^n - \vec x_S}_2 + (1-\rho)\tau \norm{\vec{Ax}_{\bar S} + \vec e}_2 
    \end{equation}
    tada \eqref{6:29} slijedi indukcijom. Neka je $S^{n+1} := \supp(\vec x^{n+1})$ skup indeksa $s$ apsolutno najve\'cih vrijednosti od $\vec x^n + \vec A^*(\vec y - \vec{Ax}^n)$. Stoga,
    \begin{equation*}
        \norm{(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n))_S}_2^2 \leq \norm{(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n))_{S^{n+1}}}_2^2
    \end{equation*}
    Nadalje, maknemo doprinos od $S \cap S^{n+1}$
    \begin{equation*}
        \norm{(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n))_{S \backslash S^{n+1}}}_2^2 \leq \norm{(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n))_{S^{n+1} \backslash S}}_2^2.
    \end{equation*}
    Desnu stranu mo\v{z}emo zapisati kao
    \begin{equation*}
        \norm{(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n))_{S^{n+1} \backslash S}}_2^2 = \norm{(\vec x^n - \vec x_S + \vec A^* (\vec y - \vec{Ax}^n))_{S^{n+1} \backslash S}}_2.
    \end{equation*}
    Lijeva strana zadovoljava,
    \begin{align*}
        \norm{(\vec x^n + \vec A^*(\vec y - \vec{Ax}^n))_{S \backslash S^{n+1}}}_2 = &\norm{(\vec x_S - \vec x^{n+1} + \vec x^n - \vec x_S + \vec A^*(\vec y - \vec{Ax}^n))_{S \backslash S^{n+1}}}_2\\
        \geq &\norm{(\vec x_S - \vec x^{n+1})_{S \backslash S^{n+1}}}_2\\ - & \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec {Ax}^n))_{S \backslash S^{n+1}}}_2.
    \end{align*}
    Sa $S \Delta S^{n+1} = (S \backslash S^{n+1}) \cup (S^{n+1} \backslash S)$ ozna\v{c}imo simetri\v{c}nu razliku skupa $S$ i $S^{n+1}$. Slijedi
    \begin{align}
        \norm{(\vec x_S - \vec x^{n+1})_{S \backslash S^{n+1}}}_2 \leq & \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec{Ax}^n))_{S \backslash S^{n+1}}}_2 \nonumber \\
        + & \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec{Ax}^n))_{S^{n+1} \backslash S}}_2 \nonumber \\
        \leq & \sqrt{2} \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec{Ax}^n))_{S \Delta S^{n+1}}}_2.\label{6:31}
    \end{align}
    Koncetrirajmo se na IHT algoritam prvo. Tada imamo,
    \begin{equation*}
        \vec x^{n+1} = (\vec x^n + \vec A^*(\vec y - \vec{Ax}^n))_{S^{n+1}}.
    \end{equation*}
    Slijedi
    \begin{align*}
        \norm{\vec x^{n+1} - \vec x_S}_2^2 = & \norm{(\vec x^{n+1} - \vec x_S)_{S^{n+1}}}_2^2 + \norm{(\vec x^{n+1} - \vec x_S)_{\overline{S^{n+1}}}}_2^2\\
        = & \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec{Ax}^n))_{S^{n+1}}}_2^2 + \norm{(\vec x^{n+1} - \vec x_S)_{S \backslash S^{n+1}}}_2^2.
    \end{align*}
    Nadalje, iz \eqref{6:31} imamo
    \begin{align*}
        \norm{\vec x^{n+1} - \vec x_S}_2^2 \leq & \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec{Ax}^n))_{S^{n+1}}}_2^2\\
        +&2\norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec{Ax}^n))_{S \Delta S^{n+1}}}_2^2 \\
        \leq & 3 \norm{(\vec x^n - \vec x_S + \vec A^* (\vec y - \vec{Ax}^n))_{S \cup S^{n+1}}}_2^2.
    \end{align*}
    Neka je $\vec y = \vec{Ax} + \vec e = \vec{Ax}_S + \vec e'$ gdje je $\vec e' := \vec {Ax}_{\bar S} + \vec e$. Iz lema \ref{lem:6:16} i \ref{lem:6:20} slijedi
    \begin{align*}
        \norm{\vec x^{n+1} - \vec x_S}_2 & \leq \sqrt{3} \norm{(\vec x^n - \vec x_S + \vec A^* \vec A (\vec x_S - \vec x^n) + \vec A^* \vec e')_{S \cup S^{n+1}}}_2\\ 
        & \leq \sqrt{3}\big[ \norm{ \big( (\vec I - \vec A^*\vec A)(\vec x^n - \vec x_S) \big)_{S \cup S^{n+1}}}_2 + \norm{(\vec A^*\vec e')_{S \cup S^{n+1}}}_2 \big]\\
        & \leq \big[ \delta_{3s}\norm{\vec x^n - \vec x_S}_2 + \sqrt{1+\delta_{2s}}\norm{\vec e'}_2 \big].
    \end{align*}
    To je nejednakost za IHT koja se tra\v{z}i u \eqref{6:30}. Odavde lako vidimo da je $\rho = \sqrt{3}\delta_{3s} < 1$ za $\delta_{3s} < 1/\sqrt{3}$ i da $(1-\rho)\tau = \sqrt{3}\sqrt{1+\delta_{2s}} \leq \sqrt{3 + \sqrt{3}} \leq 2.18$.\\
    \indent
    Prije\dj imo sada na HTP algoritam. Tada imamo
    \begin{equation*}
        \vec x^{n+1} = \argmin \big\{ \norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subseteq S^{n+1} \big\}.
    \end{equation*}
    Najbolja $\ell_2$ aproksimacija $\vec {Ax}^{n+1}$ vektora $\vec y \in \{\vec{Az},\ \supp(\vec z) \subseteq S^{n+1}\}$ karakterizirana je sa
    \begin{equation*}
        \langle \vec y - \vec {Ax}^{n+1}, \vec{Az} \rangle = 0 
    \end{equation*}
    za $\vec z$ takav da $\supp(\vec z) \subseteq S^{n+1}$. To zna\v{c}i da je $\langle \vec A^*(\vec y - \vec{Ax}^{n+1}), \vec z \rangle = 0$ kada je $\supp(\vec z) \subseteq S^{n+1}$, tj. mo\v{z}emo zaklju\v{c}iti
    \begin{equation*}
        (\vec A^*(\vec y - \vec {Ax}^{n+1}))_{S^{n+1}} = \vec 0.
    \end{equation*}
    Iz toga i \eqref{6:31} slijedi
    \begin{align*}
        \norm{\vec x^{n+1} - \vec x_S}_2^2 & = \norm{(\vevc x^{n+1} - \vec x_S)_{S^{n+1}}}_2^2 + \norm{(\vec x^{n+1} - \vec x_S)_{S \backslash S^{n+1}}}_2^2\\[0.5em]
        & \leq \norm{(\vec x^{n+1} - \vec x_S + \vec A^*(\vec y - \vec {Ax}^{n+1}))_{S^{n+1}}}_2^2\\[0.5em]
        & + 2 \norm{(\vec x^{n+1} - \vec x_S + \vec A^*(\vec y - \vec {Ax}^{n+1}))_{S \Delta S^{n+1}}}_2^2\\[0.5em]
        & \leq \big[ \norm{\big( (\vec I - \vec A^* \vec A)(\vec x^{n+1} - \vec x_S) \big)_{S^{n+1}}}_2 + \norm{(\vec A^* \vec e')_{S^{n+1}}}_2 \big]^2\\[0.5em]
        & + 2 \big[ \norm{ \big( (\vec I - \vec A^* \vec A)(\vec x^n - \vec x_S) \big)_{S \Delta S^{n+1}} }_2 + \norm{(\vec A^* \vec e')_{S \Delta S^{n+1}}}_2 \big]^2.
    \end{align*}
    Sada primjenimo leme \ref{lem:6:16} i \ref{lem:6:20},
    \begin{align*}
    \norm{\vec x^{n+1} - \vec x_S}_2^2 & \leq [\delta_{2s} \norm{\vec x^{n+1} - \vec x_S}_2 + \sqrt{1+\delta_s} \norm{\vec e'}_2]^2\\[0.5em]
    & + 2 [\delta_{3s} \norm{\vec x^n - \vec x_S}_2 + \sqrt{1 + \delta_{2s}} \norm{\vec e'}_2]^2.
    \end{align*}
    Nakon preslagivanja imamo
    \begin{align*}
        &2[\delta_{3s} \norm{\vec x^n - \vec x_S}_2 + \sqrt{1 + \delta_{2s}} \norm{\vec e '}_2]^2\\[0.5em]
        &\geq(1-\delta_{2s}^2)\big( \norm{\vec x^{n+1} - \vec x_S}_2 + \frac{\sqrt{1+\delta_s}}{1+\delta_{2s}} \norm{\vec e'}_2  \big)\big( \norm{\vec x^{n+1} - \vec x_S}_2 - \frac{\sqrt{1+\delta_s}}{1-\delta_{2s}} \norm{\vec e'}_2 \big).
    \end{align*}
    Mo\v{z}emo pretpostaviti da je $\norm{\vec x^{n+1} - \vec x_S}_2 \geq \sqrt{1+\delta_{2s}}\norm{\vec e'}_2 / (1- \delta_{2s})$. U suprotnom \eqref{6:30} odmah vrijedi za $(1-\rho)\tau$ dan u nastavku. Dakle, druga izraz u drugoj zagradi je pozitivan. Vrijedi
    \begin{equation*}
        2[\delta_{3s} \norm{\vec x^n - \vec x_S}_2 + \sqrt{1+\delta_{2s}}\norm{\vec e'}_2]^2 \geq (1-\delta_{2s}^2) \big( \norm{\vec x^{n+1} - \vec x_S}_2 - \frac{\sqrt{1+\delta_s}}{1-\delta_{2s}} \norm{\vec e'}_2  \big)^2.
    \end{equation*}
    Od tuda slijedi
    \begin{equation*}
        \norm{\vec x^{n+1} - \vec x_S}_2 \leq \frac{\sqrt{2} \delta_{3s}}{\sqrt{1-\delta_{2s}^{2}}} \norm{\vec x^n - \vec x_S}_2 + \big( \frac{\sqrt{2}}{\sqrt{1-\delta_{2s}}} + \frac{\sqrt{1+\delta_s}}{1-\delta_{2s}}   \big) \norm{\vec e'}_2.
    \end{equation*}
    To je jednakost oblika \eqref{6:30}. Primijetimo da je $\rho := \sqrt{2}\delta_{3s}/\sqrt{1-\delta_{2s}^2} \leq \sqrt{2} \delta_{3s}/\sqrt{1-\delta_{3s}^2} < 1$ za $\delta_{3s} < 1/\sqrt{3}$ i da je $(1-\rho)\tau = \sqrt{2}/\sqrt{1-\delta_{2s}} + \sqrt{1+\delta_s}/(1-\delta_{2s}) \leq 5.15$.
\end{proof}
Uzimanjem limesa kada $n \rightarrow \infty$ u \eqref{6:29} imamo da $\norm{\vec x^{\sharp}  - \vec x_S}_2 \leq \tau \norm{\vec{Ax}_{\bar S} + \vec e}_2$ ako je $\vec x^{\sharp} \in \C^N$ neko gomili\v{s}te niza $(\vec x^n)$. To gomili\v{s}te sigurno postoji zbog ograni\v{c}enosti od $\norm{\vec x^n}$. Dakle, $\norm{\vec x - \vec x^{\sharp}} \leq \norm{\vec x_{\bar S}}_2 + \norm{\vec x_S - \vec x^{\sharp}}_2$ zbog nejednakosti trokuta, pa odabirom $S$ kao skupa indeksa $s$ apsolutno najve\'cih komponenti vektora $\vec x$ dobijemo
\begin{equation}\label{6:32}
    \norm{\vec x - \vec x^{\sharp}}_2 \leq \sigma_s(\vec x)_2 + \tau \norm{\vec{Ax}_{\bar S} + \vec e}_2.
\end{equation}

\noindent
Naredni teorem daje ocjene aproksimacija.
\begin{thm}\label{tm:6:21}
    Neka je $\vec A \in \C^{m \times N}$ takva da $\delta_{3s} < 1/\sqrt{3}$. Tada za svaki $\vec x \in \C^N$ i $\vec e \in \C^m$, niz $(\vec x^n)$ definiran s \eqref{iht} ili s \eqref{htp_1}, \eqref{htp_2} s $\vec y = \vec{Ax} + \vec e,\ \vec x^0 = \vec 0$ i $2s$ umjesto $s$, za svaki $n \geq 0$ zadovoljava
    \begin{align*}
        & \norm{\vec x - \vec x^n}_1 \leq C \sigma_s(\vec x)_1 + D \sqrt{s} \norm{\vec e}_2 + 2 \rho^n \sqrt{s} \norm{\vec x}_2,\\[0.5em]
        & \norm{\vec x - \vec x^n}_2 \leq \frac{C}{\sqrt{s}}  \sigma_s(\vec x)_1 + D \norm{\vec e}_2 + 2 \rho^n \norm{\vec x}_2,
    \end{align*}
    gdje konstante $C, D > 0$ i $0<\rho < 1$ ovise samo o $\delta_{6s}$. Posebno, ako je $\vec x^{\sharp} \in \C^N$ gomili\v{s}te niza $(\vec x^n)$, onda
    \begin{align*}
        & \norm{\vec x - \vec x^{\sharp}}_1 \leq C \sigma_s(\vec x)_1 + D \sqrt{s} \norm{\vec e}_2, \\[0.5em]
        & \norm{\vec x - \vec x^{\sharp}}_2 \leq \frac{C}{\sqrt{s}}  \sigma_s(\vec x)_1 + D \norm{\vec e}_2 .
    \end{align*}
\end{thm}

\noindent
Za dokaz potrebna nam je lema.
\begin{lem}\label{lem:6:23}
    Neka je $\vec A \in \C^{m \times N}$ takva da $\delta_s < 1$. Za $\kappa, \tau > 0, \xi \geq 0$ i $\vec e \in \C^m$, pretpostavimo da vektori $\vec x, \vec x' \in \C^N$ zadovoljavaju $\norm{\vec x'}_0 \leq \kappa s$ i 
    \begin{equation*}
        \norm{\vec x_T - \vec x'}_2 \leq \tau \norm{\vec{Ax}_{\bar T} + \vec e}_2 + \xi 
    \end{equation*}
    gdje je T skup indeksa od $2s$ apsolutno najve\'cih komponenti od $\vec x$. Tada za $1 \leq p \leq 2$ vrijedi
    \begin{equation}\label{6:35}
        \norm{\vec x - \vec x'}_p \leq \frac{1+c_{\kappa} \tau}{s^{1-1/p}}\sigma_s(\vec x)_1 + d_{\kappa} \tau s^{1/p - 1/2} \norm{\vec e}_2 + d_{\kappa} s^{1/p - 1/2} \xi, 
    \end{equation}
    gdje konstante $c_{\kappa}, d_{\kappa}$ ovise samo o $\kappa$.
\end{lem}

\begin{proof}
    Iz \v{c}injenice da je vektor $\vec x_T - \vec x'$ $(2+\kappa)s$-rijedak slijedi
    \begin{align}
        \norm{\vec x - \vec x'}_p & \leq \norm{\vec x_{\bar T}}_p + \norm{\vec x_T - \vec x'}_p \leq \norm{\vec x_{\bar T}}_p + ((2+ \kappa)s)^{1/p-1/2} \norm{\vec x_T - \vec x'}_2 \nonumber  \\[0.5em]
        & \leq \norm{\vec x_{\bar T}}_p + \sqrt{2+ \kappa}s^{1/p - 1/2}(\tau \norm{\vec{Ax}_{\bar T} + \vec e}_2 + \xi). \label{6:36}
    \end{align}
    Neka je $S \subseteq T$ skup indeksa $s$ najve\'cih komponenti vektora $\vec x$. Prema propoziciji \ref{osnovna_ocjena_lp_greske}
    \begin{equation}\label{6:37}
        \norm{\vec x_{\bar T}}_p = \sigma_s(\vec x_{\bar S})_p \leq \frac{1}{s^{1-1/p}} \norm{\vec x_{\bar S}}_1 = \frac{1}{s^{1 - 1/p}} \sigma_s(\vec x)_1.  
    \end{equation}
    Particionirajmo $\bar T$ kao $\bar T = S_2 \cup S_3 \cup \dots$, gdje je
    \begin{align*}
        S_2:& \text{ skup indeksa $s$ apsolutno najve\'cih komponenti od $\vec x$ u $\bar T$},\\[0.5em]
        S_3:& \text{ skup indeksa $s$ apsolutno najve\'cih komponenti od $\vec x$ u $\overline{T \cup S_2}$},\\[0.5em]
        \vdots
    \end{align*}
    Dakle,
    \begin{align*}
        \norm{\vec{Ax}_{\bar T} + \vec e}_2 & \leq \sum_{k \geq 2} \norm{\vec{Ax}_{S_k}}_2 + \norm{\vec e}_2 \leq \sum_{k \geq 2} \sqrt{1+\delta_s} \norm{\vec x_{S_k}}_2 + \norm{\vec e}_2 \\[0.5em]
        & \leq \sqrt{2} \sum_{k \geq 2} \norm{\vec x_{S_k}}_2 + \norm{\vec e}_2.
    \end{align*}
    Iz leme \ref{lem:6:10} imamo
    \begin{equation*}
        \sum_{k \geq 2} \norm{\vec x_{S_k}}_2 \leq \frac{1}{s^{1/2}} \norm{\vec x_{\bar S}}_1 = \frac{1}{s^{1/2}} \sigma_s(\vec x)_1.  
    \end{equation*}
    Stoga,
    \begin{equation}\label{6:38}
        \norm{\vec{Ax}_{\bar T} + \vec e}_2 \leq \frac{\sqrt{2}}{s^{1/2}} \sigma_s(\vec x)_1 + \norm{\vec e}_2. 
    \end{equation}
    Kombiranjem \eqref{6:36}, \eqref{6:37} i \eqref{6:38} dobivamo \eqref{6:35} s $c_{\kappa} = \sqrt{4+2 \kappa}$ i $d_{\kappa} = \sqrt{2 + \kappa}$.
\end{proof}

\begin{proof}[Dokaz (Teorem \ref{tm:6:21}).]
    Za dani $\vec x \in C^N$ i $\vec e \in \C^m$, teorem \ref{tm:6:18} implicira da postoje $0<\rho<1$ i $\tau > 0$ koji ovise samo o $\delta_{6s}$, takvi da za svaki $n \geq 0$ vrijedi 
    \begin{equation*}
        \norm{\vec x_T - \vec x^n}_2 \leq \tau \norm{\vec{Ax}_{\bar T} + \vec e}_2 + \rho^n \norm{\vec x_T}_2,
    \end{equation*}
    gdje je $T$ skup indeksa $2s$ apsolutno najve\'cih komponenti od $\vec x$. Prema lemi \ref{lem:6:23} s $\vec x' = \vec x^n$ i $\xi = \rho^n \norm{\vec x_T}_2$ imamo da za svaki $1 \leq p \leq 2$ vrijedi,
    \begin{equation*}
        \norm{\vec x - \vec x^n}_p \leq \frac{C}{s^{1-1/p}} \sigma_s(\vec x)_1 + D s^{1/p - 1/2} \norm{\vec e}_2 + 2 \rho^n s^{1/p - 1/2} \norm{\vec x}_2, 
    \end{equation*}
    gdje $C,D > 0$ ovise samo o $\tau$, pa stoga samo o $\delta_{6s}$. Tra\v{z}ene ocjene su poseban slu\v{c}aj za $p=1$ i $p=2$.
\end{proof}

\section[Analiza greedy algoritama][Analiza greedy algoritama]{Analiza greedy algoritama}
U ovom dijelu rada pokazati \'cemo uz koje uvjete OMP i CoSaMP algoritmi uspje\v{s}no rekonstruiraju rijetke vektore. No, kod ove klase algoritama nisu dovoljni uvjeti restriktivne izometri\v{c}nosti. Uzmimo na primjer $1 < \eta < \sqrt{s}$ i definiramo $(s+1) \times (s+1)$ matricu
\begin{equation}\label{6:39}
    \vec A :=
    \left[
        \begin{array}{c | c}
            \vec I & \begin{matrix*}
                \frac{\eta}{s}\\ 
                \vdots\\
                \frac{\eta}{s}\\ 
            \end{matrix*}\\
            \hline
            0 \cdots 0 & \sqrt{\frac{s-\eta^2}{s}}\\
        \end{array}
    \right]
\end{equation}
Lako slijedi
\begin{equation*}
    \vec A^* \vec A - \vec I =
    \left[
        \begin{array}{c | c}
            \vec 0 & \begin{matrix*}
                \frac{\eta}{s}\\ 
                \vdots\\
                \frac{\eta}{s}\\ 
            \end{matrix*}\\
            \hline
            \frac{\eta}{s}  \cdots  \frac{\eta}{s} & 0
        \end{array}
    \right]
\end{equation*}
\v{S}to pokazuje da su svojstvene vrijednosti te matrice $-\eta/\sqrt{s},\ \eta/\sqrt{s}$ i $0$ kratnosti $s-1$. Stoga
\begin{equation*}
    \delta_{s+1} = \norm{\vec A^* \vec A - \vec I}_2 = \frac{\eta}{\sqrt{s}}. 
\end{equation*}
Me\dj utim $s$-rijedak vektor $\vec x = [1, \dots, 1, 0]^T$ se ne mo\v{z}e rekonstruirati iz $\vec y = \vec{Ax}$ u $s$ iteracija, po\v{s}to se u prvoj iteraciji odabere krivi indeks $s+1$. Taj problem mo\v{z}emo zaobi\'ci ve\'cim brojem iteracija ili modifikacijom algoritma tako da odbacuje krive indekse, \v{s}to zapravo CoSaMP radi.

\subsection[OMP algoritam][OMP algoritam]{OMP algoritam}
Zapravo, prou\v{c}avati \'cemo malo generalniji algoritam koji zapo\v{c}inje skupom $S^0$ i za koji je
\begin{equation}\label{6:40}
    \vec x^0 := \argmin\{\norm{\vec y - \vec{Az}},\ \supp(\vec z) \subseteq S^0\},
\end{equation}
a \v{c}ije iteracije su oblika,
\begin{align}
    &S^{n+1} = S^n \cup L_1(\vec A^*(\vec y - \vec{Ax}^n)), \tag{$OMP'_1$}\label{omp'_1}\\[0.5em]
    &x^{n+1} = \argmin\{\norm{\vec y - \vec{Az}_2,\ \supp(\vec z) \subseteq S^{n+1}}\}. \tag{$OMP'_2$}\label{omp'_2}
\end{align}
Standarni OMP algoritam odgovara odabiru $S^0 = \emptyset$ i $\vec x^0 = \vec 0$. Sljede\'ca propozicija klju\v{c}an je alat za analizu OMP algoritma, dokaz \'cemo dati kasnije.
\begin{prop}\label{prop:6:24}
    Neka je $\vec A \in \C^{m \times N}$, i neka je $\vec y = \vec{Ax} + \vec e$ za neki $s$-rijedak $\vec x \in \C^N$ takav da $S = \supp(\vec x)$ te $\vec e \in \C^m$. Nadalje, neka je $(\vec x^n)$ niz definiran s \eqref{omp'_1}, \eqref{omp'_2}. Sa $s^0$ ozna\v{c}imo kardinalitet skupa $S^0$ i neka je $s' = \card(S \backslash S^0)$. Ako je $\delta_{s+s^0+12s'} < \frac{1}{6}$, tada postoji konstana $C > 0$ koja ovisi samo o $\delta_{s+s^0+12s'}$ takva da
    \begin{equation*}
        \norm{\vec y - \vec{Ax}^{\bar n}}_2 \leq C \norm{\vec e}_2,\quad \quad \bar n = 12s'.
    \end{equation*}
\end{prop}
Primijetimo da za $\vec e = \vec 0$ i $S^0 = \emptyset$, prethodna propozicija implicira egzaktnu rekonstrukciju putem \eqref{omp'_1} i \eqref{omp'_2} u $12s$ iteracija. 

\begin{thm}
    Pretpostavimo da je $\vec A \in \C^{m \times N}$ takva da je 
    \begin{equation*}
        \delta_{13s} < \frac{1}{6}. 
    \end{equation*}
    Tada postoji konstana $C>0$ koja ovisi samo o $\delta_{13s}$ takva da za sve $\vec x \in \C^N$ i $\vec e \in \C^m$, niz $(\vec x^n)$ definiran s \eqref{omp'_1}, \eqref{omp'_2} za $\vec y = \vec{Ax}$ zadovljava
    \begin{equation*}
        \norm{\vec y - \vec{Ax}^{12s}}_2 \leq C \norm{\vec{Ax}_{\bar S} + \vec e}_2
    \end{equation*}
    za svaki $S \subseteq [N],\ \card(S) = s$. Nadalje, ako je $\delta_{26s} < 1/6$, tada postoje konstante $C,D>0$ koje ovise samo o $\delta_{26s}$ takve da za svaki $\vec x \in \C^N$ i $\vec e \in \C^m$, niz $(\vec x^n)$ zadovoljava
    \begin{equation*}
        \norm{\vec x - \vec x^{24s}}_p \leq \frac{C}{s^{1-1/p}} \sigma_s(\vec x)_1 + Ds^{1/p - 1/2} \norm{\vec e}_2  
    \end{equation*}
    za $1 \leq p \leq 2$.
\end{thm}
\begin{proof}
    Uzmimo $S \subseteq [N]$ takav da $\card(S)=s$. Vektor $\vec y$ mo\v{z}emo zapisati kao $\vec y = \vec{Ax}_S + \vec e'$ gdje je $\vec e' := \vec{Ax}_{\bar S} + \vec e$. Primjenimo propoziciju \ref{prop:6:24} za $S^0 = \emptyset$, 
    \begin{equation*}
        \norm{\vec y- \vec{Ax}^{12s}}_2 \leq C \norm{\vec e'}_2 = C \norm{\vec{Ax}_{\bar S} + \vec e}_2 
    \end{equation*}
    za neku konstantu $C>0$ koja ovisi samo o $\delta_{12s} < 1/6$. Iz 
    \begin{align*}
        \norm{\vec y - \vec{Ax}^{24s}}_2 & = \norm{\vec A(\vec x_T - \vec x^{24s}) + \vec{Ax}_{\bar T} + \vec e}_2\\[0.5em]
        & \geq \norm{\vec A(\vec x_T - \vec{x}^{24s})}_2 - \norm{\vec{Ax}_{\bar T} + \vec e}_2\\
        & \geq \sqrt{1 - \delta_{2s}} \norm{\vec x^{24s} - \vec x_T}_2 - \norm{\vec{Ax}_{\bar T} + \vec e}_2,
    \end{align*}
    slijedi
    \begin{equation*}
        \norm{\vec x^{24s} - \vec x_T}_2 \leq \frac{C'+1}{\sqrt{1 - \delta_{26s}}} \norm{\vec{Ax}_{\bar T} + \vec e}_2. 
    \end{equation*}
    Lema \ref{lem:6:23} s $\xi = 0$ daje tvrdnju.
\end{proof}

Za dokaz propozicije \ref{prop:6:24} trebat \'ce nam sljede\'ca lema.

\begin{lem}\label{lem:6:26}
    Neka je $(\vec x^n)$ niz definiran s \eqref{omp'_1}, \eqref{omp'_2} uz $\vec y = \vec{Ax} + \vec e$ za neki $s$-rijedak vektor $\vec x \in \C^N$ i za neki $\vec e \in \C^m$. Tada, za $n \geq 0$, $T \subseteq [N]$ koji nije sadr\v{z}an u $S^n$ i $\vec z  \in \C^N$s nosa\v{c}em na T,
    \begin{align*}
        \norm{\vec y &- \vec{Ax}^{n+1}}_2^2\\
        &\leq \norm{\vec y - \vec{Ax}^n}_2^2 - \frac{\norm{\vec A(\vec z - \vec x^n)}_2^2}{\norm{\vec z_{T \backslash S^n}}^2_1} \max \{0, \norm{\vec y - \vec{Ax}^n}_2^2 - \norm{\vec y - \vec{Az}}_2^2 \}\\
        &\leq \norm{\vec y - \vec{Ax}^n}_2^2 - \frac{1 - \delta}{\card(T \backslash S^n)} \max \{0, \norm{\vec y - \vec{Ax}^n}_2^2 - \norm{\vec y - \vec{Az}}_2^2 \},
    \end{align*}
    gdje je $\delta := \delta_{\card(T \cup S^n)}$.
\end{lem}

\begin{proof}
    Druga nejednost slijedi iz prve koriste\'ci
    \begin{align*}
        \norm{\vec A(\vec x^n - \vec z)}_2^2 &\geq (1-\delta)\norm{\vec x^n - \vec z}_2^2 \geq (1-\delta)\norm{(\vec x^n - \vec x)_{T \backslash S^n}}_2^2,\\[0.5em]
        \norm{\vec z_{T \backslash S^n}}_1^2 &\leq \card(T \backslash S^n) \norm{\vec z_{T \backslash S^n}}_2^2 = \card(T \backslash S^n) \norm{(\vec x^n - \vec z)_{T \backslash S^n}}_2^2.
    \end{align*}
    Iz leme \ref{lem:3:3} vidimo da se $\ell_2$ norma reziduala smanjuje za barem $|(\vec A^*(\vec y - \vec {Ax}^n))_{j^{n+1}}|^2$, gdje smo $j^{n+1}$ izabrali kao indeks najve\'ce komponente vektora $\vec A^*(\vec y - \vec{Ax}^n)$. Stoga prva nejednakost slijedi iz
    \begin{equation}\label{6:41}
        |(\vec A^*(\vec y - \vec{Ax}^n))_{j^{n+1}}|^2 \geq \frac{\norm{\vec A (\vec z - \vec x^n)}_2^2}{\norm{\vec z_{T \backslash S^n}}_1^2} (\norm{\vec y - \vec{Ax}^n}_2^2 - \norm{\vec y - \vec{Az}}_2^2)
    \end{equation}
    za $\norm{\vec y - \vec{Ax}^n}_2^2 \geq \norm{\vec y  - \vec{Az}}_2^2$. Iz leme \ref{lem:3:4} imamo da je $(\vec A^*(\vec y - \vec{Ax}^n))_{S^n} = \vec 0$, stoga
    \begin{align}
        \Re \langle \vec A(\vec z &- \vec x^n),  \vec y - \vec{Ax}^n \rangle \nonumber \\[0.5em]
        & = \Re \langle \vec z - \vec x^n, \vec A^*(\vec y - \vec{Ax}^n)\rangle = \Re \langle \vec z - \vec x^n, (\vec A^*(\vec y - \vec{Ax}^n))_{\overline{S^n}}\rangle \nonumber \\[0.5em]
        & = \Re \langle (\vec z - \vec x^n)_{T \backslash S^n} , (\vec A^*(\vec y - \vec{Ax}^n))_{T \backslash S^n}\rangle \nonumber \\[0.5em]
        & \leq \norm{(\vec z - \vec x^n)_{T \backslash S^n}}_1 \norm{\vec A^*(\vec y - \vec {Ax}^n)}_{infty} \nonumber \\[0.5em]
        & = \norm{\vec z_{T \backslash S^n}}_1 |(\vec A^*(\vec y - \vec {Ax}^n))_{j^{n+1}}|. \label{6:42}
    \end{align}
    Nadalje, 
    \begin{align}
        2 \Re \langle \vec A(\vec z &- \vec x^n), \vec y - \vec{Ax}^n \rangle \nonumber \\[0.5em]
        & = \norm{\vec A(\vec z - \vec x^n)}_2^2 + \norm{\vec y - \vec{Ax}^n}_2^2 - \norm{\vec A(\vec z - \vec x^n) - (\vec y - \vec{Ax}^n)}_2^2 \nonumber \\[0.5em]
        & = \norm{\vec A(\vec z - \vec x^n)}_2^2 + (\norm{\vec y - \vec{Ax}^n}_2^2 - \norm{\vec y - \vec{Az}}_2^2) \nonumber \\[0.5em]
        & \geq \norm{\vec A (\vec z - \vec x^n)}_2 \sqrt{\norm{\vec y - \vec{Ax}^n}_2^2 - \norm{\vec y - \vec{Az}}_2^2}, \label{6:43}
    \end{align}
    gdje zadnja nejednakost slijedi iz aritmeti\v{c}ko geometrijske nejednakosti. Kvadriramo \eqref{6:42}, \eqref{6:43} te komobiniranjem dobivamo
    \begin{equation*}
        \norm{\vec A(\vec z - \vec x^n)}_2^2 (\norm{\vec y - \vec {Ax}^n}_2^2 - \norm{\vec y - \vec{Az}}_2^2) \leq \norm{\vec z_{T \backslash S^n}}_1^2 |(\vec A^*(\vec y - \vec {Ax}^n))_{j^{n+1}}|^2.
    \end{equation*}
\end{proof}

\begin{proof}[Dokaz (Propozicija \ref{prop:6:24})]
    Tvrdnju dokazujemo indukcijom po $\card(S \backslash S^0)$. Ako je taj broj nula, tj. $S \subseteq S^0$ onda prema definicija od $\vec x^0$ imamo
    \begin{equation*}
        \norm{\vec y - \vec{Ax}^0}_2 \leq \norm{\vec y - \vec{Ax}}_2 = \norm{\vec e}_2, 
    \end{equation*}
    pa tvrdnja vrijedi za $C=1$. Pretpostavimo da tvrdnja vrijedi za sve $S$ i $S^0$ takve da je $\card(S \backslash S^0) \leq s'-1,\ s' \geq 1$. \v{Z}elimo pokazati da tvrdnja vrijedi za $s' = \card(S \backslash S^0)$. Neka je $T^0 = \emptyset$ i 
    \begin{equation*}
        T^l = \{\text{indeksi $ 2^{l-1}$ apsolutno najve\'cih komponeneti vektora }\vec x_{\overline{S^0}}\}. 
    \end{equation*}
    Primijetimo da su ti skupovi podskupovi od $S \backslash S^0$. Nadalje, definirajmo
    \begin{equation*}
        \tilde{\vec x}^l := \vec x_{\overline{S^0 \cup T^l}}, \quad l \geq 0.
    \end{equation*}
    Primijetimo da posljedni $T^l$, tj. $T^{\lceil \log_2(s')\rceil + 1}$ jednak cjelom skupu $S \backslash S^0$, pa stoga $\tilde{\vec x}^l = \vec 0$. Neka je $\mu > 0$ konstanta, koju \'cemo kasnije odabrati. Po\v{s}to je $\norm{\tilde{\vec x}^{l-1}}_2^2 \geq \mu \norm{\tilde{\vec x}^l} = 0$ za zadnji indeks, mo\v{z}emo izabrati $1 \leq L \leq \lceil \log_2(s')\rceil + 1$ takav da
    \begin{equation*}
        \norm{\tilde{\vec x}^{L-1}}_2^2 \geq \mu \norm{\tilde{\vec x}^L}_2^2. 
    \end{equation*}
    To implicira (mogu\'ce prazan) niz nejednakosti
    \begin{equation*}
        \norm{\tilde{\vec x}^0}_2^2 < \mu \norm{\tilde{\vec x}^1}_2^2, \dots \norm{\tilde{\vec x}^{L-2}}_2^2 < \mu \norm{\tilde{\vec x}^{L-1}}_2^2.
    \end{equation*}
    Za svaki $l \in [L]$, primijenimo lemu \ref{lem:6:26} na vektor $\vec z = \vec x - \tilde{\vec x}^l$, koji ima nosa\v{c} $S^0 \cup T^l$. Uzimaju\'ci u obzir da $(S^0 \cup T^l) \cup S^n \subseteq S \cup ^n$ i da $(S^0 \cup T^l) \backslash S^n \subseteq (S^0 \cup T^l) \backslash S^0 = T^l$, te oduzimaju\'ci $\norm{\vec y - \vec{Az}}_2^2 = \norm{\vec A \tilde{\vec x}^l + \vec e}_2^2$ dobivamo
    \begin{align*}
        \max \{0,&\ \norm{\vec y - \vec{Ax}^{n+1}}_2^2 - \norm{\vec{A}\tilde{\vec x}^l + \vec e}_2^2 \}\\[0.5em]
        & \leq \bigg(1- \frac{1-\delta_{s+n}}{\card(T^l)} \bigg) \max \{0,\ \norm{\vec y - \vec{Ax}^n} - \norm{\vec{A}\tilde{\vec x}^l + \vec e}_2^2\}\\[0.5em]
        & \leq \exp \bigg( -\frac{1-\delta_{s+n}}{\card(T^l)} \bigg) \max \{0,\ \norm{\vec y - \vec{Ax}^n} - \norm{\vec{A}\tilde{\vec x}^l + \vec e}_2^2\}.
    \end{align*}
    Za svaki $K \geq 0$ i za $n,k \geq 0$ takve da je $n+k \leq K$, imamo
    \begin{align*}
        \max \{ 0,&\ \norm{\vec y - \vec{Ax}^{n+k}}_2^2 - \norm{\vec A \tilde{\vec x}^l + \vec e}\}\\[0.5em]
        & \leq \exp \bigg( - \frac{k(1-\delta_{s+K})}{\card(T^l)} \bigg) \max \{ 0,\ \norm{\vec y - \vec{Ax}^{n}}_2^2 - \norm{\vec A \tilde{\vec x}^l + \vec e}\}.
    \end{align*}
    Separiraju\'ci slu\v{c}ajeve u maksimumu na desnoj strani lako vidimo
    \begin{equation*}
        \norm{\vec y - \vec{Ax}^{n+k}}_2^2 \leq \exp \bigg( -\frac{k(1-\delta_{s+K})}{\card(T^l)} \bigg) \norm{\vec y - \vec{Ax}^n}_2^2 + \norm{\vec A \tilde{\vec x}^l + \vec e}_2^2.
    \end{equation*}
    Za $\kappa \geq 0$, koji \'cemo kasnije odabrati, uzastopnom primjenom prethodne nejednakosti za
    \begin{equation*}
        \kappa_1 := \kappa \card(T^1),\dots,\ \kappa_L := \kappa \card(T^l),\ K:=k_1+ \cdots k_L,\ \nu := \exp(\kappa(1-\delta_{s+K}))
    \end{equation*}
    slijedi
    \begin{align*}
        \norm{\vec y - \vec{Ax}^{k_1}}_2^2 & \leq \frac{1}{\nu} \norm{\vec y - \vec {Ax}^{0}}_2^2 + \norm{\vec{A}\tilde{\vec x}^{1} + \vec e}_2^2,\\[0.5em]
        \norm{\vec y - \vec{Ax}^{k_1+k_2}}_2^2 & \leq \frac{1}{\nu} \norm{\vec y - \vec {Ax}^{k_1}}_2^2 + \norm{\vec{A}\tilde{\vec x}^{2} + \vec e}_2^2,\\[0.5em]
        & \vdots\\[0.5em]
        \norm{\vec y - \vec{Ax}^{k_1+\cdots+k_L}}_2^2 & \leq \frac{1}{\nu} \norm{\vec y - \vec {Ax}^{k_1+\cdots+k_{L-1}}}_2^2 + \norm{\vec{A}\tilde{\vec x}^{L} + \vec e}_2^2.
    \end{align*}
    Kombiniranjem tih nejednakosti dobivamo
    \begin{equation*}
        \norm{\vec y - \vec{Ax}^K}_2^2 \leq \frac{\vec y - \vec{Ax}^0}{\nu^L}+\frac{\norm{\vec A \tilde{\vec x}^{1}+ \vec e}}{\nu^{L-1}}+\cdots+\frac{\norm{\vec A \tilde{\vec x}^{L-1} + \vec e}}{\nu}+ \norm{\vec A \tilde{\vec x}^l + \vec e}_2^2.
    \end{equation*}
    \noindent
    Uva\v{z}imo da $\vec x - \tilde{\vec x}^0$ ima nosa\v{c} u $S^0 \cup T^0 = S^0$, definicija \eqref{6:40} implicira da je $\norm{\vec y - \vec {Ax}^0}_2^2 \leq \norm{\vec y - \vec A(\vec x - \tilde{\vec x}^0)}_2^2 = \norm{\vec A \tilde{\vec x}^0 + \vec e}_2^2$. Stoga,
    \begin{equation*}
        \norm{\vec y - \vec {Ax}^K}_2^2 \leq \sum_{l=0}^L \frac{\norm{\vec A \tilde{\vec x}^l + \vec e}_2^2}{\nu^{L-l}} \leq \sum_{l=0}^L \frac{2(\norm{\vec A \tilde{\vec x}^l}_2^2 - \norm{\vec e}_2^2)}{\nu^{L-l}}  
    \end{equation*}
    Primijetimo da za $l \leq L - 1$ i za $l = L$ vrijedi
    \begin{equation*}
        \norm{\vec A \tilde{\vec x}^l}_2^2 \leq (1+\delta_s)\norm{\tilde{\vec x}^l}_2^2 \leq (1+\delta_s)\mu^{L-1-l} \norm{\tilde{\vec x}^{L-1}}_2^2.
    \end{equation*}
    Zbog toga, imamo
    \begin{align*}
        \norm{\vec y - \vec{Ax}^K}_2^2 & \leq \frac{2(1+\delta_s)\norm{\tilde{\vec x}^{L-1}}}{\mu}\sum_{l=0}^L \bigg( \frac{\mu}{\nu} \bigg)^{L-l}+2 \norm{\vec e}_2^2 \sum_{l=0}^L \frac{1}{\nu^{L-l}}\\
        &\leq \frac{2(1+\delta_s)\norm{\tilde{\vec x}^{L-1}}_2^2}{\mu(1-\mu/\nu)}+ \frac{2\norm{\vec e}_2^2}{1-\nu}. 
    \end{align*}
    Uzmimo $\mu/\nu/2$ tako da $\mu(1-\mu/\nu)$ poprima maksimum $\nu/4$. Za $\alpha := \sqrt{8(1+\delta_s)/\nu}$ i $\beta := \sqrt{2/(1-\nu)}$,
    \begin{equation}\label{6:44}
        \norm{\vec y - \vec{Ax}^K}_2 \leq \alpha \norm{\tilde{\vec x}^{L-1}}_2 + \beta \norm{\vec e}_2.
    \end{equation}
    Nadalje, za $\gamma := \sqrt{1 - \delta_{s + s^0 + K}}$ imamo
    \begin{align*}
    \norm{\vec y - \vec{Ax}^K}_2 &= \norm{\vec A(\vec x - \vec x^K) + \vec e}_2 \geq \norm{\vec A(\vec x - \vec x^K)}_2 - \norm{\vec e}_2\\[0.5em]
        & \geq \gamma \norm{\vec x -  \vec x^K}_2 - \norm{\vec e}_2 \geq \gamma \norm{\vec x_{\overline{S^K}}} - \norm{\vec e}_2.
    \end{align*}
    Zaklju\v{c}ujemo
    \begin{equation}\label{6:45}
        \norm{\vec x_{\overline{S^K}}}_2 \leq \frac{\alpha}{\gamma} \norm{\tilde{\vec x}^{L-1}}_2 + \frac{\beta + 1}{\gamma} \norm{\vec e}_2.   
    \end{equation}
    Odaberimo sada $\kappa = 3$ tako da vrijedi
    \begin{equation*}
        \frac{\alpha}{\gamma} = \sqrt{\frac{8(1+\delta_s)}{(1-\delta_{s+s^0+K}) \exp(\kappa(1-\delta_{s+K}))}}  \leq 0.92 < 1,
    \end{equation*}
    po\v{s}to $\delta_s \leq \delta_{s+K} \leq \delta_{s+s^0+K} \leq \delta_{s+s^0+12s'} < 1/6$. Tu smo iskoristili \v{c}injenicu da je $L \leq \lceil \log_2(s') \rceil + 1$ da bi dobili
    \begin{equation*}
        K = \kappa(1+\cdots+2^{L-2} + \card(T^L)) < \kappa(2^{L-1}+s')\leq 3 \kappa s' = 9 s'.
    \end{equation*}
    Stoga u slu\v{c}aju da $((\beta+1)/\gamma)\norm{\vec e}_2 < (1- \alpha/\gamma) \norm{\tilde{\vec x}^{L-1}}_2$, iz \eqref{6:45} slijedi
    \begin{equation*}
        \norm{\vec x_{\overline{S^K}}}_2 < \norm{\tilde{\vec x}^{L-1}}_2 \quad \text{tj.} \quad \norm{(\vec x_{\overline{S^0}})_{S \backslash S^K}}_2 < \norm{(\vec x_{\overline{S^0}})_{(S \backslash S^0)\T^{L-1}}}_2.
    \end{equation*}
    Po\v{s}to $T^{L-1}$ sadr\v{z}i indekse $2^{L-2}$ apsolutno najve\'cih komponeneti od $\vec x_{\overline{S^0}}$, imamo
    \begin{equation*}
    \card(S \backslash S^k)< \card((S \backslash S^0) \backslash T^{L-1}) = s' - 2^{L-2}.
    \end{equation*}
    Primijetimo da izvr\v{s}avanje algoritma nakon $K$ iteracija odgovara zapo\v{c}injanju algoritma gdje $S^0$ zamjenimo sa $S^K$. Imamo da je $K \leq \kappa (1+\cdots+2^{L-2}+2^{L-1}) < 3 \cdot 2^L$, pa vrijedi
    \begin{equation*}
        s + \card(S^K)+12 \card(S \backslash S^K) \leq s + s^0 + K + 12(s' - 2^{L-2}) \leq s+ s^0 + 12s'
    \end{equation*}
    a od tuda slijedi $\card(S \backslash S^K) < s'$. Sada primjenimo pretpostavku indukcije
    \begin{equation*}
        \norm{\vec y - \vec{Ax}^{K+n}}_2 \leq C \norm{\vec e}_2, \quad \text{za } n=12\card(S \backslash S^K).
    \end{equation*}
    Dakle, broj potrebnih iteracija zadovoljava $K+n \leq 12s'$.\\
    U slu\v{c}aju da je $((\beta + 1)/\gamma)\norm{\vec e}_2 \geq (1-\alpha/\gamma)\norm{\tilde{\vec x}^{L-1}}_2$, \eqref{6:44} daje
    \begin{equation*}
        \norm{\vec y - \vec{Ax}^K}_2 \leq \frac{\alpha(\beta+1)}{\gamma - \alpha} \norm{\vec e}_2 + \beta \norm{\vec e}_2 =: C \norm{\vec e}_2,
    \end{equation*}
    gdje konstanta $C \geq 1$ ovisi samo o $\delta_{s+s^0+12s'}$. Pokazali smo da tvrdnja vrijedi za $s' = \card(S \backslash S^0)$.
\end{proof}

\subsection[CoSaMP algoritam][CoSaMP algoritam]{CoSaMP algoritam}
Prisjetimo se, CoSaMP zapo\v{c}inje $s$-rijetkim vektorom $\vec x^0 \in \C^N$ (uglavnom $\vec x^0 = \vec 0$) i generira niz $(\vec x^n)$ induktivno definiran sa
\begin{align}
    U^{n+1} & = \supp(\vec x^n) \cup L_{2s}(\vec A^*(\vec y - \vec {Ax}^n)),\tag{$CoSaMP_1$}\label{cosamp_1'}\\[0.5em]
    \vec u^{n+1} &= \argmin \{\norm{\vec y - \vec {Az}}_2,\ \supp(\vec z) \subseteq U^{n+1}\},\tag{$CoSaMP_2$}\label{cosamp_2'}\\[0.5em]
    \vec x^{n+1} &= H_s(\vec u ^{n+1}).\tag{$CoSaMP_3$}\label{cosamp_3'}
\end{align}
\begin{thm}\label{tm:6:27}
    Neka je $\vec A \in \C^{m \times N}$ takva da
    \begin{equation}\label{6:46}
        \delta_{4s} < \frac{\sqrt{\sqrt{11/3}-1}}{2} \approx 0.4782.  
    \end{equation}
    Tada, za $\vec x \in \C^N,\ \vec e \in \C^m$ i $S \subseteq [N],\ \card(S)=s$, niz $(\vec x^n)$ definiran s \eqref{cosamp_1'}, \eqref{cosamp_2'}, \eqref{cosamp_3'} za $\vec y = \vec{Ax} + \vec e$ zadovoljava
    \begin{equation}\label{6:47}
        \norm{\vec x^n - \vec x_S}_2 \leq \rho^n \norm{\vec x^0 - \vec x_S}_2 + \tau \norm{\vec{Ax}_{\bar S} + \vec e}_2,
    \end{equation}
    gdje $0<\rho<1$ i $\tau > 0$ ovise samo o $\delta_{4s}$.
\end{thm}

Primijetimo da \eqref{6:47} implicira ograni\v{c}enost niza $(\vec x^n)$ pa stoga imamo egzistenciju gomili\v{s}ta.

\begin{thm}\label{tm:6:28}
    Neka je $\vec A \in \C^{m \times N}$ takva da   
    \begin{equation*}
        \delta_{8s} < 0.4782. 
    \end{equation*}
    Tada za $\vec x \in \C^N$ i $\vec e \in \C^m$, niz $(\vec x^n)$ definiran s \eqref{cosamp_1'}, \eqref{cosamp_2'}, \eqref{cosamp_3'} za $\vec y = \vec{Ax}+\vec e,\ \vec x^0 = \vec 0$ i $2s$ umjesto $s$, vrijedi
    \begin{align*}
        \norm{\vec x - \vec x^n}_1 &\leq C \sigma_s(\vec x)_1 + D \sqrt{s}\norm{\vec e}_2 + 2 \rho^n \sqrt{s} \norm{\vec x}_2, \\[0.5em]
        \norm{\vec x - \vec x^n}_2 &\leq \frac{C}{\sqrt{s}} \sigma_s(\vec x)_2 + D \norm{\vec e}_2 + 2 \rho^n \norm{\vec x}_2,
    \end{align*}
    za svaki $n \leq 0$, gdje konstante $C,D >0 $ i $0 < \rho < 1$ ovise samo o $\delta_{8s}$. Posebno, ako je $\vec x^{\sharp} \in \C^N$ gomili\v{s}te niza $(\vec x^n)$, onda
    \begin{align*}
        \norm{\vec x - \vec x^{\sharp}}_1 &\leq C \sigma_s(\vec x)_1 +D \sqrt{s} \norm{\vec e}_2, \\[0.5em]  
        \norm{\vec x - \vec x^{\sharp}}_2 & \leq \frac{C}{\sqrt{s}} \sigma_s(\vec x)_1 + D \norm{\vec e}_2. 
    \end{align*}
\end{thm}

Teorem \ref{tm:6:28} slijedi iz teorema \ref{tm:6:27} koriste\'ci lemu \ref{lem:6:23} na isti na\v{c}in kao \v{s}to teorem \ref{tm:6:21} slijedi iz teorema \ref{tm:6:18}. Stoga \'cemo pokazati samo teorem \ref{tm:6:27}.

\begin{proof}[Dokaz (Teorem \ref{tm:6:27}).]
    Kao u dokazu teorema \eqref{tm:6:18} za $n \geq 0$ \v{z}elimo pokazati
    \begin{equation}\label{6:48}
        \norm{\vec x^{n+1} - \vec x_S}_2 \leq \rho \norm{\vec x^n - \vec x_S}_2 + (1-\rho)\tau \norm{\vec{Ax}_{\bar S} + \vec e}_2 
    \end{equation}
    za $0< \rho < 1$ i $\tau > 0 $ koje \'cemo kasnije odrediti. Tvrdnja \eqref{6:47} tada sljedi induktivnim argumentom. Da bi pokazali nejednakost \eqref{6:48} prou\v{c}it \'cemo svaki korak CoSaMP algoritma. Zbog jednostavnosti zanemarit \'cemo \v{c}lan s $\vec{Ax}_{\bar S} + \vec e$. \eqref{cosamp_1'} daje ocjenu za $\norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2$ u terminima $\norm{\vec x^n - \vec x_S}_2$, \eqref{cosamp_2'} daje ocjenu za $\norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2$ u terminima $\norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2$ i \eqref{cosamp_3'} daje ocjenu za $\norm{\vec x^{n+1}  - \vec x_S}_2$ u terminima $\norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2$ i $\norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2$. Kombiniranjem dobivenih ocjena, izvesti \'cemo ocjenu za $\norm{\vec x^{n+1} - \vec x_S}_2$ u terminima $\norm{\vec x^n - \vec x_S}_2$.\\
    \indent
    Zapo\v{c}nimo s \eqref{cosamp_3'}. Primijetimo da je $\vec x^{n+1}$ bolja (ili barem jednako dobra) aprokimacija vektora $\vec u^{n+1}$ od $\vec x_{S \cap U^{n+1}}$. Ozna\v{c}imo $S^{n+1}= \supp(\vec x^{n+1})$, $S^{n+1} \subseteq U^{n+1}$. Vrijedi,
    \begin{align*}
        \norm{(\vec x_S - \vec x^{n+1})_{U^{n+1}}}_2 &= \norm{\vec x_{S \cap U^{n+1}} - \vec x^{n+1}}_2\\[0.5em]
        & \leq \norm{\vec u^{n+1} - \vec x^{n+1}}_2 + \norm{\vec u^{n+1} - \vec x_{S \cap U^{n+1}}}_2\\[0.5em]
        & \leq 2 \norm{\vec u^{n+1} - \vec x_{S \cap U^{n+1}}}_2 = 2 \norm{(\vec x_S - \vec u ^{n+1})_{U^{n+1}}}_2.
    \end{align*}
    Nadalje, iz $(\vec x^{n+1})_{\overline{U^{n+1}}} = \vec 0$ i $(\vec u^{n+1})_{\overline{U^{n+1}}} = \vec 0$ slijedi
    \begin{align}
        \norm{\vec x_S - \vec x^{n+1}}_2^2 & = \norm{(\vec x_S - \vec x^{n+1})_{\overline{U^{n+1}}}}_2^2 + \norm{(\vec x_S - \vec x^{n+1})_{U^{n+1}}}_2^2 \nonumber \\[0.5em]
        & \leq \norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2^2 + 4\ \norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2^2. \label{6:49}
    \end{align}
    Iz \eqref{cosamp_2'} imamo da je vektor $\vec {Au}^{n+1}$ karakteriziran sa
    \begin{equation*}
        \langle \vec y - \vec {Au}^{n+1}, \vec{Az}\rangle  = 0 \quad \text{za } \supp(\vec z) \subseteq U^{n+1}.
    \end{equation*}
    To je ekvivalentno s $ \langle \vec A^*(\vec y - \vec{Au}^{n+1}), \vec z \rangle = 0$, tj. $(\vec A^*(\vec y - \vec{Au}^{n+1}))_{U^{n+1}} = \vec 0$, za $\supp(\vec z) \subseteq U^{n+1}$. Po\v{s}to je $\vec y = \vec{Ax}_S + \vec e'$ gdje je $\vec e' := \vec{Ax}_{\bar S} + \vec e$, imamo
    \begin{equation*}
        (\vec A^* \vec A(\vec x_S - \vec u^{n+1}))_{U^{n+1}} = - (\vec A^* \vec e')_{U^{n+1}}.
    \end{equation*}
    Slijedi,
    \begin{align*}
        \norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2 &\leq \norm{\big( (\vec I - \vec A^* \vec A)(\vec x_S - \vec u^{n+1}) \big)_{U^{n+1}}}_2 + \norm{(\vec A^* \vec e')_{U^{n+1}}}_2 \\[0.5em]
        & \leq \delta_{4s}\norm{\vec x_S - \vec u^{n+1}}_2 + \norm{(\vec A^*\vec e')_{U^{n+1}}}_2,
    \end{align*}
    gdje je zadnja nejednakost posljedica leme \ref{lem:6:16}. \\
    Mo\v{z}emo pretpostaviti da je $\norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2 > \norm{(\vec A^* \vec e')_{U^{n+1}}}_2/(1-\delta_{4s})$ jer u suprotnom \eqref{6:50} iz nastavka slijedi direktno. Dakle, iz $\norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2 > \norm{(\vec A^* \vec e')_{U^{n+1}}}_2$ slijedi
    \begin{align*}
        [\norm{(\vec x_S - \vec u^{n+1}_{U^{n+1}})}_2 &- \norm{(\vec A^* \vec e')_{U^{n+1}}}_2] \\[0.5em]
        & \leq \delta_{4s}^2 \norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2^2 + \delta_{4s}^2 \norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2^2.
    \end{align*}
    Iskoristimo $a^2 - b^2 = (a+b)(a-b)$
    \begin{align*}
        \delta_{4s}^2 \norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2 &\geq (1-\delta_{4s}^2)\\[0.5em]
        & \cdot \big( \norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2 - \frac{1}{1+\delta_{4s}} \norm{(\vec A^* \vec e')_{U^{n+1}}}_2  \big)\\[0.5em]
        & \cdot \big( \norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2 - \frac{1}{1-\delta_{4s}} \norm{(\vec A^* \vec e')_{U^{n+1}}}_2  \big).
    \end{align*}
    Srednji \v{c}lan na desnoj strani mo\v{z}emo ograni\v{c}iti odozgo predzadnjim \v{c}lanom da bi dobili
    \begin{equation*}
        \frac{\delta_{4s}^2}{1-\delta_{4s}^2} \norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2^2 \geq \big( \norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2 - \frac{1}{1-\delta_{4s}} \norm{(\vec A^* \vec e')_{U^{n+1}}}_2  \big)^2. 
    \end{equation*}
    Uzimanjem korijena i preslagivanjem imamo
    \begin{align}
        \norm{(\vec x_S - \vec u^{n+1})_{U^{n+1}}}_2^2 & \leq \frac{\delta_{4s}}{\sqrt{1-\delta_{4s}}}  \norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2 \nonumber \\[0.5em]
        & + \frac{1}{1-\delta_{4s}} \norm{(\vec A^* \vec e')_{U^{n+1}}}_2.\label{6:50} 
    \end{align}
    Nadalje, kao posljedica od \eqref{cosamp_1'}, ako $S^n$ ozna\v{c}uje nosa\v{c} od $\vec x^n$ i ako je $T^{n+1}$ skup indeksa $2s$ apsolutno najve\'cih komponenti od $\vec A^*(\vec y - \vec{Ax}^n)$, imamo
    \begin{equation*}
        \norm{(\vec A^*(\vec y - \vec {Ax}^n))_{S \cup S^n}}_2^2 \leq \norm{(\vec A^*(\vec y - \vec{Ax}^n))_{T^{n+1}}}_2^2.
    \end{equation*}
    Eliminiramo doprinos od $(S \cup S^n) \cap T^{n+1}$,
    \begin{equation*}
        \norm{(\vec A^*(\vec y - \vec{Ax}^n))_{(S \cup S^n) \backslash T^{n+1}}}_2 \leq \norm{(\vec A^*(\vec y - \vec {Ax}^n))_{T^{n+1} \backslash (S \cup S^n)}}_2.
    \end{equation*}
    Desnu stranu mo\v{z}emo zapisati kao
    \begin{equation*}
        \norm{(\vec A^*(\vec y - \vec {Ax}^n))_{T^{n+1} \backslash (S \cup S^n)}}_2 = \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec {Ax}^n))_{T^{n+1} \backslash (S \cup S^n)}}_2.
    \end{equation*}
    Za lijevu stranu imamo,
    \begin{align*}
        \norm{(\vec A^*(\vec y - \vec{Ax}^n))_{(S \cup S^n) \backslash T^{n+1}}}_2 & \geq \norm{(\vec x_S - \vec x^n)_{\overline{T^{n+1}}}}_2\\[0.5em]
        & - \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec {Ax}^n))_{(S \cup S^n) \backslash T^{n+1}}}_2.
    \end{align*}
    Dakle,
    \begin{align*}
        \norm{(\vec x_S - \vec x^n)_{\overline{T^{n+1}}}}_2 & \leq \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec{Ax}^n))_{(S \cup S^n) \backslash T^{n+1}}}_2\\[0.5em]
        & + \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec{Ax}^n))_{T^{n+1} \backslash (S \cup S^n)}}_2\\[0.5em]
        & \leq \sqrt{2} \norm{(\vec x^n - \vec x_S + \vec A^*(\vec y - \vec{Ax}^n))_{T^{n+1} \Delta (S \cup S^n)}}_2\\[0.5em]
        & \leq \sqrt{2} \norm{((\vec I - \vec A^* \vec A)(\vec x^n - \vec x_S))_{T^{n+1} \Delta (S \cup S^n)}}_2\\[0.5em]
        & + \sqrt{2} \norm{(\vec A^* \vec e')_{(S \cup S^n) \Delta T^{n+1}}}_2,
    \end{align*}
    gdje je $\vec y = \vec{Ax}_S + \vec e'$. Po\v{s}to prema \eqref{cosamp_1'} $T^{n+1} \subseteq U^{n+1}$ i $S^n \subseteq U^{n+1}$ prema \eqref{cosamp_2'}, lijeva strana gornje nejednakosti mo\v{z}e se ocijeniti kao
    \begin{equation*}
        \norm{(\vec x_S - \vec x^n)_{\overline{T^{n+1}}}}_2 \geq \norm{(\vec x_S - \vec x^n)_{\overline{U^{n+1}}}}_2 = \norm{(\vec x_S)_{\overline{U^{n+1}}}}_2 = \norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2.
    \end{equation*}
    Desnu stranu mo\v{z}emo ocijeniti odozgo koriste\'ci lemu \ref{lem:6:16},
    \begin{align}
        \norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2 & \leq \sqrt{2} \delta_{4s} \norm{\vec x^n - \vec x_S}_2 \nonumber \\[0.5em]
            & + \sqrt{2}\norm{(\vec A^*\vec e')_{(S \cup S^n) \Delta T^{n+1}}}_2.\label{6:51}
    \end{align}
    Kombiniranjem \eqref{6:49}, \eqref{6:50} i nejednakosti $a^2 + (b+c)^2 \leq (\sqrt{a^2 + b^2} + c)^2$ imamo
    \begin{align*}
        \norm{\vec x_S - \vec x^{n+1}}_2^2 & \leq \norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2^2\\[0.5em]
        + & \bigg( \frac{\delta_{4s}}{\sqrt{1-\delta_{4s}^2}}\norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2 + \frac{1}{1-\delta_{4s}} \norm{(\vec A^* \vec e')_{U^{n+1}}}_2   \bigg)^2\\[0.5em]
        \leq & \bigg( \frac{1+3 \delta_{4s}^2}{\sqrt{1-\delta_{4s}^2}}\norm{(\vec x_S - \vec u^{n+1})_{\overline{U^{n+1}}}}_2 + \frac{2}{1-\delta_{4s}} \norm{(\vec A^* \vec e')_{U^{n+1}}}_2   \bigg)^2.
    \end{align*}
    Uva\v{z}imo sada \eqref{6:51}
    \begin{align*}
        \norm{\vec x_S - \vec x^{n+1}}_2 & \leq \sqrt{\frac{2\delta_{4s}^2 (1+3 \delta_{4s}^2)}{1 - \delta_{4s}^2}} \norm{\vec x^n - \vec x_S}_2\\[0.5em]
        & + \sqrt{\frac{2(1+3 \delta_{4s}^2)}{1- \delta_{4s}^2}} \norm{(\vec A^* \vec e')_{(S \cup S^n) \Delta T^{n+1}}}_2  + \frac{2}{1-\delta_{4s}} \norm{(\vec A^* \vec e')_{U^{n+1}}}_2. 
    \end{align*}
    Prema lemi \ref{lem:6:20} nejednakost \eqref{6:48} vrijedi za
    \begin{equation}
        \rho = \sqrt{\frac{2\delta_{4s}^2 (1+3 \delta_{4s}^2)}{1 - \delta_{4s}^2}}, \quad (1-\rho)\tau = \sqrt{\frac{2(1+3 \delta_{4s}^2)}{1- \delta_{4s}^2}} + \frac{2 \sqrt{1+ \delta_{4s}}}{1-\delta_{4s}}. 
    \end{equation}
    Konstanta $\rho$ je manja od $1$ ako i samo je $6 \delta_{4s}^4 + 3 \delta_{4s}^2 - 1 < 0$. To se dogodi onda kada je $\delta_{4s}^2 < (\sqrt{11/3} - 1)/4$.

\end{proof}

% KRAJ
% Na kraju diplomkog rada stavlja se  bibliografija
% Najprije definiramo nacin prikazivanja bibliografije, u ovom slucaju verzija amsplain stila
\bibliographystyle{babamspl} % babamspl ili babplain

% U datoteku diplomski.bib se stavljaju bibliografske reference
% Bibliografske reference u bib formatu se mogu dobiti iz MathSciNet baze, Google Scholara, ArXiva, ...
\bibliography{diplomski}
 
\pagestyle{empty} % ne zelimo brojanje sljedecih stranica

% I na koncu idu sazeci na hrvatskom i engleskom

\begin{sazetak}
    U ovom radu upoznali smo se s matemati\v{c}kim temeljima teorije sa\v{z}etog uzorkovanja. Rje\v{s}avali smo problem rekonstrukcije vektora $\vec x \in \C^N$ iz vektora mjerenja $\vec y = \vec {Ax}$ gdje je $\vec A \in \C^{m \times N}$. U slu\v{c}aju da je $N = m$ za rekonstrukciju je dovoljno rije\v{s}iti kvadratni sustav linearnih jednad\v{z}bi. Ako je $m < N$, klasi\v{c}na teorija linearne algebre ka\v{z}e da ovakvi sustavi mogu imati beskona\v{c}no rje\v{s}enja. No, uz dodatni uvjet rijetkosti vektora $\vec x$ pokazali smo da je rekonstrukcija mogu\'ca i \v{s}tovi\v{s}e postoje efikasni algoritmi za rje\v{s}enje. Prvo poglavlje zapo\v{c}inje uvodom u osnovne pojmove teorije rijetkih vektora. Dali smo definiciju rijetkih vektora, izveli smo ocjene za $\ell_p$-gre\v{s}ku najbolju $s$-rijetke aproksimacije vektora $\vec x$ i dali dva na\v{c}ina kako se mogu definirati kompresibilni vektori. To je od prakti\v{c}ne koristi po\v{s}to u primjeni rijetko rukujemo pravim rijetkim vektorima, ve\'c su to uglavnom kompresibilni vektori. Zatim smo istra\v{z}ili koji je minimalni broj mjerenja $m$ potrebnih za rekonstrukciju, te pokazujemo kako $\ell_0$-minimizacija \eqref{problem_minimizacije} daje rje\v{s}enje problema sa\v{z}etog uzorkovanja. Na\v{z}alost minimizacija $\ell_0$-norme je nekonveksan a uz to i NP-te\v{z}ak problem, \v{s}to smo pokazali tako da smo poznati NP-te\v{z}ak problem pokriva\v{c}a tro\v{c}lanim skupovima reducirali na \eqref{problem_minimizacije}. U drugom poglavlju dajemo pregled ostalih, efikasnih algoritama za rekonstrukciju. Ti algoritmi mogu se u grubo podijeliti na optimizacijske, greedy i grani\v{c}ne metode. Kod optimizacijskih algoritama najva\v{z}niji je BP (eng. \textit{basis pursuit}) algoritam ili $\ell_1$-minimizacija. Njega zapravo mo\v{z}emo shvatiti kao konveksnu relaksaciju problema \eqref{problem_minimizacije}. Nadalje, opisujemo OMP i CoSaMP greedy algoritme, te BT, IHT, HTP grani\v{c}ne metode. Tre\'ce poglavlje posve\'ceno je $\ell_1$-minimizaciji. Upoznajemo se sa svojstom nul-prostora matrice $\vec A$ te pokazujemo kako je ono dovoljan i nu\v{z}an uvjet za uspje\v{s}nu rekonstrukciju vektora $\vec x$ $\ell_1$-minimizacijom. Nadalje, uvodimo pojmove stabilnosti i robusnosti rekonstrukcijske metode. Neformalno, ta dva svojstva govore da se rekonstrukcijska metoda dobro pona\v{s}a s obzirom na defekte rijetkosti i na gre\v{s}ke mjerenja. Uz oja\v{c}ano svojstvo nul-prostora pokazujemo da je $\ell_1$-minimizacija stabilna i robusna. U \v{c}etvrtom poglavlju uvodimo koherenciju, koju mo\v{z}emo shvatiti kao mjeru kvalitete matrice rekonstrukcije. Slijede rezultati i ocjene vezane uz koherenciju te dajemo eksplicitnu konstrukciju odre\dj enih matrica male koherencije. Zatim analiziramo uz koje uvjete na koherenciju algoritmi iz drugog poglavlja posti\v{z}u egzaktnu rekonstrukciju. U petom poglavlju dajemo novu mjeru kvalitete matrice $\vec A$, svojstvo restriktivne izometri\v{c}nosti. Ona rje\v{s}ava nedostatke koherencije, tj. omogu\'cuje analizu algoritama za velike vrijednosti rijetkosti $s$. Isto kao kod koherencije provodimo analizu algoritama i pokazujemo uz koje uvjete ti algoritmi posti\v{z}u stabilnu i robusnu rekonstrukciju. Valja napomenuti da ovaj rad nije iscrpni pregled teorije sa\v{z}etog uzorkovanja. Naime, pokazuje se da je konstrukcija eksplicitnih matrica s dovoljno malim konstantama svojstva restriktivne izometri\v{c}nosti te\v{z}ak problem, koji nije rje\v{s}en. Veliki napredak napravili su Terence Tao i Emmanuel Candes u \cite{CandesTao}, gdje su pokazli da slu\v{c}ajne Gaussove matrice zadovoljavaju svojstvo restriktivne izometri\v{c}nosti s velikom vjerojatno\v{s}\'cu. To je otvorilo put stohasti\v{c}koj teoriji i slu\v{c}ajnim matricama u teoriju sa\v{z}etog uzorkovanja.
\end{sazetak}

\begin{summary}
    In this thesis we cover the mathematical foundations of the theory of compressive sensing. The problem in focus is the reconstruction of the vector $\vec x \in \C^N$ from the measurement vector $\vec y = \vec{Ax}$, where $\vec A \in \C^{m \times N}$. Given that $N=m$, it is enough to solve a square system of linear equations. If $m < N$, the classical theory of linear algebra tells us that such systems can have a infinite number of solutions. However, if we assume that the vector $\vec x$ is sparse, we are able to show that the reconstruction is not only possible, but efficient reconstruction algorithms exist. First chapter gives an introduction to the theory of sparse vectors. We define the notion of sparsity, derive the $\ell_p$ bound for the best $s$-term sparse approximation of a vector $\vec x$ and give two ways of defining compressible vectors. Which is of practical use because compressible vectors are much more common than perfectly sparse vectors in real world applications. Next, we investigate the minimal number $m$ of measurements necessary for a perfect reconstruction and we see how the $\ell_0$-minimization \eqref{problem_minimizacije} naturally arises as a reconstruction strategy. Unfortunately, $\ell_0$-minimization is non-convex and additionally a NP-hard problem. We show this fact by reducing the well known NP-hard problem of covering by $3$-sets to the problem of $\ell_0$-minimization. In the second chapter we give a summary of efficient reconstruction algorithms, which can be divided into three categories: optimization methods, greedy methos and threshold-based methods. In the optimization methods category the most important is the basis pursuit algorithm or $\ell_1$-minimization. We can think of it as a convex relaxation of the \eqref{problem_minimizacije} problem. Furthermore, we give a description of the OMP and CoSaMP greedy algorithms, and BT, IHT, HTP thresholding-based methods. The third chapter is dedicated to the $\ell_1$-minimization. We study the null-space property of the matrix $\vec A$ and show how it ensures a successful reconstruction via the $\ell_1$-minimization. We introduce the definition of stability and robustness of a reconstruction method and prove how with a stronger version of the null-space property, $\ell_1$-minimization has those properties. In chapter number four, we give the definition of coherence, which can be thought of as a reconstruction matrix $\vec A$ quality. We derive some bounds and give an explicit construction of matrices with small coherence. Next, we analyse the conditions for coherence under which the reconstruction algorithms give a successful reconstruction. In the fifth chapter we introduce the notion of restricted isometry property, which is in fact a new quality measure for the matrix $\vec A$, and this enables us to study the reconstruction algorithms for large values of sparsity $s$. Same as with the coherence, we analyse the reconstruction algorithms with regard to the restricted isometry property. We should note that this thesis is not covering the whole theory of compressive sensing as it turns out that the explicit construction of matrices with a small restricted isometry constant is a hard problem which still hasn't been resolved. A big step forward has been made by Terence Tao and Emmanuel Candes in \cite{CandesTao}, where they have shown that random Gauss matrices satisfy the restricted isometry property with a high probability and introduced stohastics into the theory of compressive sensing.

\end{summary}

% te zivotopis

\begin{cv}
    Ro\dj en sam 25.08.1993 u Puli. Poha\dj ao sam osnovnu \v{s}kolu Ka\v{s}tanjer, a zatim prva dva razreda Gimnazije Pula, prirodoslovno-matemati\v{c}ki smjer. Kao HMC stipendist, zadnja dva razreda srednje \v{s}kole zavr\v{s}avam u Cheltenham College-u u Velikoj Britaniji gdje stje\'cem A-levels kvalifikacije. Godine 2012. upisujem prediplomski sveu\v{c}ili\v{s}ni studij Matematike na Prirodoslovno-matemati\v{c}kom fakultetu Sveu\v{c}ili\v{s}ta u Zagrebu. Na istom fakultetu, 2016. godine upisujem diplomski studij Primjenjene matematike. Od 2016.-2019. godine radim kao student in\v{z}injer za ugra\dj ene sustave u firmi Ericsson Nikola Tesla, a od 2019. godine zapo\v{s}ljavam se kao razvojni in\v{z}injer u firmi AG04. Slobodno vrijeme provodim bave\'ci se biciklizmom, slobodnim penjanjem, planinarenjem te tr\v{c}enjem planinskih ultra maratona.
\end{cv}

\end{document}
