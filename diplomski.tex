% Predlozak za pisanje diplomskog rada na PMF-MO
% Opcenita uputstva za LaTeX se mogu npr. naci na 
% http://web.math.hr/nastava/rp3, http://web.math.hr/nastava/s4-prof/latex.pdf
% NE PREPORUCA se "Ne baÅ¡ tako kratak uvod u TEX", buduci se radi o vrlo starom prirucniku
% koji nije pogodan za moderne verzije LaTEXa.
% Originalna verzija "The not so short..." na http://tobi.oetiker.ch/lshort/lshort.pdf 
% je obnovljena i daje bolji uvid u moderne verzije LaTeXa

% Stil je optimiziran za kreiranje pdf dokumenta (npr. pomocu pdflatex-a, XeLaTeX-a)

\documentclass[a4paper,twoside,12pt]{memoir} % jednostrano: promijeniti twoside u oneside

% Paket inputenc omogucava direktno unosenje hrvatskih dijakritickih znakova 
% opcija utf8 za unicode (unix, linux, mac)
% opcija cp1250 za windowse
\usepackage[utf8]{inputenc}  % ukoliko se koristi XeLaTeX onda je \usepackage{xunicode}\usepackage{xltxtra}
\usepackage{mathrsfs} 
% Stil za diplomski, unutra je ukljucena podrska za hrvatski jezik
\usepackage{diplomski}
% bibliografija na hrvatskom
\usepackage[languagenames,fixlanguage,croatian]{babelbib} % zahtijeva datoteku croatian.bdf
% hiperlinkovi 
\usepackage[pdftex]{hyperref} 
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}
\usepackage{enumitem}
\usepackage{multicol}

%\newcommand{\vect}[1]{\boldsymbol{\mathrm{#1}}}
\newcommand{\vect}[1]{\mathbf{#1}}
\renewcommand{\vec}{\vect}
\newcommand{\card}{\text{\normalfont{card}}}
\newcommand{\supp}{\text{\normalfont{supp}}}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\norms}[1]{\left\lVert#1\right\rVert}
\newcommand{\rank}{\text\normalfont{rank}}
%\newcommand{\argmin}{\text{\normalfont{arg}}\min}
%\newcommand{\argmax}{\text{\normalfont{arg}}\max}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\sgn}{\text{\normalfont{sgn}}}
\renewcommand{\Re}{\text{\normalfont{Re}}}
\renewcommand{\Im}{\text{\normalfont{Im}}}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\cone}{cone}
\DeclareMathOperator{\tr}{tr}

\newenvironment{alg}[1]
{
    \bigskip
    \begin{tcolorbox}[arc=0mm,boxrule=1.2pt,colframe=black,colback=white,detach title, before upper={\medskip\begin{center}\textbf{#1}\end{center}\hline\newline\medskip},frame hidden]
    \medskip
}
{
    \medskip
\end{tcolorbox}
    \bigskip
}


% Odabir familije fontova:
% koristenjem XeLaTeX-a mogu se koristiti svi fontovi instalirani na racunalu, npr
% \defaultfontfeatures{Mapping=tex-text}
% \setmainfont[Ligatures={Common}]{Hoefler Text}
% ili
% \newcommand{\nas}[1]{\fontspec{Adobe Garamond Pro}\fontsize{24pt}{24pt}\color{Chocolate}\selectfont #1}
% i onda \nas{Naslov ...}
%\usepackage{lmodern} % times new roman 
\usepackage[T1]{fontenc}
%\usepackage{newtxtext,newtxmath}
\usepackage{lmodern}
\usepackage{mathtools}

\usepackage{lineno}
%\linenumbers

% Paket graphicx sluzi za manipuliranje grafikom 
\usepackage[pdftex]{graphicx} % ukoliko se koristi XeLaTeX onda je \usepackage[xetex]{graphicx}
\usepackage[most]{tcolorbox}
% Paket amsmath je vec ukljucen
% Dodatno definirane matematicke okoline:
% teorem (okolina: thm), lema (okolina: lem), korolar (okolina: cor),
% propozicija (okolina: prop), definicija (okolina: defn), napomena (okolina: rem),
% slutnja (okolina: conj), primjer (okolina: exa), dokaz (okolina: proof)
% Definirane su naredbe za ispisivanje skupova N, Z, Q, R i C
% Definirane su naredbe za funkcije koje se u hrvatskoj notaciji oznacavaju drukcije 
% nego u americkoj: tg, ctg, ... (\tgh za tangens hiperbolni)
% Takodjer su definirane naredbe za Ker i Im (da bi se razlikovala od naredbe za imaginarni dio kompleksnog
% broja, naredba se zove \slika).

\pagestyle{headings}
% uz paket fancyhdr mogu se lako kreirati fancy zaglavlja i podnozja

% Podaci koje treba unijeti
\title{Sa\v{z}eto uzorkovanje}
\author{Marco Hrli\'c}
\advisor{Prof. dr. sc. Damir Baki\'c}  % obavezno s titulom (prof. dr. sc ili doc. dr. sc.)
\date{2019.}  % oblika mjesec, godina

% Moguce je unijeti i posvetu
% Ukoliko nema posvete, dovoljno je iskomentirati/izbrisati sljedeci redak 
\dedication{Albini}

\begin{document}
\setlength\abovedisplayskip{10pt}
\setlength\belowdisplayskip{10pt}
\setlength\abovedisplayshortskip{10pt}
\setlength\belowdisplayshortskip{10pt}

\nocite{*}

% Naredna frontmatter generira naslovnu stranicu, stranicu za potpise povjerenstva, eventualnu posvetu i sadrzaj
% Moze se iskomentirati ukoliko nije u pitanju konacna verzija
\frontmatter

% Tekst diplomskog ...

% Diplomski rad treba poceti s uvodnim poglavljem  
\begin{intro}
...
\end{intro}

\chapter[Rijetka rje\v{s}enja][Rijetka rje\v{s}enja]{Rijetka rje\v{s}enja}\label{chapter_rijetka_rijesenja}	
% ukoliko naslov nije jako dugacak dovoljno je samo \chapter{Naslov poglavlja} 

\section[Rijetsko i sa\v{z}etost vektora][Rijetsko i sa\v{z}etost vektora]{Rijetsko i sa\v{z}etost vektora}
%\subsection{Naslov podsekcije}
Uvedimo potrebnu notaciju. Neka je $[N]$ oznaka za skup $\{1,2,...,N\}$ gdje je $N\in\N$. Sa $\card(S)$ ozna\v{c}ujemo kardinalitet skupa $S$. Nadalje, $\bar{S}$ je komplement od $S$ u $[N]$, tj. $\bar{S}=[N]\backslash S$.

\begin{defn}
    Nosa\v{c} vektora $\vec{x} \in \C^{N}$ je skup indeksa njegovih ne-nul elemenata, tj.
    $$\supp(\vec{x}):=\{j\in[N]:x_j \neq 0 \}$$
\end{defn}

\noindent Za vektor $\vec{x}\in\C^{N}$ ka\v{z}emo da je $s$-rijedak ako vrijedi $$\|\vec{x}\|_0 := \card(\supp(\vec{x})) \leq s$$
Primjetimo,
$$\|\vec{x}\|_p^p := \sum_{j=1}^N|x_j|^p \xrightarrow{p\rightarrow 0} \sum_{j=1}^N\mathbf{1}_{\{x_j \neq 0\}} = \card(\{j \in [N]:x_j \neq 0\}) = \|\vec{x}\|_0$$
Gdje smo koristili da je $\mathbf{1}_{\{x_j \neq 0\}} = 1$  ako je $x_j \neq 0$ te $\mathbf{1}_{\{x_j \neq 0\}} = 0$  ako je $x_j = 0$. Drugim rije\v{c}ima, $\|\vec{x}\|_0$ je limes $p$-te potencije $\ell_p$-kvazinorme vektora $\vec{x}$ kada $p$ te\v{z}i k nuli. Kvazinorma definira se jednako kao standardna $\ell_p$-norma, jedino \v{s}to nejednakost trokuta oslabimo, tj. 
$$\|\vec{x}+\vec{y}\|\leq C(\|\vec{x}\|+\|\vec{y}\|)$$ 
za neku konstantu $C \geq 1$.
Funkciju $\|\cdot\|_0$ \v{c}esto nazivamo $\ell_0$-norma vektora $x$, iako  ona nije niti norma niti kvazinorma. U samoj praksi, te\v{s}ko je tra\v{z}iti rijetkost vektora, pa je stoga prirodno zahtjevati slabiji uvjet \textit{kompresibilnosti}.  
\begin{defn}\label{greska_naj_s_aprox}
    $\ell_p$-gre\v{s}ku najbolje $s$-rijetke aproksimacije vektora $\vec{x}\in\C^{N}$ definiramo sa 
    $$\sigma_s(\vec{x})_p := \inf\big\{\|\vec{x}-\vec{z}\|_p,\ \vec{z} \in \C^{N} \ \text{je s-rijedak}\big\}$$
\end{defn}
\indent Primjetimo da se infimum posti\v{z}e za svaki $s$-rijedak vektor $\vec{z} \in \C^{N}$ koji ima ne-nul elemente koji su jednaki sa $s$ najve\'cih komponenti vektora $\vec{x}$. Iako takav $\vec{z} \in \C^{N}$ nije jedinstven, on posti\v{z}e infimum za svaki $p > 0$. Neformalno, mogli bi re\'ci da je vektor $\vec{x} \in \C^{N}$ \textit{kompresibilan} ako gre\v{s}ka njegove najbolje $s$-rijetke aproksimacije brzo konvergira u $s$. Da bi to formalno iskazali, od koristi \'ce biti ocjena na $\sigma_s(\cdot)_p$. Po\v{s}to nam za to ne\'ce biti va\v{z}an poredak elemenata vektora $\vec{x}$, uvodimo sljede\'cu definiciju koja \'ce nam olaksati ra\v{c}un.

\begin{defn}
    Nerastu\'ci poredak vektora $\vec{x} \in \C^{N}$ je vektor $\vec{x}^* \in \R^{\N}$ takav da
    $$x^*_1 \geq x^*_2 \geq x^*_3 \geq \dots \geq 0$$
    te postoji permutacije $\pi : [N]\rightarrow[N]$ takva da $x^*_j=|x_{\pi(j)}|$ za sve $j\in [N]$.
\end{defn}
\begin{prop}\label{osnovna_ocjena_lp_greske}
    Za svaki $q > p > 0$ i za svaki $\vec{x}\in \C^{N}$ vrijedi
    $$\sigma_s(\vec{x})_q \leq \frac{1}{s^{1/p - 1/q}}\|\vec{x}\|_p.$$
\end{prop}
\begin{proof}
    Neka je $\vec{x}^* \in \R^N$ nerastu\'ci poredak vektora $\vec{x}\in\C^N$. Tada slijedi,
    \begin{equation*}
    \begin{split} 
        \sigma_s(\vec{x})_q^q &= \sum_{j=s+1}^{N}(x_j^*)^q=\sum_{j=s+1}^{N}(x_j^*)^p(x_j^*)^{q-p} \leq (x_s^*)^{q-p} \sum_{j=s+1}^{N}(x_j^*)^p \\ & \leq \bigg(\frac{1}{s}\sum_{j=1}^{s}(x_j^*)^p\bigg)^{\frac{q-p}{p}}\bigg( \sum_{j=s+1}^N(x_j^*)^p\bigg) \leq \bigg( \frac{1}{s} \|\vec{x}\|_p^p \bigg)^{\frac{q-p}{p}}\|\vec{x}\|_p^p \\ & = \frac{1}{s^{q/p-1}}\|\vec{x}\|_p^q
    \end{split}
    \end{equation*}
    Prva nejednakost slijedi iz \v{c}injenice da je $x_j^* \leq x_s^*$ za svaki $j \geq s+1$. Druga nejednakost je tako\dj er posljedica nerasta komponenti od $\vec{x}^*$. Potenciranjem obje strane s $1/q$ slijedi tvrdnja.
\end{proof}
Primjetimo da ako je $\vec{x}$ iz jedini\v{c}ne $\ell_p$-kugle za neki mali $p>0$, onda prethodna propozicija garantira kovergenciju od $\sigma_s(\vec{x})_q$ u $s$, gdje $\ell_p$-kuglu definiramo kao
$$B_p^N := \big\{ \vec{z} \in \C^N : \|\vec{z}\|_p \leq 1\big\}$$
Vratimo se sada ocjeni iz propozicije \ref{osnovna_ocjena_lp_greske}. Sljede\'ci teorem daje najmanju konstantu $c_{p,q}$ takvu da vrijedi $\sigma_s(\vec{x})_q\leq c_{p,q}s^{-1/p+1/q}\|\vec{x}\|_p$ te zapravo predstavlja ja\v{c}u tvrdnju.
\begin{thm}\label{tm:2:5}
    Za svaki $q > p > 0$ i za svaki $\vec{x}\in\C^N$ vrijedi
    \begin{equation*}
    \sigma_s(\vec{x})_q \leq \frac{c_{p,q}}{s^{1/p - 1/q}}\|\vec{x}\|_p
    \end{equation*}
    gdje je
    $$
    c_{p,q} := \bigg[ \bigg(\frac{p}{q}\bigg)^{p/q}\bigg( 1-\frac{p}{q}^{1-p/q}\bigg) \bigg]^{1/p}\leq1.
    $$
\end{thm}
Istaknimo za \v{c}esti odabir $p=1$ i $q=2$
\begin{equation*}
    \sigma_s(\vec{x})_2 \leq \frac{1}{2\sqrt{s}}\|\vec{x}\|_1
\end{equation*}
\begin{proof}
    Neka je $\vec{x}^*$ nerastu\'ci poredak vektora $\vec{x}\in\C^N$ i $\alpha_j := (x_j^*)^p$. Dokazati \'cemo ekvivaltenu tvrdnju
    \begin{equation}\label{ocjena_ekv_tvrdnja}
    \begin{rcases}
{\alpha_1 \geq \alpha_2 \geq \cdots \geq \alpha_N \geq 0} \\
{\alpha_1 + \alpha_2 + \cdots + \alpha_N \leq 1} 
\end{rcases}\implies \alpha_{s+1}^{q/p} + \alpha_{s+2}^{q/p} + \cdots + \alpha_{s+N}^{q/p} \leq \frac{c^q_{q}}{s^{q/p-1}}
    \end{equation}
    Stoga, za $r:=q/p>1$, problem se svodi na maksimizaciju konveksne funkcije
    $$
    f(\alpha_1, \alpha_2, \dots, \alpha_N) := \alpha_{s+1}^r + \alpha_{s+2}^r + \cdots +\alpha_{N}^r
    $$
    na konveksnom mnogokutu
    $$
    \mathcal{C} := \big\{ (\alpha_1, \cdots, \alpha_N)\in \R^N :  \alpha_1 \geq \alpha_2 \geq \cdots \geq \alpha_N \geq 0 i  \alpha_1 + \alpha_2 + \cdots + \alpha_N \leq 1\big\}
    $$
    Prema teoremu (todo) $f$ posti\v{z}e maksimum na nekom od vrhova mnogokuta $\mathcal{C}$, a vrhovi od $\mathcal{C}$ su dani kao sjeci\v{s}ta  $N$ hiperplohi koje dobijemo tako da u \eqref{ocjena_ekv_tvrdnja} $N$ nejednakosti pretvorimo u jednakosti. Mogu\v{c}nosti su:
    \begin{enumerate}
        \item $\alpha_1=\cdots=\alpha_N \ \implies\  f(\alpha_1, \alpha_2, \dots, \alpha_N) = 0$.
        \item $\alpha_1+\cdots+\alpha_N=1$ i $\alpha_1=\cdots=\alpha_k>\alpha_{k+1}=\cdots=\alpha_N=0$ za neki \\ $1\leq k \leq s \  \implies \  f(\alpha_1, \alpha_2, \dots, \alpha_N) = 0$
        \item $\alpha_1+\cdots+\alpha_N=1$ i $\alpha_1=\cdots=\alpha_k>\alpha_{k+1}=\cdots=\alpha_N=0$ za neki\\ $s+1\leq k \leq N \  \implies \  \alpha_1=\cdots\alpha_k=1/k$ te $f(\alpha_1, \alpha_2, \dots, \alpha_N) = (k-s)/k^r$
    \end{enumerate}
    Dakle, slijedi da 
    $$
    \max\limits_{(\alpha_1,\dots,\alpha_N)\in\mathcal{C}} f(\alpha_1, \alpha_2, \dots, \alpha_N) = \max\limits_{s+1\leq k \leq N} \frac{k-s}{k^r}
    $$
    Shvatimo sada $k$ kao realnu varijablu i zamjetimo da $g(k):=(k-s)/k^r$ raste do kriti\v{c}ne to\v{c}ke $k^*=(r/(r-1))s$ nakon koje opada.
    $$
    \max\limits_{(\alpha_1,\dots,\alpha_N)\in\mathcal{C}} f(\alpha_1, \alpha_2, \dots, \alpha_N) \leq g(k^*) = \frac{1}{r}\bigg( 1- \frac{1}{r}\bigg)^{r-1}\frac{1}{s^r-1}=c^q_{p,q}\frac{1}{s^{q/p}-1}
    $$
\end{proof}
\indent Alternativni na\v{c}in na koji bi mogli definirati pojam \textit{kompresibilnosti} za vektor $\vec{x}\in\C^N$ je da zahtjevamo da je broj
$$\card(\{j\in[N]:|x_j|\geq t\})$$
tj. broj njegovih zna\v{c}ajnih ne-nul komponenti dovoljno mali. Ovaj pristup vodi na definiciju slabih $\ell_p$-prostora.
\begin{defn}
Za $p>0$, slabi $\ell_p$-prostor s oznakom $w\ell_p^N$ definiramo kao prostor $\C^N$ sa kvazinormom
\begin{equation}\label{slaba_kvazinorma}
    \|\vec{x}\|_{p, \infty}:=\inf\bigg\{ M \geq 0: \card (\{j\in [N]: |x_j|\geq t \})\leq \frac{M^P}{t^p},\ \forall t>0    \bigg\}
\end{equation}
\end{defn}
\noindent
Da bi pokazali da je \eqref{slaba_kvazinorma} zapravo kvazinorma, potreban nam je sljede\'ci rezultat.
\begin{prop}
    Neka su $\vec{x}^1,\dots\vec{x}^k\in\C^N$. Tada za svaki $p>0$ vrijedi 
    \begin{equation*}
    \|\vec{x}^1+\dots+\vec{x}^k\|_{p,\infty} \leq k^{\max\{1, 1/p\}}(\|\vec{x}^1\|_{p, \infty} + \cdots + \|\vec{x}^k\|_{p, \infty})
    \end{equation*}
\end{prop}
\begin{proof}
    Neka je $t>0$. Ako je $|x_j^1+\cdots+x_j^k|\geq t$ za neki $j\in [N]$, tada imamo da je $|x_j^i|\geq t/k$ za neki $i \in [k]$. Dakle, vrijedi
    \begin{equation*}
        \big\{ j\in [N]:|x_j^1+\cdots+x_j^k| \geq t \big\} \subset \bigcup\limits_{i\in [k]} \big \{ j \in [N] : |x_j^i| \geq t/k \big \}
    \end{equation*}
    pa je stoga
    \begin{align*}
        \card\big( \big\{ j\in [N] : |x_j^1+\cdots+k_j^k| \geq t \big\}\big)&\leq\sum\limits_{i\in [k]}\frac{\|\vec{x}^i\|^p_{p, \infty}}{(t/k)^p} \\ 
                                                                            &= \frac{k^p(\|\vec{x}^1\|^p_{p, \infty}+\cdots + \|\vec{x}^k\|^p_{p, \infty})}{t^p}
    \end{align*}
    Prema definiciji slabe $\ell_p$-kvazinorme \eqref{slaba_kvazinorma} vektora $\vec{x}^1+\cdots+\vec{x}^k$ dobivamo
    \begin{equation*}
        \|\vec{x}^1+\cdots+\vec{x}^k\|_{p, \infty}\leq k\big(\|\vec{x}^1\|^p_{p,\infty}+ \cdots +\|\vec{x}^k\|^p_{p,\infty}\big) 
    \end{equation*}
    Ako je $p \leq 1$, uspore\dj uju\'ci $\ell_p$ i $\ell_1$ norme na $\R^k$ slijedi
    \begin{equation*}
        \big(\|\vec{x}^1\|^p_{p,\infty}+ \cdots +\|\vec{x}^k\|^p_{p,\infty}\big)^{1/p} \leq k^{1/p-1}\big(\|\vec{x}^1\|_{p,\infty}+ \cdots +\|\vec{x}^k\|_{p,\infty}\big)
    \end{equation*}
    te ako je $p \geq 1$ slijedi
    \begin{equation*}
        \big(\|\vec{x}^1\|^p_{p,\infty}+ \cdots +\|\vec{x}^k\|^p_{p,\infty}\big)^{1/p} \leq \|\vec{x}^1\|_{p,\infty}+ \cdots +\|\vec{x}^k\|_{p,\infty}.
    \end{equation*}
    Tvrdnja slijedi kombiniranjem dobivenih ocjena.
\end{proof}

\noindent
Uzmimo $\vec{x}, \vec{y} \in \C^N$ i neka je $\lambda \in \C$ proizvoljan.

\begin{enumerate}
    \item Neka je $\|\vec{x}\|_{p, \infty}=0$. Iz \eqref{slaba_kvazinorma} slijedi $ \card(\{j \in [N]: |x_j| \geq t\}) = 0$ za svaki $t > 0$ pa je stoga broj ne-nul komponenti on $\vec{x}$ jednak nuli, tj. $\vec{x}=0$ 
    \item Ako je $\lambda$ nula, $\|\lambda \vec{x}\| = | \lambda | \| \vec{x} \|$ vrijedi trivijalno. Za $\lambda \neq 0$, imamo \\
        $\card(\{ j \in [N]: |\alpha x_j| \geq t \}) = \card(\{ j \in [N]: |x_j| \geq t/|\alpha| \})\leq (\alpha M)^p/t^p$ za svaki $t>0$. Dakle, opet $\|\lambda \vec{x}\| = | \lambda | \| \vec{x} \|$.
    \item $\|\vec{x}+\vec{y}\|\leq C(\|\vec{x}\|+\|\vec{y}\|)$ je sada direktna posljedica prethodne propozicije.
\end{enumerate}

\noindent sljede\'ca propozicija daje alternativni izraz za slabu $\ell_p$-kvazinormu.
\begin{prop}\label{slaba_kvazinorma_2}
    Za $p>0$, vrijedi
    \begin{equation*}
        \|\vec{x}\|_{p, \infty} = \max \limits_{k \in [N]}k^{1/p}x_k^{*}
    \end{equation*}
    gdje je $\vec{x}^* \in \R^N$ nerastu\'ci poredak vektora $\vec{x}\in \C^N$.
\end{prop}
\begin{proof}
    Primjetimo prvo da iz \eqref{slaba_kvazinorma} slijedi da je $\|\vec{x}\|_{p, \infty}=\|\vec{x}^*\|_{p, \infty}$, pa zapravo pokazujemo da je $\|\vec{x}\|:= \max_{k \in [N]}k^{1/p}x_k^* = \|\vec{x}^*\|$. Nadalje, za $t>0$ vrijedi da je $\{j \in [N]: x^*_j \geq t\}=[k]$ za neki $k \in [N]$ ili je $\{j \in [N]: x^*_j \geq t\}=\emptyset$. U prvom slu\v{c}aju $t \leq x^*_k \leq \|\vec{x}\|/k^{1/p}$ pa je $\card(\{j \in [N]:x_j^* \geq t \}) = k \leq \|\vec{x}\|/k^{1/p}$. U drugom slu\v{c}aju ista nejednakost vrijedi trivijalno. Iz definicije slabe $\ell_p$-kvazinorme \eqref{slaba_kvazinorma} sada dobivamo $\|\vec{x}^*\|_{p, \infty} \leq \|\vec{x}\|$. Pretpostavimo da je $\|\vec{x}^*\|_{p, \infty} < \|\vec{x}\|$. Tada postoji $\varepsilon > 0$ takav da $(1+ \varepsilon)\|\vec{x}^*\|_{p, \infty} \leq \|\vec{x}\|$. Slijedi da je $(1 + \varepsilon)\|\vec{x}^*\| \leq  k^{1/p}x^*_k$ za neki $k \in [N]$ pa stoga
    \begin{equation*}
        [k] \subseteq \big\{ j \in [N] : (1 + \varepsilon)\|\vec{x}^*\|_{p, \infty}/k^{1/p} \leq x_j^* \big\}
    \end{equation*}
    Ponovo iz \eqref{slaba_kvazinorma} imamo
    \begin{equation*}
        k \leq \frac{\|\vec{x}^*\|^p_{p, \infty}}{\big( (1 + \varepsilon)\|\vec{x}^*\|_{p, \infty}k^{1/p}\big)^p}=\frac{k}{(1 + \varepsilon)^p}
    \end{equation*}
    Kontradikcija, dakle mora vrijediti $\|\vec{x}\| = \|\vec{x}^*\|_{p, \infty}$.
\end{proof}
\noindent Sada lagano mo\v{z}emo usporediti slabi i jaku $\ell_p$ normu,
\begin{prop}
    Za svaki $p > 0$ i za svaki $\vec{x} \in \C^N$,
    \begin{equation*}
        \|\vec{x}\|_{p, \infty} \leq \|\vec{x}\|_p
    \end{equation*}
\end{prop}
\begin{proof}
    Neka je $k \in [N]$,
    \begin{equation*}
        \|\vec{x}\|_p^p = \sum_{j=1}^{N}(x_j^*)^p \geq \sum_{j=1}^{k}(x_j^*)^p \geq k(x_k^*)^p
    \end{equation*}
    Tvrdnja slijedi potenciranjem na $1/p$ i uzimaju\'ci maksimum po $k$ i primjenom prethodne propozicije.
\end{proof}
Koriste\'ci propoziciju \eqref{slaba_kvazinorma_2} mo\v{z}emo dobiti verziju ocjene iz propozicije \eqref{osnovna_ocjena_lp_greske} sa slabom $\ell_p$ normom.
\begin{prop}
    Za svaki $q>p>0$ i $\vec{x} \in \C^N$, vrijedi
    \begin{equation*}
        \sigma_s(\vec{x})_q \leq \frac{d_{p,q}}{s^{1/p-1/q}}\|\vec{x}\|_{p, \infty}
    \end{equation*}
    gdje je
    \begin{equation*}
        d_{p,q} := \big( \frac{p}{q-p} \big)^{1/q}.
    \end{equation*}
\end{prop}
\begin{proof}
    Bez smanjenja op\v{c}enitosti mo\v{z}emo pretpostaviti da je $\norm{\vec{x}}_{p,\infty} \leq 1$, pa je $x_k^* \leq 1/k^{1/p}$ za svaki $k \in [N]$. Tada vrijedi,
    \begin{equation*}
    \sigma_s(\vec{x})^q_q = \sum_{k=s+1}^{N} (x_k^*)^q \leq \sum_{k=s+1}^N \frac{1}{k^{q/p}} \leq \int_s^N \frac{1}{t^{q/p}} dt = - \frac{1}{q/p-1} \frac{1}{t^{q/p-1}}\bigg\rvert^{t=N}_{t=s} \leq \frac{p}{q-p} \frac{1}{s^{q/p-1}}.
    \end{equation*}
    Potenciranjem sa $1/q$ slijedi tvrdnja.
\end{proof}
Prethodna propozicija daje da su vektori $\vec{x} \in \C^N$ koji su kompresibilni u smislu $\norm{\vec{x}}_{p, \infty} \leq 1$ za mali $p>0$, tako\dj er kompresibilni u smislu da gre\v{s}ka njihove najbolje $s$-rijetke aproksimacije brzo konvergira sa $s$. Iska\v{z}imo jo\v{s} jedan tehni\v{c}ki rezultat,
\begin{lem}
    Neka su $\vec{x}, \vec{y} \in \C^N$. Tada vrijedi,
    \begin{equation} \label{nerastuci_poredak_ocjena_1}
        \norm{\vec{x}^* - \vec{y}^*}_{\infty} \leq \norm{\vec{x} - \vec{y}}_{\infty}
    \end{equation}
    Nadalje, za $s \in [N]$,
    \begin{equation}\label{nerastuci_poredak_ocjena_2}
        |\sigma_s(\vec{x})_1 - \sigma(\vec{y})_1| \leq \norm{\vec{x} - \vec{y}}_1
    \end{equation}
    i za $k>s$,
    \begin{equation}\label{nerastuci_poredak_ocjena_3}
        (k-s)x_k^* \leq \norm{\vec{x} - \vec{y}}_1 + \sigma_s(\vec{y})_1
    \end{equation}
\end{lem}
\begin{proof}
    Za $j \in [N]$, skup indeksa $j$ najve\'cih komponenti vektora $\vec{x}$ ima ne-trivijalni presjek sa skupom od $N-j+1$ najmanjih komponenti vektora $\vec{y}$. Izaberimo indeks $l$ iz tog presjeka. Tada vrijedi, 
    \begin{equation*}
        x_j^* \leq |x_l| \leq |y_l| + \norm{\vec{x} - \vec{y}}_{\infty} \leq z_j^* + \norm{\vec{x} - \vec{y}}_{\infty}
    \end{equation*}
    Zamjenom uloga od $\vec{x}$ i $\vec{y}$ slijedi \eqref{nerastuci_poredak_ocjena_1}.
    Neka je $\vec{v} \in \C^N$ najbolja $s$-rijetka aproksimacija vektora $\vec{y}$. Tada
    \begin{equation*}
        \sigma_s(\vec{x})_1 \leq \norm{\vec{x} - \vec{v}}_1 \leq \norm{\vec{x} - \vec{y}}_1 + \norm{\vec{y} - \vec{v}}_1 = \norm{\vec{x} - \vec{y}}_1 + \sigma_s(\vec{y})_1 
    \end{equation*}
    Ponovno, zbog simetrije slijedi \eqref{nerastuci_poredak_ocjena_2}. Napokon, ocjena \eqref{nerastuci_poredak_ocjena_3} slijedi iz \eqref{nerastuci_poredak_ocjena_2} te iz \v{c}injenice
    \begin{equation*}
        (k-s)x_k^* \leq \sum_{j=s+1}^{k}x_j^* \leq \sum_{j \geq s+1} x_j^* = \sigma_s(\vec{x})_1.
    \end{equation*}
\end{proof}

\section[Minimalni broj mjerenja][Minimalni broj mjerenja]{Minimalni broj mjerenja}
Problem sa\v{z}etog uzorkovanja sastoji se od rekonstrukcije $s$-rijetkog vektora $\vec{x} \in \C^N$ iz sustava
\[\vec{y} = \vec{A}\vec{x}\]
Matricu $\vec{A} \in \C^{m\times N}$ nazivamo \textit{matrica mjerenja}. Ako je $m < N$, za ovakav sustav linearnih jednad\v{z}bi ka\v{z}emo da je \textit{neodre\dj en}. Iako iz klasi\v{c}ne teorije linearne algebre ovakvi sustavi imaju beskona\v{c}no mnogo rije\v{s}enja, pokazati \'ce se da je dodatna pretpostavka rijetkosti vektora $x$ dovoljno za jedinstvenost rje\v{s}enja. U ovom poglavlju istra\v{z}iti \'cemo koji je minimalni broj mjerenja, tj. $m$ broj redaka matrice $\vec{A}$, koji garantira rekonstrukciju $s$-rijetkog vektora $\vec{x}$. Zapravo, postoje dva pristupa ovom problemu. Mo\v{z}emo zahtjevati da problem mjerenja rekonstruira sve $s$-rijetke vektore $\vec{x} \in \C^N$ istodobno ili mo\v{z}emo tra\v{z}iti rekonstrukciju specifi\v{c}nog, tj. predodre\dj enog vektora $\vec{x} \in \C^N$. Taj pristup \v{c}ini se neprirodan, no pokazuje se da je on va\v{z}an u prou\v{c}avanju problema gdje matricu $\vec{A}$ biramo nasumi\v{c}no. \\ 
\indent Poka\v{z}imo da su za danu rijetkost $s$, matricu $\vec{A} \in \C^{m \times N}$ i $s$-rijedak vektor $\vec{x} \in \C^N$, naredne tvrdnje ekvivaltentne: 
\begin{enumerate}
    \item Vektor $\vec{x}$ je jedinstveno $s$-rijetko rje\v{s}enje sustava $\vec{A}\vec{z}=\vec{y}$ gdje je $\vec{y} = \vec{Ax}$, tj. $\{\vec{z} \in \C^N : \vec{A}\vec{z}= \vec{A}\vec{x},\ \norm{\vec{z}}_0 \leq s\} = \{\vec{x}\}$
    \item Vektor $\vec{x}$ je jedinstveno rje\v{s}enje problema minimizacije
        \begin{equation}\label{problem_minimizacije}
            \min\limits_{\vec{z} \in \C^N} \norm{\vec{z}}_0\quad \text{uz uvjet}\ \vec{Az} = \vec y \tag{$P_{0}$}
        \end{equation}
\end{enumerate}
Ako je $\vec{x} \in \C^N$ jedinstveno $s$-rijetko rje\v{s}enje od $\vec{Az} = \vec y$ takvo da je $\vec y = \vec{Ax}$, onda rje\v{s}enje $x^{\sharp}$ od \eqref{problem_minimizacije} je $s$-rijetko i zadovoljava $\vec{Ax} = \vec y$ pa je $\vec x^\sharp = \vec x$. Drugi smjer slijedi trivijalno.


\subsection[Rekonstrukcija svih rijetkih vektora][Rekonstrukcija svih rijetkih vektora]{Rekonstrukcija svih rijetkih vektora}
Neka je $\vec{A} \in \C^{m \times N}$ i $S \subset [N]$, sa $\vec A_S$ ozna\v{c}ujemo matricu formiranu od stupaca od $\vec A$ indeksiranih sa $S$. Sli\v{c}no, sa $\vec x_S$ ozna\v{c}ujemo ili vektor iz $\C^{S}$ koji se sastoji od komponenti vektora $\vec x$ indeksiranih po $S$, tj. $(\vec x_S)_l = x_l$ za sve $l \in S$, ili vektor iz $\C^N$ koji se podudara s $\vec x$ na komponentama indeksiranim u $S$ i jednak je nula na indeksima koji nisu u $S$, tj. $(\vec x_S)_l = x_l$ za $l \in S$ i $(\vec x_S)_l =0$ za $ l \notin S$. Iz konteksta \'ce uvijek biti jasno na koju definiciju se misli.

\begin{thm} \label{rekonstrukcija_tm1}
    Neka je $\vec A \in \C^{m \times N}$. Ekvivalentno je:
    \begin{enumerate}[label=(\alph*)]
        \item Svaki $s$-rijedak vektor $\vec x \in \C^N$ je jedinstveno rje\v{s}enje od $\vec{Ax}=\vec{Az}$, tj. ako je $\vec{Ax}=\vec{Az}$ i ako su $\vec x$, $\vec z$ oboje $s$-rijetki tada $\vec x = \vec z$.
        \item Jezgra od $\vec A$ ne sadr\v{z}i niti jedan $2s$-rijedak vektor osim nul-vektora, tj. $\ker \vec A \cap \{\vec z \in \C^N: \norm{\vec z}_0 \leq 2s\} = \{\vec 0\}$
        \item Za svaki $S \subset [N]$ takav da $\card(S) \leq 2s$, podmatrica $\vec A_S$ je injektivna kao preslikavanje sa $\C^S$ u $\C^m$.
        \item Svaki skup od $2s$ stupaca matrice $\vec A$ je linearno nezavisan skup.
    \end{enumerate}
\end{thm}
\begin{proof}
    \begin{itemize}
        \item[]$(b)\implies(a)$. Neka su $\vec x$ i $\vec z$ $s$-rijetki vektori takvi da $\vec{Ax} = \vec{Az}$. Tada je $\vec x - \vec z$ $2s$-rijedak i $\vec A(\vec x - \vec z) = \vec 0$. Po\v{s}to $\ker \vec A$ ne sadr\v{z}i $2s$-rijetke vektore osim nul-vektora, mora vrijediti $\vec x = \vec z$.
        \item[] $(a)\implies(b)$. Obratno, pretpostavimo da za svaki $s$-rijetki vektor $\vec x \in \C^N$ vrijedi $\{\vec z \in \C^N : \vec{Az} = \vec{Ax}, \norm{\vec z}_0 \leq s\} = \{\vec x\}$. Neka je $\vec v \in \ker \vec A$, $2s$-rijedak. Tada $\vec v$ mo\v{z}emo rastaviti kao $\vec v = \vec x - \vec z$ gdje su $\vec x$ i $\vec z$ $s$-rijetki takvi da $\supp(\vec x)\cap \supp(\vec z) = \emptyset$. Imamo da je $\vec{Ax}=\vec{Az}$ pa prema pretpostavci vrijedi $\vec{x}=\vec{z}$. Po\v{s}to su nosa\v{c}i od $\vec x$ i $\vec z$ disjunktni, mora vrijediti $\vec x = \vec z = \vec 0$ pa je stoga i $\vec v = 0$.
        \item[] $(b)\implies(c)$. Pretpostavimo suprotno, $\ker \vec A \cap \{\vec z \in \C^N: \norm{\vec z}_0 \leq 2s\} = \{\vec 0\}$ i da postoji $S \in [N]$ takav da je $\card(S) \leq 2s$ te da $\vec A_s$ nije injektivna. To zna\v{c}i da postoji vektor $\vec x \in \C^{\card(S)} \backslash \{\vec 0\}$ takav da je $\vec A_S \vec x = \vec 0$. Definiramo vektor $\tilde{\vec{x}}\in \C^N$ sa 
            \begin{equation*}
                \tilde{x}_j = 
                \begin{cases}
                    x_j \quad & \text{za}\ j \in S \\
                    0 \quad & \text{za}\  j \in \bar S \\
                \end{cases}
            \end{equation*}
            Dakle, imamo $\vec x \neq \vec 0$, $\norm{\vec x}_0 \leq 2s$ i vrijedi $\vec{Ax}=0$, tj. $\vec x \in \ker \vec A$. Kontradikcija s $(b)$.
        \item[]$(c)\implies(d)$. Odaberimo $2s$  stupaca od $\vec A$. Skup indeksa tih stupaca ozna\v{c}imo sa $S$. Prema $(c)$, matrica $\vec A_S$ je injektivna, a to zna\v{c}i da su njeni stupci linearno nezavisni, pa su stoga i $2s$ odabranih stupaca matrice $\vec A$ linearno nezavisni.
        \item[]$(d)\implies(b)$. Pretpostavimo da jezgra od $\vec A$ sadr\v{z}i $2s$-rijedak ne-nul vektor $\vec x \in \C^N$. Neka je $S$ skup indeksa ne-nul elemenata vektora $\vec x$. To zna\v{c}i da je $\vec A_S \vec x_S = 0$, i $\vec x_S \neq \vec 0$. Dakle $\vec A_S$ nije injektivna, pa stoga i skup stupaca od $\vec A$ indeksiranih sa $S$ nije linearno nezavisan, \v{s}to je kontradikcija sa $(d)$.
\end{itemize}
\end{proof}

Uo\v{c}imo da ako je mogu\v{c}e rekonstruirati svaki $s$-rijedak vektor $\vec x \in \C^N$ iz vektora mjerenja $\vec y = \vec{Ax} \in \C^m$, tada vrijedi $(a)$. Prema pro\v{s}lom teoremu tada vrijedi i tvrdnja $(d)$ pa je stoga $\rank(\vec A) \geq 2s$. Tako\dj er vrijedi da je $\rank(\vec A) \leq m$ pa imamo 
\begin{equation*}
    m \geq 2s.    
\end{equation*}
To zna\v{c}i da je potrebno barem $2s$ mjerenja da bi rekonstruirali svaki $s$-rijedak vektor. Pokazati \'cemo da je, makar u teoriji, dovoljno to\v{c}no $2s$ mjerenja.

\begin{thm}
    Za svaki $N \geq 2s$, postoji matrica mjerenja $\vec A \in \C^{2s \times N}$ takva da se svaki $s$-rijedak vektor $\vec x \in \C^N$ mo\v{z}e rekonstruirati iz vektora mjerenja $\vec y = \vec{Ax} \in \C^m$ kao rje\v{s}enje problema minimizacije \eqref{problem_minimizacije}.
\end{thm}
\begin{proof}
    Fiksirajmo $t_N>\cdots t_2 > t_1 > 0$ i neka je $\vec A \in \C^{2s \times N}$ dana sa
    \begin{equation}\label{vandermont_matrica}
        \vec A = 
        \begin{bmatrix}
            1 & 1 & \cdots & 1 \\ 
            t_1 & t_2 & \cdots & t_N \\
            \vdots & \vdots & \cdots & \vdots \\
            t_1^{2s-1} & t_2^{2s-1} & \cdots & t_N^{2s-1} \\
        \end{bmatrix}
    \end{equation}
    Nadalje, neka je $S=\{j_1 < \cdots < j_{2s}\}$ skup indeksa. Matrica $\vec A_S \in \C^{2s \times 2s}$ je transponirana \textit{Vandermontova matrica}. Prema (TODO) slijedi
    \begin{equation*}
        \det(\vec{A}_S) = \prod_{k < l} (t_{j_l} - t_{j_k})>0.
    \end{equation*}
    To zna\v{c}i da je matrica $\vec A$ invertibilna, pa posebno i injektivna. Tada je zadovoljena tvrdnja $(c)$ teorema \eqref{rekonstrukcija_tm1}, pa je po istom teoremu zadovoljena i tvrdnja $(a)$, tj. svaki $s$-rijedak vektor $\vec x \in \C^N$ zadovoljava $\vec{Az}=\vec{Ax}$. Stoga je taj vektor mogu\'ce jedinstveno rekonstruirati putem minimizacije \eqref{problem_minimizacije}.
\end{proof}
\indent Zapravo, mnogo matrica zadovoljava uvjet $(c)$ iz teorema \eqref{rekonstrukcija_tm1}. Na primjer, potencije od $t_1,\dots,t_N$ u \eqref{vandermont_matrica} ne moraju biti uzastopne. Nadalje, brojevi $t_1,\dots,t_N$ ne moraju biti pozitivni, niti realni sve dok vrijedi $\det(\vec A_S) \neq 0$. Posebno, mo\v{z}emo uzeti $t_l = e^{2\pi i (l-1)/N}$ za $l \in [N]$, teorem (TODO) garantira da parcijalna Fourierova matrica
\begin{equation*}
   \vec A = 
   \begin{bmatrix*}
       1 & 1 & 1 & \cdots & 1 \\
       1 & e^{2 \pi i/ N} & e^{2 \pi i2/ N} & \cdots & e^{2 \pi i(N-1)/ N} \\ 
       \vdots & \vdots & \vdots & \vdots & \vdots \\ 
       1 & e^{2 \pi i(2s-1)/ N} & e^{2 \pi i(2s-1)2/ N} & \cdots & e^{2 \pi i(2s-1)(N-1)/ N} \\ 
   \end{bmatrix*}
\end{equation*}
rekonstruira svaki $s$-rijedak vektor $\vec x \in \C^N$ iz $\vec y = \vec{Ax} \in \C^{2s}$.
Zapravo mo\v{z}e se pokazati da skup $(2s) \times N$ matrica takvih da $\det(\vec A_S) = 0$ za neki $S \subset [N]$ i $\card(S) \leq 2s$ ima Lebesgueovu mjeru nula, pa stoga gotovo sve $(2s) \times N$ matrice rekonstruiraju svaki $s$-rijedak vektor $\vec x \in \C^N$ iz $\vec y = \vec{Ax} \in \C^{2s}$. Me\dj utim u praksi nije isplativo rje\v{s}avati problem minimizacije \eqref{problem_minimizacije}, \v{s}to \'cemo kasnije i pokazati.




\subsection[Rekonstrukcija zadanog rijetkog vektora][Rekonstrukcija zadanog rijetkog vektora]{Rekonstrukcija zadanog rijetkog vektora}
Promatramo problem gdje je $s$-rijedak vektor $\vec x \in \C^N$ unaprijed zadan i poznat, a matricu $\vec A \in \C^{m \times N}$ \v{z}elimo odabrati tako da ona garantira rekonstrukciju vektora $\vec x$ iz mjerenja $\vec y = \vec{Ax} \in \C^m$. Isprva, ovaka pristup izgleda neprirodan zbog \v{c}injenice da je vektor $\vec x$ apriorno poznat. Ideja je da \'ce uvjeti rekonstrukcije vrijediti za gotovo sve $(s+1) \times N$ matrice, \v{s}to podupire \v{c}injenicu da se u praksi matrice mjerenja \v{c}esto odabiru na nasumi\v{c}an na\v{c}in.
\begin{thm}
Za svaki $N \geq s + 1$ i za dani $s$-rijedak vektor $\vec x \in \C^N$, postoji matrica mjerenja $\vec A \in \C^{(s+1) \times N}$, takva da se vektor $\vec x$ mo\v{z}e rekonstruirati iz mjerenja $\vec y = \vec{Ax} \in \C^m$ kao rje\v{s}enje minimizacije \eqref{problem_minimizacije}.
\end{thm}
\begin{proof}
    Neka je $\vec A \in \C^{(s+1) \times N}$ matrica za koju se $s$-rijedak vektor $\vec x$ ne mo\v{z}e rekonstruirati iz $\vec y = \vec{Ax}$ putem minimizacije \eqref{problem_minimizacije}. To zna\v{c}i da postoji vektor $\vec z \in \C^N$ razli\v{c}it od $\vec x$, takav da $S=\supp(\vec z)=\{j_1, \dots, j_s\}$, $\card(S) \leq s$ (ako je $\norm{\vec z}_0 < s$, u $S$ dodamo proizvoljne elemente $j_l \in [N]$) i $\vec{Az}=\vec{Ax}$. Ako je $\supp(\vec x) \subset S$, tada iz $\big( \vec A (\vec z - \vec x)  \big)_{[s]}=0$ slijedi da $\vec A_{[s], S}$ nije invertibilna, tj.
    \begin{equation*}
        f(a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N}) := \det(\vec A_{[s], S}) = 0.
    \end{equation*}
    Ako $\supp(\vec x) \not\subset S$ tada je dimenzija prostora $V:=\{ \vec u \in \C^N: \supp(\vec u ) \subset S \} + \C \vec x$ jednaka $s+1$, i linearno preslikavanje $G:V \rightarrow \C^{s+1}$, $\vec v \mapsto \vec{Av}$ nije invertibilno, po\v{s}to je $G(\vec z - \vec x)=0$. Matrica linearnog preslikavanja $G$ u bazi $(\vec e_{j_1}, \dots, \vec e_{j_s}, \vec x)$ prostora V, je oblika
    \begin{equation*}
        B_{\vec x, S}:=
        \begin{bmatrix*}
            a_{1, j_1} & \cdots & a_{1,j_s} & \sum_{j \in \supp(\vec x)}x_j a_{1,j} \\
            \vdots & \ddots & \vdots & \vdots \\
            a_{s+1, j_1} & \cdots & a_{s+1,j_s} & \sum_{j \in \supp(\vec x)}x_j a_{s+1,j}
        \end{bmatrix*}
    \end{equation*}
    i imamo
    \begin{equation*}
        g_S(a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N}) := \det (B_{\vec x, S})=0.
    \end{equation*}
    Dakle, vrijedi
    \begin{equation*}
        (a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N}) \in f^{-1}(\{0\}) \cup \bigcup \limits_{\card(S)=s}g_S^{-1}(\{0\}).
    \end{equation*}
    Primjetimo da su skupovi $f^{-1}(\{0\})$ i $g^{-1}_S(\{0\})$ Lebesgueove mjere nula iz razloga \v{s}to su $f$ i $g_S$ polinomi u varijablama $(a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N})$. Dakle, elemente matrice $\vec A$ moramo izabrati izvan skupa mjere nula, da bi osigurali rekonstrukciju vekotora $\vec x$ iz $\vec y = \vec{Ax}$.
\end{proof}

\section[NP-slo\v{z}enost $\mathbf{\ell_0}$-minimizacije][NP-slo\v{z}enost $\ell_0$-minimizacije]{NP-slo\v{z}enost $\ell_0$-minimizacije}
Kao \v{s}to smo najavili, pokazati \'cemo da je u praksi neisplativno rje\v{s}avati problem $\ell_0$-minimizacije u svrhu rekonstrukcije vektora $\vec x$ iz mjerenja $\vec y = \vec{Ax}$. Prisjetimo se, problem koji rje\v{s}avamo je oblika, 
\begin{equation*}
    \min_{\vec z \in \C^N} \norm{\vec z}_0 \quad \text{uz uvjet } \vec{Az}=\vec y. 
\end{equation*}
Po\v{s}to je minimizator najvise $s$-rijedak, najjednostavniji algoritam za rje\v{s}avanje ovog problema je rje\v{s}iti sve pravokutne sustave $\vec A_S \vec u = \vec y$ ili sve kvadratne sustave oblika $\vec A_S^* \vec A_S \vec u = \vec A_S^* \vec y$ za svaki $\vec u \in \C^S$ gdje S ide po svim poskupovima od [N], veli\v{c}ine $s$. No ispada da broj podskupova $N \choose s$, \v{s}to za male probleme sa $N = 1000$ i $s=10$, iznosi ${1000 \choose 10} \geq (\frac{1000}{10})^{10}=10^{20}$. Kada bi jedan $10 \times 10$ sustav mogli rje\v{s}iti u $10^{-10}$ sekundi, trebalo bi nam vi\v{s}e od 300 godina da sve rje\v{s}imo. Sada \'cemo pokazati za\v{s}to je zapravo op\'cenitiji problem
\begin{equation}
\min_{\vec z \in \C^N}\ \norm{\vec z}_0 \quad \text{uz uvjet }\norm{\vec{Az}- \vec{y}}_2 \leq \eta \tag{$P_{0, \eta}$}\label{problem_minimizacije_generalni}
\end{equation}
NP-te\v{z}ak.\\
\indent Uvedimo prvo potrebne pojmove iz kompleksnosti algoritama. Za algoritam ka\v{z}emo da je \textit{polinomijalnog-vremena} ako je broj koraka do rje\v{s}enja ograni\v{c}en polinomom u varijabli veli\v{c}ine ulaza. Nadalje, uvedimo neformalne definicije klasa problema odlu\v{c}ivanja:
\begin{itemize}
    \item $\mathfrak{P}$: Svi problemi odlu\v{c}ivanja za koje postoji algoritam polinomijalnog vremena koji daje rje\v{s}enje.
    \item $\mathfrak{NP}$: Svi problemi odlu\v{c}ivanja za koje postoji algoritam polinomijalnog vremena koji provjerava to\v{c}nost rje\v{s}enja.
    \item $\mathfrak{NP}$-te\v{s}ki: Svi problemi (ne nu\v{z}no problemi odre\dj ivanja) za koje se algoritam za rje\v{s}enje mo\v{z}e u polinomijalnom vremenu transformirati u algoritam rje\v{s}enja za bilo koji $\mathfrak{NP}$ problem.
    \item $\mathfrak{NP}$-potpuni: Svi problemi koji su istovremeno $\mathfrak{NP}$ i $\mathfrak{NP}$-te\v{s}ki.
\end{itemize}
Pitanje je li $\mathfrak{P}$ strogo sadr\v{z}ano u  $\mathfrak{NP}$ do dan danas nije odgovoreno. No, vjeruje se da postoje problemi za koje ne postoji algoritam rje\v{s}enja polinomijalnog vremena, ali postoji algoritam koji \'ce provjeriti to\v{c}nost rje\v{s}enja u polinomijalnom vremenu.
Najpoznatiji $\mathfrak{NP}$-potpun problem je problem putuju\'ceg prodava\v{c}a. No, iskoristiti \'cemo  problem egzaktnog pokriva\v{c}a tro\v{c}lanim skupovima da bi pokazali da je problem \eqref{problem_minimizacije_generalni} $\mathfrak{NP}$-te\v{z}ak.

\subsection[Egzaktni pokriva\v{c} tro\v{c}lanim skupovima][Egzaktni pokriva\v{c} tro\v{c}lanim skupovima]{Egzaktni pokriva\v{c} tro\v{c}lanim skupovima}
Za danu kolekciju $\{\mathcal{C}_i;\ i \in [N]\}$ tro\v{c}lanih podskupova od $[m]$, postoji li egzaktni pokriva\v{c} skupa $[m]$, tj. postoji li $J \subset [N]$ takav da $\cup_{j \in J}\mathcal{C}_j=[m]$, gdje je $\mathcal{C}_j \cap \mathcal{C}_k = \emptyset$ za svaki $j,k \in J$ razli\v{c}iti? Poznato je da je taj problem $\mathfrak{NP}$-potpun (vidi TODO).
\begin{thm}
    Za svaki $\eta \geq 0,\ \vec A \in \C^{m \times N}$ i $\vec y \in \C^m$, problem minimizacije \eqref{problem_minimizacije_generalni} je $\mathfrak{NP}$-potpun.
\end{thm}
\begin{proof}
    Zbog linearnosti problema \eqref{problem_minimizacije_generalni}, mo\v{z}emo uzeti da je $\eta < 1$. Pokazati \'cemo da se problem egzaktnog pokriva\v{c} mo\v{z}e u polinomijalnom vremenu reducirati na problem $\ell_0$-minimizacije. Neka je $\{\mathcal{C}_i;\ i \in [N]\}$ kolekcija tro\v{c}anih podskupova $[m]$. Definirajmo vektora $\vec a_1, \vec a_2,\dots \vec a_N \in \C^m$
    \begin{equation*}
        (\vec a_i)_j = 
        \begin{cases*}
            1\ \text{za } j \in \mathcal{C}_i,\\
            0\ \text{za } j \not\in \mathcal{C}_i
        \end{cases*}
    \end{equation*}
    Definiramo matricu $\vec A \in \C^{m \times N}$ i vektor $\vec y \in \C^m$ sa
    \begin{equation*}
        \vec A = [\vec a_1\ \vec a_2\ \cdots \ \vec a_N], \qquad \vec y = [1,1, \dots, 1]^T.
    \end{equation*}
    Po\v{s}to je $N \leq {m \choose 3}$, to mo\v{z}emo napraviti u polinomijalnom vremenu. Ako $\vec z \in \C^N$ zadovoljava $\norm{\vec{Az}-y}_2 \leq \eta$, tada su svih $m$ komponenti od $\vec{Az}$ udaljeljene od $1$ za najvi\v{s}e $\eta$, pa su te komponente razli\v{c}ite od nula, jer smo $\eta$ uzeli manji od $1$. Dakle, vrijedi $\norm{\vec{Az}}_0 = m$. Ali po\v{s}to svaki od vektora $\vec a_i$ imam to\v{c}no tri ne-nul komponente, vektor $\vec{Az}=\sum_{j=1}^N z_j \vec a_j$ ima najvi\v{s}e $r \norm{\vec z}_0$ ne-nul elemenata, tj. $\norm{\vec{Az}}_0 \leq 3 \norm{\vec{z}}_0$. Dakle, za svaki vektor $\vec z \in \C^N$ koji zadovoljava $\norm{\vec{Az}-\vec y}_2 \leq \eta$ vrijedi $\norm{\vec z}_0 \geq m/3$. Neka je sada $\vec x \in \C^N$ rje\v{s}enje $\ell_0$-minimizacije \eqref{problem_minimizacije_generalni}. Imamo dva slu\v{c}aj za normu vektora $\vec x$:
    \begin{enumerate}
        \item Ako je $\norm{\vec{x}}_0 = m/3$ tada je $\{\mathcal{C}_j;\ j \in \supp(\vec x)\}$ egzaktni pokriva\v{c} skupa $[m]$ jer ina\v{c}e bi neke od $m$ komponenti od $\vec{Ax}$ bile jednake od nula.
        \item Ako je $\norm{\vec{x}}_0 > m/3$ tada ne mo\v{z}e postojati egzaktni pokriva\v{c} $\{\mathcal{C}_j;\ j \in J\}$ jer bi u suprotnom vektor $\vec z \in \C^N$ definiran tako da je $z_j = 1$ ako je $j \in J$i $z_j = 0$ ako je $j \not \in J$, zadovoljavao $\vec{Az}=\vec y$ i $\norm{\vec z}_0=m/3$, \v{s}to je kontradikcija s minimalnosti vektora $\vec x$.
    \end{enumerate}
    Dakle, rje\v{s}avanjem problem $\ell_0$-minimizacije, mo\v{z}emo rje\v{s}iti problem egzaktnog pokriva\v{c}a tro\v{c}lanim skupovima, pa je stoga i sam problem $\ell_0$-minimizacije $\mathfrak{NP}$-potpun.
\end{proof}
\v{C}ini se da prethodni teorem predstavlja ozbiljnu zapreku u prakti\v{c}nom rje\v{s}avanju problema sa\v{z}etog uzorkovanja. No primjetimo, teorem tvrdi da je algoritam koji rje\v{s}ava problem $\ell_0$-minimizacije, za sve mogu\'ce matrie $\vec A$ i vektore $\vec y$ barem klase $\mathfrak{NP}$. Naravno, u samoj praksi nije nu\v{z}no zahtjevati rekonstrukciju za sve takve matrice i vektore. Naime, pokazat \'cemo da postoje algoritmi koji uspje\v{s}no rekonstruiraju $\vec x$ iz $\vec y$ za posebno dizajnirane matrice $\vec A$.




\chapter[Osnovni algoritmi sa\v{z}etog uzorkovanja][Osnovni algoritmi sa\v{z}etog uzorkovanja]{Osnovni algoritmi sa\v{z}etog uzorkovanja}\label{chapter_algoritmi}
Algoritmi za rje\v{s}avanje problema sa\v{z}etog uzorkovanja, koje \'cemo predstaviti, podijeljeni su u tri kategorije: optimizacije, greedy metode i grani\v{c}ne metode. U ovom poglavlju dati \'cemo samo pregled najpopularnijih algoritama, dok \'cemo formalnu analizu nekih od njih ostaviti za kasnije, nakon \v{s}to razvijemo potrebne teorijske alate.
\section[Optimizacijske metode][Optimizacijske metode]{Optimizacijske metode}
Op\v{c}eniti problem optimizacije je oblika
\begin{equation*}
    \min_{\vec x \in \R^N} F_0(\vec x)\quad\text{uz uvjet }F_i(\vec x) \leq b_i,\ i \in [n]
\end{equation*}
gdje $F_0:\R^N \rightarrow \R$ zovemo \textit{funkcija cilja}, a funkcije $F_1, \dots, F_n: \R^N \rightarrow \R$ zovemo \textit{funkcije ograni\v{c}enja}. Ako su $F_0, F_1, \dots, F_n$ konveksne funkcije, tada ovaj problem zovem \textit{problem konveksne optimizacije}. Ako su te funkcije linearne, tada je to \textit{problem linearnog programiranja}. Primjetimo da je problem rekonstrukcije rijetkog vektora \eqref{problem_minimizacije}, zapravo problem minimizacije. No, na\v{z}alost taj problem nije konveksan i kao \v{s}to smo u prethodnom poglavlju pokazali, op\v{c}enito je $\mathfrak{NP}$-te\v{z}ak. Prisjetimo se da $\norm{\vec z}_q^q$ konvergira k $\norm{\vec z}_0$ za $q \rightarrow 0^+$, pa je prirodno  \eqref{problem_minimizacije} aproksimirati problemom
\begin{equation}
    \min \norm{\vec z}_q\quad\text{uz uvjet }\vec{Az}=\vec y\tag{$P_{q}$}\label{problem_minimizacije_aprox}
\end{equation}
Poka\v{z}e se da za $q > 1$, \v{c}ak $1$-rijetki vektori nisu rje\v{s}enja od \eqref{problem_minimizacije_aprox}. Dok za $0 < q < 1$, \eqref{problem_minimizacije_aprox} ponovno nije konveksan i dalje je op\v{c}enito $\mathfrak{NP}$-te\v{z}ak. Za $q=1$, problem postaje konveksan
\begin{equation}
    \min \norm{\vec z}_1 \quad \text{uz uvjet }\vec{Az}=\vec y.\tag{$P_{1}$}\label{problem_minimizacije_l1}
\end{equation}
To je zapravo konveksna relaksacija problema \eqref{problem_minimizacije} i zovemo ga $\ell_1$\textit{-minimizacija} ili \textit{BP} algoritam (eng. \textit{basis pursuit}).

\begin{alg}{$\ell_1$-minimizacija (BP)}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$. \\
    \textit{Problem:}
        \begin{equation}
            \vec x^{\sharp} = \argmin \norm{\vec z}_1 \quad \text{uz uvjet }\vec{Az}=\vec y\tag{$\ell_1-min$}\label{algoritam_l1_minimizacija}
        \end{equation} \\
        \textit{Izlaz:} vektor $\vec x^{\sharp}$
\end{alg}

\noindent Poka\v{z}imo sada da su $\ell_1$-minimizatori rijetki vektori u realnom slu\v{c}aju.
\begin{thm}
    Neka je $\vec A \in \R^{m \times N}$ matrica mjerenja sa stupcima $\vec a_1, \dots, \vec a_N$. Ako je $\vec x^{\sharp}$ minimizator od
    \begin{equation*}
        \min_{\vec z \in \R^N} \norm{\vec z}_1\quad \text{uz uvjet } \vec{Az}=\vec y,
    \end{equation*}
    tada je skup $\{\vec a_j,\ j \in \supp(\vec x^{\sharp})\}$ linearno nezavisan i vrijedi
    \begin{equation*}
        \norm{\vec{x}^{\sharp}}_0 = \card(\supp(\vec x^{\sharp})) \leq m. 
    \end{equation*}
\end{thm}
\begin{proof}
    Pretpostavimo suprotno, tj. da je skup $\{\vec a_j,\ j \in \supp(\vec x^{\sharp})\}$ linearno zavisan. Neka je $S= \supp(\vec x^{\sharp})$. To zna\v{c}i da postoji ne-nul vektor $\vec v \in \R^N$ sa nosa\v{c}em na $S$ takav da $\vec{Av} = \vec 0$. Tada za svaki $t \not= 0$
    \begin{equation*}
        \norm{\vec x^{\sharp}}_1 < \norm{\vec x^{\sharp} + t \vec v}_1 = \sum_{j \in S}|x_j^{\sharp} + tv_j| = \sum_{j \in S}\sgn(x_j^{\sharp}+tv_j)(x_j^{\sharp}+tv_j)
    \end{equation*}
    Ako je $|t|$ dovoljno mali, tj. $|t| < \min_{j \in S}|x_j^{\sharp}|/ \norm{\vec v}_{\infty}$ onda vrijedi
    \begin{equation*}
        \sgn(x_j^{\sharp}+tv_j) = \sgn(x_j^{\sharp})\quad \text{za svaki }j \in S.
    \end{equation*}
    Dakle, za $0<|t|<\min_{j \in S}|x_j^{\sharp}|/ \norm{\vec v}_{\infty}$ slijedi
    \begin{equation*}
    \begin{split}
        \norm{\vec x^{\sharp}}_1 & <  \sum_{j \in S} \sgn(x_j^{\sharp})(x_j^{\sharp}+tv_j)=\sum_{j \in S} \sgn(x_j^{\sharp})(x_j^{\sharp})+t \sum_{j \in S}\sgn(x_j^{\sharp})v_j \\ &= \norm{\vec x^{\sharp}}_1 + t \sum_{j \in S}\sgn(x_j^{\sharp})v_j.
    \end{split}
    \end{equation*}
    No, to je kontradikcija jer $t \not = 0$ mo\v{z}emo odabrati dovoljno mali tako da je \\ $t \sum_{j \in S}\sgn(x_j^{\sharp})v_j \leq 0$.
\end{proof}

\indent
U realnom slu\v{c}aju, \eqref{problem_minimizacije_l1} mo\v{z}emo reinterpretirati kao problem linearnog programiranja, tako da uvedemo pomo\v{c}ne varijable $\vec z^+,\ \vec z^- \in \R^N$ definirane sa
\begin{multicols}{2}
    \noindent
    \begin{equation*} 
        z_j^+ = 
        \begin{cases}
            z_j\ & \text{za } z_j > 0, \\
            0\ & \text{za } z_j \leq 0
        \end{cases}
    \end{equation*}
    \begin{equation*} 
        z_j^- = 
        \begin{cases}
            0\ & \text{za } z_j > 0, \\
            -z_j\ & \text{za } z_j \leq 0
        \end{cases}
    \end{equation*}
\end{multicols}
za svaki $j \in [N]$. Tada je problem \eqref{problem_minimizacije_l1} ekvivaltan problemu
\begin{equation}
    \min_{\vec z^+,\vec z^- \\ \in \R^N} \sum_{j=1}^{N}(z_j^+ + z_j^-) \quad \text{uz uvjet }
    \begin{bmatrix*}
        \vec A & -\vec A
    \end{bmatrix*}
    \begin{bmatrix*}
        \vec z^+ \\ \vec z^-
    \end{bmatrix*}
    = \vec y, \quad
    \begin{bmatrix*}
        \vec z^+ \\ \vec z^-
    \end{bmatrix*}
    \geq 0.\tag{$P_1'$}
\end{equation}
Isto ne vrijedi za kompleksni slu\v{c}aj. Tu \v{c}injenicu pokazati \'cemo na op\'{c}enitijim problemu,
\begin{equation}\label{problem_minimizacije_l1_kvadraticni}
    \min \norm{\vec z}_1 \quad \text{uz uvjet } \norm{\vec{Az}-y}_2 \leq \eta.\tag{$P_{1,\eta}$}
\end{equation}
Taj problem je zapravo pogodniji za praksu, po\v{s}to vektor $\vec y \in \C^m$ ne mo\v{z}emo izmjeriti s beskona\v{c}nom to\v{c}no\v{s}\'cu, ve\'c uz neku gre\v{s}ku $\vec e \in \C^m$ pa je stoga
\begin{equation*}
    \vec y = \vec{Ax} + \vec e. 
\end{equation*}
Takvoj gre\v{s}ki \v{c}esto mo\v{z}emo ocjeniti $\ell_2$-normu, po\v{s}to ona ima interpretaciju energije,
\begin{equation*}
    \norm{\vec e}_2 \leq \eta, \quad \text{za neki } \eta > 0.
\end{equation*}
Za dani vektor $\vec z \in \C^N$, neka su $\vec u,\ \vec v \in \R^N$ njegovi realni i imaginarni djelovi te neka je $\vec c \in \R^N$ takav d je $c_j \geq |z_j| = \sqrt{u_j^2+v_j^2}$ za sve $j \in [N]$. Problem \eqref{problem_minimizacije_l1_kvadraticni} je tada ekvivaltan problemu
\begin{equation}
\begin{split}
    \min_{\vec c, \vec u, \vec v \in \R^N}\sum_{j=1}^N c_j\quad \text{uz uvjete }& 
    \norms{
        \begin{bmatrix*}
            \Re(\vec A) & -\Im(\vec A) \\
            \Im(\vec A) & \Re(\vec A)
        \end{bmatrix*}
        \begin{bmatrix*}
           \vec u \\ \vec v 
        \end{bmatrix*}
        -
        \begin{bmatrix*}
            \Re(\vec y) \\ \Im(\vec y) 
    \end{bmatrix*} }_2 \leq \eta \\
    &\sqrt{u_j^2 + v_j^2} \leq c_j,\quad \forall j \in [N].
\end{split}\tag{$P_{1, \eta}'$}\label{problem_minimizacije_l1_kvadraticni_drugi_oblik}
\end{equation}
Ovo je \textit{problem konike drugog reda}. Primjetimo da za $\eta=0$ dobivamo formulaciju problema \eqref{problem_minimizacije_l1} za kompleksni slu\v{c}aj u takvom obliku.
\\\indent Princip rje\v{s}avanja \eqref{problem_minimizacije_l1_kvadraticni} zove se \textit{kvadrati\v{c}no ograni\v{c}ena $\ell_1$-minimizacija} ili \textit{$\ell$-minimizacija osjetljiva na \v{s}um} (eng. \textit{quadratically constrainted basis pursuit}).
\begin{alg}{Kvadrati\v{c}no ograni\v{c}ena $\ell_1$-minimizacija}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, razina \v{s}uma $\eta$. \\
    \textit{Problem:}
        \begin{equation}
            \vec x^{\sharp} = \argmin \norm{\vec z}_1 \quad \text{uz uvjet }\norm{\vec{Az}-y}_2 \leq \eta\tag{$\ell_1-min_{\eta}$}\label{algoritam_l1_minimizacija_kvadraticna}
        \end{equation} \\
        \textit{Izlaz:} vektor $\vec x^{\sharp}$
\end{alg}
Rje\v{s}enje $\vec x^{\sharp}$ povezano je s rje\v{s}enjem problema $\ell_1$-minimizacije sa ugra\dj enim uklanjanjem \v{s}uma
\begin{equation}\label{problem_minimizacije_l1_sum}
    \min_{\vec z \in \C^N} \lambda\norm{\vec z}_1 + \norm{\vec{Az}-\vec y}_2^2
\end{equation}
za neki $\lambda \geq 0$. Tako\dj er povezano je s rje\v{s}enjem \textit{LASSO} problema, za neki $\tau \geq 0$,
\begin{equation}\label{lasso}
    \min_{\vec z \in \C^N} \norm{\vec{Az}-\vec y}_2\quad \text{uz uvjet } \norm{\vec z}_1\leq \tau
\end{equation}
To upravo tvrdi naredna propozicija.
\begin{prop}
    \begin{enumerate}[label=(\alph*)]
        \item Ako je $\vec x$ minimizator problema \eqref{problem_minimizacije_l1_sum} sa $\lambda > 0$, onda postoji $\eta = \eta_{\vec x} \geq 0$ takva da je $\vec x$ minizator kvadrati\v{c}no ograni\v{c}ene $\ell_1$-minimizacije \eqref{problem_minimizacije_l1_kvadraticni}.
        \item Ako je $\vec x$ jedinstveni minimizator problema \eqref{problem_minimizacije_l1_kvadraticni} sa $\eta \geq 0$, onda postoji $\tau = \tau_{\vec x} \geq 0$ takav da je $\vec x$ minimizator LASSO problema \eqref{lasso}.
        \item Ako je $\vec x$ minimizator LASSO problema \eqref{lasso}, onda postoji $\lambda = \lambda_{\vec x} \geq  0$ takva da je $\vec x$ minimizator problema \eqref{problem_minimizacije_l1_sum}.
    \end{enumerate}
\end{prop}
\begin{proof}
    \begin{enumerate}[label=(\alph*)]
        \item Neka je $\eta := \norm{\vec{Ax}- \vec y}_2$ i $\vec z \in \C^N$ takav da je $\norm{\vec{Az-y}}_2 \leq \eta$. Po\v{s}to je prema pretpostavci $\vec x$ minimizator od \eqref{problem_minimizacije_l1_sum} slijedi,
            \begin{equation*}
                \lambda \norm{\vec x}_1 + \norm{\vec{Ax} -\vec{y}}_2^2 \leq \lambda \norm{\vec z}_1 + \norm{\vec{Az} - \vec{y}}_2^2 \leq \lambda \norm{\vec z}_1 + \norm{\vec{Ax}-\vec y}_2^2.
            \end{equation*}
            Dakle slijedi da je $\norm{\vec x}_1 \leq \norm{\vec{y}}_1$, pa je $\vec x$ minimizator problema \eqref{problem_minimizacije_l1_kvadraticni}

        \item Neka je $\eta := \norm{\vec x}_1$ i neka je $\vec z \in \C^N\backslash\{\vec x\}$ takav da je $\norm{\vec z}_1 \leq \tau$. Po\v{s}to je $\vec x$ jedinstveni minimizator od \eqref{problem_minimizacije_l1_kvadraticni} to zna\v{c}i da $\vec z$ ne mo\v{z}e zadovoljavati uvjet iz \eqref{problem_minimizacije_l1_kvadraticni}, pa stoga $\norm{\vec{Az}- \vec{y}}_2 > \eta \geq \norm{\vec{Ax}-\vec y}_2$. Dakle, $\vec x$ je jedinstveni minimizator \textit{LASSO} problema.
        \item Za dokaz ove tvrdnje potrebni su alati konveksne analize, vidi (TODO).
    \end{enumerate}
\end{proof}

\section[Greedy metode][Greedy metode]{Greedy metode}
Upoznati \'cemo se sa dva iterativna greedy algoritma koji se \v{c}esto koriste u kontekstu sa\v{z}etog uzorkovanja. Prvo algoritam koji \'cemo prou\v{c}iti zove se \textit{OMP} (skra\v{c}enica od eng. \textit{orthogonal matching pursuit}).
\begin{alg}{OMP}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$. \\
    \textit{Inicijalizacija:} $S^0 = \emptyset$, $\vec x^0 = \vec 0$ \\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{align}
            &S^{n+1} = S^n \cup \{j_{n+1}\},\quad j_{n+1} := \argmax\limits_{j \in [N]}\{|(\vec A^*(\vec y - \vec{Ax}^n))_j|\},\tag{$OMP_1$}\label{omp_1}
        \\
            &\vec x^{n+1} = \argmin\limits_{\vec z \in \C^N}\{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset S^{n+1}\}.\tag{$OMP_2$}\label{omp_2}
        \end{align} \\
        \textit{Izlaz:} $\bar{n}$-rijedak vektor $\vec x^{\sharp}=\vec{x}^{\bar{n}}$.
\end{alg}

Numeri\v{c}ki najskuplja operacija ovog algoritma je \eqref{omp_2}. Situacije se mo\v{z}e popraviti kori\v{s}tenjem $QR$ dekompozicije matrice $\vec{A}_{S_n}$. Tada se mogu iskoristiti efikasni algoritmi za a\v{z}uriranje $QR$ dekompozicije kada se u matricu doda novi stupac. Nadalje, za dodatna ubrzanja mogu se iskoristiti i algoritmi za brzo matrica-vektor mno\v{z}enje bazirani na brzoj Fourierovoj transformaciji (vidi TODO).
\newline\indent
Indeks $j_{n+1}$ bira se tako da se reducira $\ell_2$-norma reziduala $\vec{y} - \vec{Ax}^n$ \v{s}to je vi\v{s}e mogu\'ce. Sljede\'ca lema opravdava za\v{s}to je smisleno $j$ odabrati takav da maksimizira vrijednost $|{(\vec{A}^*(\vec{y}-\vec A \vec x^n))_j}|$.
\begin{lem}
    Neka je $\vec A \in \C^{m \times N}$ sa $\ell_2$-normaliziranim stupcima. Ako su $S \subset [N]$, $\vec v \in \C^N$ sa nosa\v{c}em na $S$, $j \in [N]$, te ako vrijedi
    \begin{equation*}
        \vec w := \argmin_{\vec z \in \C^N} \{ \norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset S \cup \{j\} \},
    \end{equation*}
    tada
    \begin{equation*}
        \norm{\vec y - \vec{Aw}}_2^2 \leq \norm{\vec y - \vec{Av}}_2^2 - |{(\vec{A}^*(\vec y - \vec{Av}))_j|^2.
    \end{equation*}
\end{lem}
\begin{proof}
    Po\v{s}to svaki vektor oblika $\vec v + t \vec e_j,\ t \in \C$ ima nosa\v{c} u $S \cup \{j\}$ vrijedi,
    \begin{equation*}
        \norm{\vec y - \vec{Aw}}_2^2 \leq \min_{t \in \C} \norm{\vec y - \vec{A}(\vec v + t \vec e_j)}_2^2
    \end{equation*}
    Stavimo da je $t = \rho e^{i \theta}$, gdje je $\rho \geq 0$ i $\theta \in [0,2 \pi)$. Imamo,
    \begin{align*}
        \norm{\vec y - \vec{A}(\vec v + t \vec e_j)}_2^2 &= \norm{\vec y - \vec{Av} - t \vec{A}\vec{e}_j}_2^2\\
        &= \norm{\vec y - \vec{Av}}_2^2 + |t|^2 \norm{\vec{Ae}_j}_2^2 - 2 \Re(\bar{t}\langle \vec{y} - \vec{Av}, \vec{Ae}_j \rangle)\\
        &= \norm{\vec y - \vec{Av}}_2^2 + \rho^2 - 2 \Re(\rho e^{-i \theta}(\vec{A}^*(\vec y - \vec{Av}))_j)\\
        & \geq \norm{\vec y - \vec{Av}}_2^2 + \rho^2 - 2 \rho |{(\vec{A}^*(\vec y - \vec{Av}))_j}|^2
    \end{align*}
    gdje jednakost vrijedi za pogodno odabrani $\theta$. Kao kvadratni polinom u varijabli $\rho$, zadnji izraz poprima minimum za $\rho = |{(\vec{A}^*(\vec y - \vec{Av}))_j}|$.
\end{proof}

Korak \eqref{omp_2} mo\v{s}e se prikazati u obliku
\begin{equation*}
    \vec{x}_{S^{n+1}}^{n+1} = \vec{A}_{S^{n+1}}^{\dagger}\vec y,
\end{equation*}
gdje je $\vec{x}_{S^{n+1}}^{n+1}$ restrikcija od $\vec x^{n+1}$ na svoj nosa\v{c} $S^{n+1}$ i gdje je $\vec A^{\dagger}_{S^{n+1}}$ pseudo-inverz od $\vec{A}_{S^{n+1}}$ (vidi TODO). Drugim rje\v{c}ima to zna\v{c}i da je $\vec z = \vec x ^{n+1}_{S^{n+1}}$ rje\v{s}enje sustava $\vec A^*_{S^{n+1}} \vec A_{S^{n+1}} \vec z = \vec A^*_{S^{n+1}} \vec y$. Ta \v{c}injenica je korisna i u drugim algoritmima koji imaju korak sli\v{c}an \eqref{omp_2}.
\begin{lem}\label{omp_lema_1}
    Neka je $S \subset [N]$ i
    \begin{equation*}
        \vec v := \argmin_{\vec z \in \C^N} \{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset S\},
    \end{equation*}
    tada je
    \begin{equation}\label{omp_ortogonalnost}
        (\vec A^*(\vec y - \vec{Av}))_S = \vec 0.
    \end{equation}
\end{lem}
\begin{proof}
    Prema definiciji vektora $\vec v$, vektor $\vec{Av}$ je orthogonalna projekcija vektora $\vec y$ na prostor $\{\vec{Az},\ \supp(\vec{z} \subset S)\}$, pa je karakteriziran relacijom ortogonalnosti
    \begin{equation*}
        \langle \vec y - \vec{Av}, \vec{Az} \rangle = 0 \quad \text{za sve }\vec z \in \C^N \ \text{takve da } \supp(\vec z) \subset S.
    \end{equation*}
    Dakle, imamo da vrijedi $\langle \vec A^*( \vec y - \vec{Av}), \vec{z} \rangle = 0$ za sve $\vec z \in \C^N,\ \supp(\vec z) \subset S$, \v{s}to vrijedi ako i samo ako vrijedi \eqref{omp_ortogonalnost}.
\end{proof}

Prirodan uvjet zaustavljanja OMP-a je kada se postigne $\norm{\vec{y} - \vec{Ax}^{\bar{n}}} \leq \varepsilon$ ili $\norm{\vec A^*(\vec y - \vec{Ax}^{\bar{n}})_{\infty}} \leq \varepsilon$ za neku toleranciju $\varepsilon > 0$. Ako nam je dostupna estimacija rijetkosti $s$ rje\v{s}enja $\vec x$, tada je razumno stati kada je $\bar{n} = s$. Sljede\'ci rezultat govori o uvjetim za uspje\v{s}nu rekonstrukciju $s$-rijetkog vektora u $s$ iteracija OMP algoritma.

\begin{prop}\label{prop:3:5}
    Neka je $\vec A \in \C^{m \times N}$, svaki ne-nul vektor $\vec x \in \C^N$ sa nosa\v{c}emo na skupu $S$, kardinaliteta $s$ mo\v{z}e se rekonstruirati iz $\vec y = \vec{Ax}$ u najvi\v{s}e $s$ iteracija OMP algoritma ako i samo ako je matrica $\vec A_S$ injektivna i 
    \begin{equation}\label{uvjet_rekon_omp}
        \max_{j \in S}|(\vec A^* \vec r)_j| > \max_{l \in \bar{S}}|(\vec A^* \vec r)_l|
    \end{equation}
    za sve ne-nul $\vec r \in \{\vec{Az},\ \supp(\vec z) \subset S\}$.
\end{prop}
% TODO nije mi bas jasno ovo
\begin{proof}
    Pretpostavimo da OMP algoritam rekonstruira sve vektore sa nosa\v{c}emo na skupu $S$ u najvi\v{s}e $s = \card(S)$ iteracija. Neka su $\vec v, \vec w$ sa nosa\v{c}em na $S$, takvi da je $\vec{Av}=\vec{Aw}$. Zbog pretpostavke, $\vec v$ i $\vec w$ moraju biti jednaki, a to zna\v{c}i da je matrica $\vec A_S$ injektivna. Nadalje, ako je $\vec y = \vec{Ax}$ za neki $\vec x \in \C^N$ sa $\supp(\vec x)=S$, indeks $l \in \bar S$ ne mo\v{z}e biti izabran u prvoj iteraciji, po\v{s}to indeks izabran u prvoj iteraciji ostaje uvijek u nosa\v{c}u, a po pretpostavci OMP rekonstruira $\vec x$ iz $\vec y = \vec{Ax}$ u to\v{c}no $s$ iteracija. Dakle za $n=0$ iz \eqref{omp_1} imamo da je $\max_{j \in S}|(\vec A^*y)_j| > |(\vec A^*y)_l|$ za svaki $l \in \bar{S}$, pa stoga vrijedi $\max_{j \in S}|(\vec A^*y)_j| > \max_{l \in \bar{S}}|(\vec A^*y)_l|$ za sve ne-nul $\vec y \in \{\vec{Az},\ \supp(\vec z) \subset S\}$. \\
    \indent
    Obratno, pretpostavimo da je $\vec{Ax}^1 \neq y,\dots,\vec{Ax}^{s-1} \neq y$ jer u suprotnom nemamo \v{s}to dokazivati. Pokazati \'cemo da $S^n \subset S,\ \card(S^n)=n$ za $0 \leq n \leq s$. To \'ce implicirati $S^s = S$. Nadalje, \eqref{omp_2} daje $\vec{Ax}^s = \vec y$ a iz injektivnosti od $\vec{A}_S$ slijedi $\vec x_s = \vec{x}$. Dakle, neka je $0 \leq n \leq s-1$. Ako je $S^n \subset S$, to povla\v{c}i da je $\vec r^n := \vec y - \vec{Ax}^n \in \{\vec{Az},\ \supp(\vec z) \subset S\}$, pa prema \eqref{uvjet_rekon_omp} indeks $j_{n+1}$ le\v{z}i u S, pa $S^{n+1} = S \cup \{j_{n+1}\} \subset S$. Ovo induktivno pokazuje da je $S^n$ podskup od $S$ za svaki $0 \leq n \leq s$. Nadalje, neka je $1 \leq n \leq s-1$. Lema \eqref{omp_lema_1} daje $(\vec{A}^* \vec r^n)_{S^n} = \vec 0$. Stoga, iz \eqref{omp_1} vidimo da indeks $j_{n+1}$ ne le\v{z}i u $S^{n}$, jer bi u protivnom $\vec A^* \vec r^n = \vec 0$, a po \eqref{uvjet_rekon_omp} $\vec r^n = \vec 0$. Dakle, $\card(S^n)=n$.
\end{proof}
\indent
Slabost OMP algoritma le\v{z}i u \v{c}injenici da ako krivi indeks u\dj e u nosa\v{c}, on ostaje u nosa\v{c}u u svim sljede\'cim iteracijama. Stoga $s$ iteracija algoritma nije dovoljno za rekonstrukciju vektora koji je $s$-rijedak. Mogu\'ce rje\v{s}enje je pove\'cati broj iteracija. Naredni algoritam, CoSaMP (eng. \textit{compressive sampling matching pursuit algorithm}), koristi druga\v{c}iju strategiju kada nam je dostupna estimacija rijetkosti $s$. Uvedimo oznake $H_s(\vec z)$ za najbolju $s$-rijetku aproksimaciju vekotra $\vec z \in \C^N$ i $L_s(\vec z)$ za nosa\v{c} od $H_s(\vec z)$, tj.
\begin{align}
    &L_s(\vec z) := \text{skup indeksa $s$ najve\'cih komponeneti vekora } \vec z \in \C^N \\
    &H_s(\vec z) := \vec z_{L_s(\vec z)}.
\end{align}
Nelinearni operator $H_s$ zovemo \textit{hard thresholding} operator reda $s$. Za dani vektor $\vec z \in \C^N$ on pu\v{s}ta $s$ apsolutno najve\'cih komponeneti a ostale postavi na nulu. Primjetimo da to nije nu\v{z}no jedinstveno definiramo. Da bi zaobi\v{s}li taj problem, skup indeksa $L_s(\vec z)$ biramo iz svih mogu\'cih kandidata leksikografskim poredkom.

\begin{alg}{CoSaMP}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Inicijalizacija:} $s$-rijedak vektor $\vec x^0$ (npr. $\vec x^0 = \vec 0$).\\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{align*}
            U^{n+1} & = \supp(\vec x^n)\cup L_{2s}(\vec A^*(\vec y - \vec{Ax}^n))  \tag{$CoSaMP_1$}\label{cosamp_1}\\
            \vec u^{n+1} & = \argmin_{\vec z \in \C^N}\{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset U^{n+1}\}  \tag{$CoSaMP_2$}\label{cosamp_2}\\
            \vec x^{n+1} & = H_s(\vec u^{n+1})  \tag{$CoSaMP_3$}\label{cosamp_3}
        \end{align*}
        \textit{Izlaz:} $\bar{n}$-rijedak vektor $\vec x^{\sharp}=\vec{x}^{\bar{n}}$.
\end{alg}



\section[Grani\v{c}ne metode][Grani\v{c}ne metode]{Grani\v{c}ne metode}
Algoritmi predstavljeni u ovom poglavlju tako\dj er koriste \textit{hard thresholding} operator $H_s$. Prvi algoritam, BT (eng. \textit{basic thresholding}), sastoji se od odre\dj ivanja nosa\v{c}a $s$-rijetkog vektora $\vec x \in \C^N$, koji se rekonstruira iz $\vec y = \vec{Ax} \in \C^m$, kao indeksi $s$ najve\'cih komponenti vektora $\vec A^* \vec y$, te tra\v{z}enja vektora koji najbolje aproksimira mjerenje $\vec y$

\begin{alg}{BT}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Problem:}
        \begin{align*}
            S^{\sharp} &= L_s(\vec A^* \vec y),\tag{$BT_1$}\label{bt_1}\\
            \vec x^{\sharp} &= \argmin_{\vec z \in \C^N}\{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset S^{\sharp}\}.\tag{$BT_2$}\label{bt_2}\\
        \end{align*}
        \textit{Izlaz:} $s$-rijedak vektor $\vec x^{\sharp}$.
\end{alg}

\noindent Dovoljni i nu\v{z}i uvjeti rekonstrukcije jednostavnim BT algoritmom, sli\v{c}ni su uvjetu \eqref{uvjet_rekon_omp}.
\begin{prop}
    BT algoritam rekonstruira vektor $\vec x \in \C^N$ sa nosa\v{c}em na $S$, iz $\vec y = \vec{Ax}$ ako i samo ako
    \begin{equation}\label{bt_uvjet}
        \min_{j \in S}|(\vec A^* \vec y)_j| > \max_{l \in \bar{S}} |(\vec A^* \vec y)_l| .
    \end{equation}
\end{prop}
\begin{proof}
    Vektor $\vec x$ mo\v{z}e se rekonstruirati ako i samo ako skup indeksa $S^{\sharp}$ u \eqref{bt_1} jednak skupu $S$. A to vrijedi ako i samo ako je element vektora $\vec A^* \vec y$ s indeksom iz $S$, ve\'ci od svakog elementa vektora $\vec A^* \vec y$ s indeksom u $\bar{S}$.
\end{proof}
\indent
IHT (eng. \textit{iterative hard thresholding}) algoritam rje\v{s}ava kvadratni sustav $\vec A^* \vec A \vec z= \vec A^* \vec y$ umjesto $\vec{Az}=\vec y$. To mo\v{z}emo interpretirati kao rje\v{s}avanje problema fiksne to\v{c}ke $\vec z = (\vec{I}- \vec A^* \vec A ) \vec z + \vec A^* \vec y$. Prirodno je gledati iteracije oblika $\vec x^{n+1} = (\vec{I}- \vec A^* \vec A) \vec x^n + \vec A^* \vec y$. Po\v{s}to tra\v{z}imo $s$-rijetko rje\v{s}enje u svakoj iteraciji uzimamo samo $s$ apsolutno najve\'cih komponenti od $(\vec{I} - \vec A^* \vec A ) \vec x^n + \vec A^* \vec y$.

\begin{alg}{IHT}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Inicijalizacija:} $s$-rijedak vektor $\vec x^0$ (npr. $\vec x^0 = \vec 0$).\\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{equation}
            x^{n+1} = H_s(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n).\tag{$IHT$}\label{iht}\\
        \end{equation}
        \textit{Izlaz:} $s$-rijedak vektor $\vec x^{\sharp}=\vec x^{\bar n}$.
\end{alg}

Primjetimo da IHT algoritam ne koristi orthogonalne projekcije, \v{s}to je njegova prednost. No, ako smo spremi platiti cjenu projekcija, ima smisla gledati vektor koji ima isti nosa\v{c} kao $\vec x^{n+1}$ koji najbolje aproksimira mjerenje. Upravo je to strategija HTP (eng. \textit{hard thresholding pursuit}) algoritma.

\begin{alg}{HTP}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Inicijalizacija:} $s$-rijedak vektor $\vec x^0$ (npr. $\vec x^0 = \vec 0$).\\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{align*}
            S^{n+1} &= L_s(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n),\tag{$HTP_1$}\label{htp_1}\\
            \vec x^{n+1} &= \argmin_{\vec z \in \C^N}\{ \norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset S^{n+1} \}.\tag{$HTP_2$}\label{htp_2}
        \end{align*}
        \textit{Izlaz:} $s$-rijedak vektor $\vec x^{\sharp}=\vec x^{\bar n}$.
\end{alg}





\chapter[$\ell_1$-minimizacija][$\ell_1$-minimizacija]{$\ell_1$-minimizacija}
Prisjetimo se, problem sa\v{z}etog uzorkovanja sastoji se od rekonstrukcije $s$-rijetkog vektora $\vec x \in \C^N$ iz mjerenja $\vec y = \vec{Ax} \in \C^m$, gdje je $m < N$. Prirodno se name\'ce problem $\ell_0$-minimizacije,
\noindent
\begin{equation}
\min_{\vec z \in \C^N} \norm{\vec z}_0\quad \text{uz uvjet }\vec{Az} = \vec{y}\tag{$P_0$}
\end{equation}
U poglavlju \eqref{chapter_rijetka_rijesenja} vidjeli smo da je taj problem op\'cenito $\mathfrak{NP}$-te\v{z}ak. U poglavlju \eqref{chapter_algoritmi} pokazali smo nekoliko u\v{c}inkovitih strategija za rje\v{s}avanje problema sa\v{z}etog uzorkovanja. U ovom poglavlju fokusirati \'cemo se na strategiju $\ell_1$-minimizacije
\begin{equation}
    \min_{\vec z \in \C^N} \norm{\vec z}_1 \quad \text{uz uvjet }\vec{Az}=\vec y.\tag{$P_1$}
\end{equation}
Prou\v{c}iti \'{c}emo uvjete na matricu $\vec A$ koji osiguravaju egzaktnu ili aproksimativnu rekonstrukciju vektora $\vec x$.

\section[Svojstvo nul-prostora][Svojstvo nul-prostora]{Svojstvo nul-prostora}
Argumenti u ovom potpoglavlje vrijede u oba kontekstu realnih i u konteksu kompleksnih prostora. Stoga \'cemo rezultate prvo iznjeti za polje $\K$, koje mo\v{z}e $\R$ ili $\C$. Nakon toga uspostaviti \'cemo ekvivalentnost realnog i kompleksnog svojstva nul-prostora.

\begin{defn}
    Za matricu $\vec A \in \K^{m \times N}$ ka\v{z}emo da zadovoljava \textit{svojstvo nul-prostora} za skup $S \subset [N]$ ako
    \begin{equation}\label{svojstvo_nul_prostora}
        \norm{\vec v_S}_1 < \norm{\vec v_{\bar{S}}}_1  \quad \text{za svaki }\vec v \in \ker \vec A \backslash \{\vec 0\}.
    \end{equation}
    Nadalje, ka\v{z}emo da $\vec A$ zadovoljava svojstvo nul-prostora reda $s$ ako zadovoljava gornju nejednakost za svaki $S \subset [N]$ takav da $\card(S) \leq s$.
\end{defn}

Primjetimo da za vektor $\vec v \in \ker \vec A \backslash \{ \vec 0\}$ svojstvo nul-prostora vrijedi za svaki $S \subset [N]$ takav da $\card(S) \leq s$, \v{c}im vrijedi za skup indeksa $s$ apsolutno najve\'cih komponenti vektora $\vec v$. \\
\indent Postoje dvije dodatne formulaciju svojsta nul-prostora. Prvu dobijemo tako da gornjoj nejednakosti dodamo $\norm{\vec v_s}_1$ s obje strane. Tada imamo
\begin{equation}\label{svojstvo_nul_prostora_form_1}
    2 \norm{\vec v_S}_1 < \norm{\vec v}_1 \quad \text{za svaki } \vec v \in \ker \vec A \backslash \{\vec 0\}.
\end{equation}
Drugu dobijemo tako da u skup $S$ stavimo $s$ apsolutno najve\'cih  komponenti vektora $\vec v$ i ovaj put nejednakosti dodamo $\norm{\vec v_{\bar S}}_1$ s obje strane. Tada imamo
\begin{equation}\label{svojstvo_nul_prostora_form_2}
    \norm{\vec v}_1 < 2 \sigma_s(\vec v)_1 \quad \text{za sve } \vec v \in \ker \vec A \backslash \{\vec 0\}.
\end{equation}
Prisjetimo se definicije \ref{greska_naj_s_aprox} $\ell_p$-gre\v{s}ke najbolje $s$-rijetke aproksimacija vektora $\vec x \in \K^N$,
\begin{equation*}
    \sigma_s(\vec x)_p = \inf_{\norm{\vec z} \leq s} \norm{\vec x - \vec z}_p.
\end{equation*}
Sljede\'ci teorem govori o veci svojstva nul-prostora i egzaktne rekonstrukcije rijetkog vektora putem $\ell_1$-minimizacije.
\begin{thm}\label{bp_tm1}
    Za $\vec A \in \K^{m \times N}$, svaki vektor $\vec x \in \K^N$ sa nosa\v{c}em na $S$ je jedinstveno rje\v{s}enje od \eqref{problem_minimizacije_l1} sa $\vec y = \vec {Ax}$ ako i samo ako $\vec A$ zadovoljava svojstvo nul-prostora za skup $S$.
\end{thm}
\begin{proof}
    Neka je skup indeksa $S$ fiksan. Pretpostavimo da je svaki vektor $\vec x \in \K^N$ s nosa\v{c}em na $S$ jedinstveni minimizator od $\norm{\vec z}_1$ takav da $\vec {Az} = \vec {Ax}$. Stoga za svaki $\vec v \in \ker \vec A \backslash \{\vec 0\}$, vektor $\vec v_S$ je jedinstveni minimizator od $\norm{\vec z}_1$ takav da $\vec{Az} = \vec{Av}_S$. Ali imamo $\vec A (-\vec v_{\bar S}) = \vec A \vec v_S$ i $-\vec v_{\bar S} \neq \vec v_S$ jer je $\vec v \neq \vec 0$ i $ \vec 0 = \vec{Av} = \vec A (\vec v_S + \vec v_{\bar S})$. Dakle, mora vrijediti $\norm{\vec v_S}_1 < \norm{\vec v_{\bar S}}_1$.
\indent
Obratno, pretpostavimo da $\vec A$ zadovoljava svojstvo nul-prostora za skup $S$. Tada za vektor $\vec x \in \K^N$ sa nosa\v{c}em na $S$ i za $\vec z \in \K^N$, $\vec z \neq \vec x$ takvi da $\vec{Az}=\vec{Ax}$, ozna\v{c}imo vektor $\vec v := \vec x - \vec z \in \ker \vec A \backslash \{\vec 0\}$. Imamo,
\begin{equation*}
    \norm{\vec x} \leq \norm{\vec x - \vec z_S}_1 + \norm{\vec z_S}_1 = \norm{\vec v_S}_1 + \norm{\vec z_S}_1 < \norm{\vec v_{\bar S}}_1 + \norm{\vec z_S}_1 = \norm{- \vec z_{\bar S}}_1 + \norm{\vec z_S}_1 = \norm{\vec z}_1
\end{equation*}
Dakle, vektor $\vec x$ je minimizator od \eqref{problem_minimizacije_l1}.
\end{proof}

\indent
Variranjem skupa $S$, sljede\'ci rezultat sljedi direktno iz prethodnog teorema. 

\begin{thm}\label{svojstvo_nul_prostora_tm}
    Za matricu $\vec A \in \K^{m \times N}$, svaki $s$-rijedak vektor $\vec x \in \K^N$ je jedinstveno rje\v{s}enje problema \eqref{problem_minimizacije_l1} uz $\vec y = \vec{Ax}$ ako i samo ako $\vec A$ zadovoljava svojstvo nul-prostora reda $s$.
\end{thm}

Primjetimo da prethodni teorem tvrdi da za svaki $\vec y = \vec{Ax}$, gdje je $\vec x$ $s$-rijedak, $\ell_1$-minimizacija \eqref{problem_minimizacije_l1} zapravo rje\v{s}ava problem $\ell_0$-minimizacije \eqref{problem_minimizacije} kada vrijedi svojstvo nul-prostora reda $s$. Zaista, pretpostavimo da se svaki $s$-rijedak vektor $\vec{x}$ mo\v{z}e rekonstruirati $\ell_1$-minimizacijom iz $\vec y = \vec{Ax}$. Neka je $\vec z$ minimizator $\ell_0$ problema \eqref{problem_minimizacije} sa $\vec y = \vec{Ax}$, tada je $\norm{\vec z}_0 \leq \norm{\vec x}_0$ pa je $\vec z$ tako\dj er $s$-rijedak. No, svaki $s$-rijedak vektor je jedinstveni $\ell_1$-minimizator, slijedi da je $\vec x = \vec z$.
\newline \indent
Za algoritam rekonstrukcije po\v{z}eljno je da zadr\v{z}i mogu\v{c}nost rekonstrukcije ako su neka od mjerenja reskaliraju, ispermutiraju ili dodaju nova. $\ell_1$-minimizacija ima takvo svojstvo. Formalno, gore opisane promijene zapravo predstavljaju zamjenu matrice $\vec A$ matricama $\vec{\hat A}$ i $\vec{\tilde A}$
\begin{align*}
    & \vec{\hat A} := \vec{GA}, \quad \text{gdje je }\vec{G}\text{ neka invertibilna }m \times m \text{ matrica},\\
    & \vec{\tilde A} := 
    \begin{bmatrix*}
        \vec A \\ \vec B
    \end{bmatrix*}
    , \quad \text{gdje je }\vec{B}\text{ neka }m' \times N \text{ matrica}.
\end{align*}
Primjetimo da je $\ker \vec{\hat A} = \ker \vec A$ i $\ker \tilde A \subset \ker \vec A$, pa svojstvo nul-prostora vrijedi i za matrice $\vec{\hat A}$ i $\vec{\tilde A}$.
\newline
\newline
\indent
Za kraj prou\v{c}iti \'cemo utjecaj polja $\K$. Razlika izme\dj u $\ker_{\R}\vec A$ i\\ $\ker_{\C} \vec A = \ker_{\R} \vec A + i \ker_{\R} \vec A$ vodi u slu\v{c}aju da je $\K=\R$ na realno svojstvo nul-prostora, 
\begin{equation}\label{svojstvo_nul_prostora_realno}
    \sum_{j \in S}|v_j| < \sum_{l \in \bar{S}}|v_l| \quad \text{za svaki } \vec v \in \ker_{\R} \vec A,\ \vec v \neq \vec 0,  
\end{equation}
a u slu\v{c}aju da je $\K = \C$, na kompleksno svojsto nul-prostora,
\begin{equation}\label{svojstvo_nul_prostora_kompleksno}
    \sum_{j in S}\sqrt{v_j^2 + w_j^2} < \sum_{l \in \bar{S}}\sqrt{v_j^2 + w_j^2}\quad \text{za svaki } \vec v, \vec w \in \ker_{\R} \vec A,\ \ ( \vec v, \vec w ) \neq \vec (\vec 0,\vec 0).  
\end{equation}

\noindent
Zapravo, pokazati \'cemo da su svojstva nul-prostora me\dj usobno ekvivalentna u realnom i kompleksnom slu\v{c}aju. Zato mo\v{z}emo re\'ci da realna matrica mjerenja egzaktno rekonstruira sve rijetke vektore $\ell_1$-minimizacijom. 
\begin{thm}
    Neka je $\vec A \in \R^{m \times N}$, tada je realno svojstvo nul-prostora \eqref{svojstvo_nul_prostora_realno} za skup $S$ ekvivalentno je kompleksnom svojstvu nul-prostora \eqref{svojstvo_nul_prostora_kompleksno} za isti skup $S$.
\end{thm}
\begin{proof}
    Primjetimo \eqref{svojstvo_nul_prostora_realno} slijedi direktno iz \eqref{svojstvo_nul_prostora_kompleksno} za $\vec w = \vec 0$. Uzmimo sada $\vec v, \vec w \in \ker_{\R}\vec A$, takvi da $(\vec v, \vec w) \neq (\vec 0, \vec 0)$. Ako su $\vec v$ i $\vec w$ linearno zavisni. tj. $\vec v = \alpha \vec w$ za neki $\alpha \in \R \backslash \{ \vec 0 \}$ onda je 
   \begin{align*}
       \sum_{j \in S} \sqrt{v_j^2+w_j^2} &=  \sum_{j \in S} \sqrt{(1+\alpha^2)w_j^2}=\sqrt{1+\alpha^2}\sum_{j \in S} \sqrt{w_j^2}\\
       &< \sqrt{1+\alpha^2}\sum_{j \in \bar S} \sqrt{w_j^2} = \sum_{j \in \bar S} \sqrt{(1+\alpha^2)w_j^2} = \sum_{j \in \bar S} \sqrt{v_j^2+w_j^2}
   \end{align*} 
   Pretpostavimo sada da su $\vec v$ i $\vec w$ linearno nezavisni i definirajmo $\vec u := \cos \theta \vec v + \cos \theta \vec v \in \ker_{\R} \vec A \backslash \{\vec 0\}$. Tada za svaki $\theta \in \R$,
   \begin{equation}\label{svojstvo_nul_prostora_r_c_nejed1}
       \sum_{j \in S} |\cos \theta v_j + \sin \theta w_j| < \sum_{l \in \bar S} |\cos \theta v_l + \sin \theta w_l|.
   \end{equation}
   Za svaki $k \in [N]$, neka je $\theta_k \in [-\pi, \pi]$ takav da
   \begin{equation*}
       v_k = \sqrt{v_k^2 + w_k^2}\cos{\theta_k}, \quad w_k = \sqrt{v_k^2 + w_k^2}\sin{\theta_k}
   \end{equation*}
   Iz \eqref{svojstvo_nul_prostora_r_c_nejed1} slijedi,
   \begin{equation*}
       \sum_{j \in S}\sqrt{v_j^2+w_j^2}|\cos(\theta - \theta_j)|<\sum_{l \in \bar S}\sqrt{v_l^2+w_l^2}|\cos(\theta - \theta_l)|
   \end{equation*}
   Integriranjem po $\theta \in [-\pi,\pi]$ dobijemo
   \begin{equation*}
       \sum_{j \in S}\sqrt{v_j^2+w_j^2}\int_{-\pi}^{\pi}  |\cos(\theta - \theta_j)| d \theta<\sum_{l \in \bar S}\sqrt{v_l^2+w_l^2}\int_{-\pi}^{\pi}  |\cos(\theta - \theta_l)|
   \end{equation*}
    No lako se provjeri da je
    \begin{equation*}
         \int_{-\pi}^{\pi}  |\cos(\theta - \theta_j)| d \theta = 4
    \end{equation*}
    tj. da je pozitivan i neovisan o $\theta' \in [-\pi, \pi]$.
\end{proof}

\subsection[Nekonveksna minimizacija][Nekonveksna minimizacija]{Nekonveksna minimizacija}
Prisjetimo se, $\ell_0$ norma vektora $\vec z \in \C^N$ aproksimirana je $q$-tom potencijom svoje $\ell_q$-kvazinorme,
\begin{equation*}
\|\vec{z}\|_p^p := \sum_{j=1}^N|x_j|^p \xrightarrow{p\rightarrow 0} \sum_{j=1}^N\mathbf{1}_{\{z_j \neq 0\}} = \|\vec{z}\|_0
\end{equation*}
To sugestira da $\ell_0$-minimizaciju \eqref{problem_minimizacije} zamjenimo sa
\begin{equation}
    \min_{\vec z \in \C^N} \norm{\vec z}_q \quad \text{uz uvjet }\vec{Az} = \vec{y}.\tag{$P_q$}
\end{equation}
Za $0 < q < 1$ taj je problem nekonveksan i $\mathfrak{NP}$-te\v{z}ak. No, \v{z}elimo teoretski potvrditi ideju da \eqref{problem_minimizacije_aprox} dobro aproksimira \eqref{problem_minimizacije} za male $q$.
Sljede\'ci teorem daje analogon svojstva nul-prostora za $0<q<1$. Dokaz je tako\dj er analogan dokazu teorema \ref{svojstvo_nul_prostora_tm} te se koristi \v{c}injenica da za $\ell_q$-kvazinorma zadovoljava nejednakost trokuta.  
\begin{thm}\label{svojstvo_nul_prostora_tm_2}
    Za matricu $\vec A \in \C^{m \times N}$ i $0<q<1$, svaki $s$-rijedak vektor $\vec x \in \C^N$ je jedinstveno rje\v{s}enje problema \eqref{problem_minimizacije_aprox} uz $\vec y = \vec{Ax}$ ako i samo ako 
    \begin{equation*}
        \norm{\vec v_S}_q < \norm{\vec v_{\bar{S}}}_q  \quad \text{za svaki }\vec v \in \ker \vec A \backslash \{\vec 0\}.
    \end{equation*}
\end{thm}
Sada mo\v{z}emo dokazivati da rekonstrukcija $\ell_q$-minimizacijom implicira rekonstrukciju $\ell_p$-minimizacijom za $o<p<q<1$.
\begin{thm}
    Za matricu $\vec A \in \C^{m \times N}$ i $0<p<q<1$, ako je svaki $s$-rijedak vektor $\vec x \in \C^N$ jedinstveno rje\v{s}enje problema \eqref{problem_minimizacije_aprox} uz $\vec y = \vec{Ax}$  onda je $\vec x$ tako\dj er i rje\v{s}enje problema $(P_p)$ za $\vec y = \vec{Ax}$.
\end{thm}
\begin{proof}
    Prema teoremu \ref{svojstvo_nul_prostora_tm_2} dovoljno je pokazati da vrijedi
    \begin{equation}\label{svojstvo_nul_prostora_tm_3_nejed}
        \sum_{j \in S} |v_j|^p < \sum_{l \in \bar S}|v_l|^p,
    \end{equation}
    ako je $\vec v \in \ker \vec A \backslash \{\vec 0\}$, $S$ skup indeksa od $s$ apsolutno najve\'cih komponeneti od $\vec v$ i ako ista nejednakost vrijedi za $q$.
    Dakle, pretpostavimo da \eqref{svojstvo_nul_prostora_tm_3_nejed} vrijedi za q. Tada je nu\v{z}no $\vec v_{\bar S} \neq \vec 0$ po\v{s}to je $S$ skup indeksa od $s$ apsolutno najve\'cih komponeneti ne-nul vektora $\vec v$. Stoga \eqref{svojstvo_nul_prostora_tm_3_nejed} mo\v{z}emo napisati u obliku
    \begin{equation}\label{svojstvo_nul_prostora_tm_3_nejed_2}
        \sum_{j \in S} \frac{1}{\sum_{l \in \bar S}(|v_l|/|v_j|)^p} < 1.  
    \end{equation}
    Primjetimo da $|v_l|/|v_j| \leq 1$ za $l \in \bar S$ i $j \in S$. Stoga je lijeva strana \eqref{svojstvo_nul_prostora_tm_3_nejed_2} nepadaju\'ca funkcija u varijabli $0<p \leq 1$. Pa stoga njena vrijednost u $p < q$ ne prelazi njezinu vrijednost u $q$, koji je manji od 1 po pretpostavci.
\end{proof}

\section[Stabilnost][Stabilnost]{Stabilnost}
Signali u praksi gotovo nikad nisu idealno rijetki. U najboljem slu\'caju blizu su rijetkim vektorima. Stoga, \v{z}elimo da metode sa\v{z}etog uzorkovanja rekonstruiraju vektor $\vec x \in \C^N$ sa gre\v{s}kom koja je kontrolirana udaljenosti vektora $\vec x$ do $s$-rijetkih vektora. Za algoritme koji imaju to svojsto ka\v{z}emo da su \textit{stabilni} s obzirom na defekte rijetkosti. Pokazati \'cemo da je $\ell_1$-minimizacija \eqref{problem_minimizacije_l1} stabilna pod ja\v{c}im svojstvom nul-prostora.

\begin{defn}
    Matrica $\vec A \in \C^{m \times N}$ zadovoljava \textit{stabilno svojstvo nul-prostora} sa konstantom $0<\rho<1$ za skup $S \subset [N]$ ako
    \begin{equation*}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 \quad \text{za svaki }\vec v \in \ker \vec A.
    \end{equation*}
    Nadalje, ka\v{z}emo da $\vec A$ zadovoljava \textit{stabilno svojstvo nul-prostora reda} $s$ sa konstantom $0<\rho<1$ ako zadovoljava zadovoljava gornju nejednakost za svaki $S \subset [N]$ takav da $\card(S)=s$.
\end{defn}

\begin{thm}\label{stabilnost_tm_1}
    Ako matrica $\vec A \in \C^{m \times N}$ zadovoljava stabilno svojstvo nul-prostora reda $s$ sa konstantom $0<\rho<1$, tada za svaki $\vec x \in \C^N$, rje\v{s}enje $\vec x^{\sharp}$ problema \eqref{problem_minimizacije_l1} sa $\vec y = \vec{Ax}$ aproksimira vektor $\vec x$ s $\ell_1$-gre\v{s}kom
    \begin{equation}\label{stabilnost_tm_1_nejed}
        \norm{\vec x - \vec x ^{\sharp}} \leq \frac{2(1+\rho)}{(1-\rho)}\sigma_s(\vec x)_1.
    \end{equation}
\end{thm}
\noindent
Sada vi\v{s}e nemamo jedinstvenost $\ell_1$-minimizatora. Prethodni teorem biti \'ce direktna posljedica ja\v{c}e tvrdnje,
\begin{thm}\label{stabilnost_tm_2}
    Ako matrica $\vec A \in \C^{m \times N}$ zadovoljava stabilno svojstvo nul-prostora sa konstantom $0<\rho<1$ za skup $S$ ako i samo ako
    \begin{equation}\label{stabilnost_tm_2_nejed}
        \norm{\vec z - \vec x}_1  \leq \frac{1+\rho}{1-\rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1) 
    \end{equation}
    za sve vektore $\vec x, \vec z \in \C^N$ za $\vec{Az} = \vec{Ax}$.
\end{thm}

Poka\v{z}imo kako teorem \ref{stabilnost_tm_1} slijedi iz \ref{stabilnost_tm_2}:
Neka je $S$ skup $s$ apsolutno najve\'cih komponeneti vekotora $\vec x$, tako da $\norm{\vec x_{\bar S}}=\sigma_s(\vec x)_1$. Ako je $\vec x^{\sharp}$ minimizator problema \eqref{problem_minimizacije_l1}, tada vrijedi $\norm{\vec x^{\sharp}}_1 \leq \norm{\vec x}_1$ i $\vec{Ax}^{\sharp}= \vec{Ax}$. Dakle, desnu strana \eqref{stabilnost_tm_2_nejed} za $\vec z = \vec x^{\sharp}$ mo\v{z}emo ocjeniti desnom stranom \eqref{stabilnost_tm_1_nejed}.\\
\indent
Prije dokaza teorema \ref{stabilnost_tm_2} poka\v{z}imo jo\v{s} jedan koristan rezultat.
\begin{lem}\label{stabilnost_lema_1}
    Za $S \subset [N]$ i vektore $\vec x, \vec z \in \C^N$ vrijedi,
    \begin{equation*}
        \norm{(\vec x - \vec z)_{\bar S}}_1 \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \norm{(\vec x - \vec z)_S}_1 + 2 \norm{\vec x_{\bar S}}_1
    \end{equation*}
\end{lem}
\begin{proof}
    Imamo,
    \begin{align*}
        \norm{\vec x}_1 &= \norm{\vec x_{\bar S}}_1 + \norm{\vec{x}_S}_1 \leq \norm{\vec x_{\bar S}}_1 + \norm{(\vec x - \vec z)_S}_1 + \norm{\vec z_{S}}_1\\
        \norm{(\vec x - \vec z)_{\bar S}}_1 &\leq \norm{\vec x_{\bar S}}_1 + \norm{\vec z_{\bar S}}_1.
    \end{align*}
    Sumiranjem ove dvije nejednakosti, slijedi
    \begin{equation*}
        \norm{\vec x}_1 + \norm{(\vec x - \vec z)_{\bar S}}_1 \leq 2 \norm{\vec x_{\bar S}}_1 +  \norm{(\vec x - \vec z)_S}_1 + \norm{\vec z}_1.
    \end{equation*}
\end{proof}

\begin{proof}[Dokaz (Teorem \ref{stabilnost_tm_2})]
    Pretpostavimo da matrica $\vec A$ zadovoljava \eqref{stabilnost_tm_2_nejed} za sve vektore $\vec x, \vec z \in \C^N$ uz $\vec{Az} = \vec{Ax}$. Za dani vektor $\vec{v} \in \ker \vec A$, po\v{s}to je $\vec{Av}_{\bar S} = \vec A(-\vec v_S)$ mo\v{z}emo primjeniti \eqref{stabilnost_tm_2_nejed} sa $\vec x = - \vec v_S$ i $\vec z = \vec v_{\bar S}$. Slijedi,
    \begin{equation*}
        \norm{\vec v}_1 \leq \frac{1+\rho}{1-\rho}(\norm{\vec v_{\bar S}}_1 - \norm{\vec v_S}_1). 
    \end{equation*}
    To mo\v{z}emo zapisati kao 
    \begin{equation*}
        (1-\rho)(\norm{\vec v_S}_1 + \norm{\vec v_{\bar S}}_1)  \leq  (1+\rho)(\norm{\vec v_{\bar S}}_1 + \norm{\vec v_S}_1).
    \end{equation*}
    Jednostavnom manipulacijom slijedi
    \begin{equation*}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 
    \end{equation*}
    \indent
    Obratno, neka matrica $\vec A$ zadovoljava stabilno svojstvo nul-prostora s konstantom $0<\rho<1$ za skup $S$. Neka su $\vec x, \vec z \in \C^N$ takvi da $\vec{Az} = \vec{Ax}$, po\v{s}to je $\vec v := \vec z - \vec x \in \ker \vec A$, stabilno svojstvo nul-prostora daje
    \begin{equation}\label{stabilnost_tm_3_nejed_1}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1.
    \end{equation}
    Nadalje, iz lema \ref{stabilnost_lema_1} slijedi
    \begin{equation}\label{stabilnost_tm_3_nejed_2}
        \norm{\vec v_{\bar S}}_1  \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \norm{\vec v_S}_1 + 2 \norm{\vec x_{\bar S}}_1.
    \end{equation}
    Substituiramo \eqref{stabilnost_tm_3_nejed_1} u \eqref{stabilnost_tm_3_nejed_2},
    \begin{equation*}
        \norm{\vec v_{\bar S}}_1  \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \rho\norm{\vec v_{\bar S}}_1 + 2 \norm{\vec x_{\bar S}}_1.
    \end{equation*}
    Po\v{s}to je $\rho < 1$,
    \begin{equation*}
        \norm{\vec v_{\bar S}}_1 \leq \frac{1}{1 - \rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2\norm{\vec x_{\bar S}}_1).  
    \end{equation*}
    Ponovno iskoristimo \eqref{stabilnost_tm_3_nejed_1},
    \begin{equation*}
        \norm{\vec v}_1 = \norm{\vec v_{\bar S}}_1 +  \norm{\vec v_{S}}_1  \leq (1 + \rho) \norm{\vec v_{\bar S}}_1 \leq \frac{1+\rho}{1-\rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1).
    \end{equation*}
\end{proof}

\section[Robusnost][Robusnost]{Robusnost}
Jasno je da u realnosti signal nikad ne mo\v{z}emo mjeriti sa beskona\v{c}nom to\v{c}no\v{s}\'cu. U na\v{s}em kontekstu to zna\v{c}i da je vektor mjerenja $\vec y \in \C^m$ aproksimacija vektora $\vec{Ax} \in \C^m$, tj. formalno
\begin{equation*}
    \norm{\vec{Ax} - \vec{y}} \leq \eta
\end{equation*}
za neki $\eta \leq 0$ i neku normu na $\C^m$. Od metode rekonstrukcije tra\v{z}imo da udaljenost rekonstruiranog vektora $\vec x^{\sharp}$ i orginalnog vektora $\vec x$ bude kontrolirana preciznosti mjerenja $\eta$. Ako metoda zadovoljava to svojstvo ka\v{z}emo da je \textit{robusna} ili \textit{otporna} na gre\v{s}ke mjerenja. Pokazati \'cemo da BP algoritam ($\ell_1$-minimizacija) robusna ako \eqref{problem_minimizacije_l1} zamjenimo konveksni problemom
\begin{equation}
    \min_{\vec z in \C^N} \norm{\vec z}_1 \quad \text{uz uvjet } \norm{\vec{Az} - \vec y} \leq \eta \tag{P_{1, \eta}}
\end{equation}
te ako vrijedi sljede\v{c}a ja\v{c}a varijanta svojstva nul-prostora.
\begin{defn}
    Za matricu $\vec A \in \C^{m \times N}$ ka\v{z}emo da zadovoljava \textit{robusno svojstvo nul-prostora} s konstantama $0<\rho<1$ i $\tau > 0$ za skup $S \subset [N]$ ako 
    \begin{equation}\label{robusnost_defn_nejed}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 + \tau \norm{\vec{Av}} \quad \text{za sve } \vec v \in \C^N.
    \end{equation}
    Nadalje, ka\v{z}emo da $\vec A$ zadovoljava robusno svojstvo nul-prostora s konstantama $0<\rho<1$ i $\tau > 0$ reda $s$ ako zadovoljava gornje svojstvo za svaki $S \subset [N]$ takav da $\card(S) \leq s$.
\end{defn}

\noindent
Primjetimo da definicija ne tra\v{z}i da je $\vec v \in \ker \vec A$. Kada bi to vrijedilo propao bi \v{c}lan $\norm{\vec{Ax}}$ i time bi dobili stabilno svojstvo nul-prostora. 
\begin{thm}
    Neka matrica $\vec A \in \C^{m \times N}$ zadovoljava robusno svojstvo nul-prostora reda $s$ sa konstantama $0<\rho<1$ i $\tau > 0$. Tada za svaki vektor $\vec x \in \C^N$, rje\v{s}enje problema \eqref{problem_minimizacije_l1_kvadraticni} za $\vec y = \vec{Ax}+\vec{e}$ i $\norm{\vec e} \leq \eta$ aproksimira vektor $\vec x$ sa gre\v{s}kom
    \begin{equation*}
        \norm{\vec x - \vec x^{\sharp}}_1 \leq \frac{2(1+\rho)}{(1-\rho)} \sigma_s(\vec x)_1 + \frac{4 \tau}{1-\rho}\eta 
    \end{equation*}
\end{thm}
Dokazati \'cemo ja\c{u} tvrdnju,
\begin{thm}\label{tm:4:20}
    Matrica $\vec A \in \C^{m \times N}$ zadovoljava robusno svojstvo nul-prostora sa konstantama $0<\rho<1$ i $\tau > 0$ za skup $S$ ako i samo ako
    \begin{equation}\label{robusno_tm2_nejed}
        \norm{\vec z - \vec x}_1 \leq \frac{1+\rho}{1-\rho} (\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1) + \frac{2 \tau}{1 - \rho} \norm{\vec A (\vec z - \vec x)}  
    \end{equation}
    za sve vektore $\vec x, \vec z \in \C^N$.
\end{thm}
\begin{proof}
    Pretpostavimo da $\vec A$ zadovoljava \eqref{robusno_tm2_nejed}. Za $\vec v \in \C^N$, uzmimo $\vec x = - \vec v_S$ i $\vec z = \vec v_{\bar S}$. Slijedi,
    \begin{equation*}
        \norm{\vec v}_1 \leq \frac{1+\rho}{1- \rho}(\norm{\vec v_{\bar S}}_1 - \norm{\vec v_S}_1) + \frac{2 \tau}{1 - \rho} \norm{\vec{Av}}.
    \end{equation*}
    Preslagivanjem \v{c}lanova dobivamo,
    \begin{equation*}
        (1-\rho)(\norm{\vec v_S}_1 + \norm{\vec v_{\bar S}}_1) \leq (1 + \rho)(\norm{\vec v_{\bar S}}_1 - \norm{\vec v_S}_1) + 2 \tau \norm{Av}\
    \end{equation*}
    tj. imamo
    \begin{equation*}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 + \tau \norm{\vec{Av}}.
    \end{equation*} 

    \indent Obratno, neka $\vec A$ zadovoljava robusno svojstvo nul-prostora sa konstantama $0<\rho<1$ i $\tau > 0$ za skup $S$. Za $\vec x, \vec z \in \C^N$, neka je $\vec v := \vec z - \vec x$. Iz robusnog svojstvo nul-prostora i leme \ref{stabilnost_lema_1} slijedi,
   \begin{align*}
       &\norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 + \tau \norm{\vec{Av}},\\
       &\norm{\vec v_{\bar S}}_1 \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \norm{\vec v_S}_1 + 2 \norm{\vec x_{\bar S}}_1.
   \end{align*} 
   Kombiniranjem te dvije nejednakosti slijedi,
   \begin{equation*}
       \norm{\vec v_{\bar S}}_1 \leq \frac{1}{1-\rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1 + \tau \norm{Av}). 
   \end{equation*}
   Ponovno iskoristimo robusno svojstvo nul-prostoram
   \begin{align*}
       \norm{\vec v}_1 &= \norm{\vec v_{\bar S}}_1 + \norm{\vec v_S}_1 \leq (1 + \rho)\norm{\vec v_{\bar S}}_1 + \tau \norm{\vec{Av}}\\
       & \leq \frac{1+\rho}{1-\rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1) + \frac{2 \tau}{1-\rho}  \norm{\vec{Av}}
   \end{align*}
\end{proof}

Sada \'cemo pobolj\v{s}ati prethodni rezultat robusnosti, dati \'cemo $\ell_p$ ocjenu gre\v{s}ke za $p \geq 1$. Za to potrebna nam je jo\v{s} jedna varijantna svojstva nul-prostora,
\begin{defn}
    Za $q \geq 1$, matrica $\vec A \in \C^{m \times N}$ zadovoljava $\ell_q$-robusno svojstvo nul-prostora reda $s$ sa konstantama $0 < \rho < 1$ i $\tau > 0$, ako za svaki $S \subset [N]$, takav da $\card(S) \leq s$,
    \begin{equation*}
        \norm{\vec v_S}_q \leq \frac{\rho}{s^{1-1/q}}\norm{\vec v_{\bar S}}_1 + \tau \norm{\vec{Av}}\quad \text{za svaki }\vec{v} \in \C^N.
    \end{equation*}
\end{defn}
\noindent Iz $\norm{\vec v_S}_p \leq s^{1/p - 1/q}\norm{\vec v_S}_q$ za $1 \leq p \leq q$, $\ell_1$-robusno svojstvo nul-prostora implicira
\begin{equation*}
    \norm{\vec v_S}_p \leq \frac{\rho}{s^{1-1/p}}\norm{\vec v_{\bar S}}_1 + \tau s^{1/p-1/q}\norm{\vec{Av}} \quad \text{za sve } \vec v \in \C^N.
\end{equation*}
Stoga, za $1 \leq p \leq q$, $\ell_q$-robusno svojstvo nul-prostora implicira $\ell_p$-robusno svojstvo nul-prostora s jednakim konstanama, do na promjenu norme. Sljede\'ci teorem daje robusnost kvadrati\v{c}no ograni\v{c}ene $\ell_1$-minimizacije.
\begin{thm}\label{robusnost_l1_min_kvad_ogr}
    Neka matrica $\vec A \in \C^{m \times N}$  zadovoljava $\ell_2$-robusno svojstvo nul-prostora reda $s$ sa konstanama $0<\rho<1$ i $\tau >0$. Tada za svaki $\vec x \in \C^N$, rje\v{s}enje $\vec x^{\sharp}$ problema \eqref{problem_minimizacije_l1_kvadraticni} aproksimira $\vec x$ s $\ell_p$-gre\v{s}kom
    \begin{equation}
        \norm{\vec x - \vec x^{\sharp}}_p \leq \frac{C}{s^{1-1/p}} \sigma_s(\vec x)_1 + D s^{1/p - 1/2} \eta, \quad 1 \leq p \leq 2,
    \end{equation}
    za neke konstane $C,D > 0$ koje ovise samo o $\rho$ i $\tau$.
\end{thm}
Ovaj teorem je direktna posljedica narednog op\v{c}enitijeg teorema za $q = 2$ i $\vec z = \vec x^{\sharp}$.
\begin{thm}
    Neka je $1 \leq p \leq q$ i neka matrica $\vec A \in \C^{m \times N}$ zadovoljava $\ell_q$-robusno svojstvo nul-prostora reda $s$ sa konstantama $0 < \rho < 1$ i $\tau > 0$. Tada za svaki $\vec x, \vec z \in \C^N$,
    \begin{equation*}
        \norm{\vec z - \vec x}_p \leq \frac{C}{s^{1-1/p}}(\norm{z}_1 - \norm{\vec x}_1 + 2 \sigma_s(\vec x)_1) + D s^{1/p-1/q} \norm{\vec A (\vec z - \vec x)},
    \end{equation*}
    gdje su $C:=(1+\rho)^2/(1-\rho)$ i $D:=(3+\rho)\tau/(1-\rho)$.
\end{thm}
\begin{proof}
    Iskoristimo prvo da $\ell_q$-robusno svojstvo nul-prostora implicira $\ell_1$-robusno i $\ell_p$-robusno svojstvo nul-prostora, tj.
    \begin{equation}\label{4:18}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 + \tau s^{1-1/q} \norm{\vec{Av}},
    \end{equation}
    \begin{equation}\label{4:19}
        \norm{\vec v_S}_1 \leq \frac{\rho}{s^{1-1/p}} \norm{\vec v_{\bar S}}_1 + \tau s^{1/p - 1/q} \norm{\vec{Av}},
    \end{equation}
    za svaki $\vec v \in \C^N$ i za sve $S \subset [N]$, takve da $\card(S) \leq s$. Uva\v{z}avaju\'ci \eqref{4:19} i primjenom teorema \ref{tm:4:20} s skupom $S$ koji je jednak skupu $s$ apsolutno najve\'cih komponenti vektora $\vec x$, imamo
    \begin{equation}\label{4:20}
        \norm{\vec z - \vec x}_1 \leq \frac{1+\rho}{1-\rho}(\norm{\vec z }_1 - \norm{\vec x}_1 + 2 \sigma_s(\vec x)_1)+ \frac{2 \tau}{1 - \rho}s^{1-1/q} \norm{\vec A (\vec z - \vec x)}.
    \end{equation}
    Nadalje, odabirom skupa S kao skupa $s$ apsolutno najve\v{c}ih komponenti vektora $\vec z - \vec x$, iz teorema \ref{tm:2:5} slijedi
    \begin{equation*}
        \norm{\vec z - \vec x}_p \leq \norm{(\vec z - \vec x)_{\bar S}}_p + \norm{(\vec z - \vec x)_S}_p \leq \frac{1}{s^{1-1/p}}\norm{\vec z - \vec x}_1 + \norm{(\vec z - \vec x)_S}_p.
    \end{equation*}
    Iz \eqref{4:19} imamo,
    \begin{align}\label{4:21}
        \norm{\vec z - \vec x}_p &\leq \frac{1}{s^{1-1/p}} \norm{\vec z - \vec x}_1 + \frac{2}{s^{1-1/p}} \norm{(\vec z - \vec x)_{\bar S}}_1 + \tau s^{1/p - 1/q} \norm{\vec A (\vec z - \vec x)}\nonumber \\
        &\leq  \frac{1+\rho}{s^{1-1/p}}  \norm{\vec z - \vec x}_1 + \tau s^{1/p - 1/q} \norm{ \vec A (\vec z - \vec x)}.
    \end{align}
    Preostaje \eqref{4:20} u \eqref{4:21}.
\end{proof}

\section[Rekonstrukcija predodre\dj enog vektora][Rekonstrukcija predodre\dj enog vektora]{Rekonstrukcija predodre\dj enog vektora}
Ukoliko \v{z}elimo rekonstruirati predore\dj eni rijetki vektor $\vec x$ umjesto sve rijetke vektore s nosa\v{c}emo u nekom skupu $S$, potrebno nam je finije svojstvo rekonstrukcije od svojstva nul-prostora. Naglasimo da se \'ce ovdje biti sitna razlika izme\dj u realnog i kompleksnog slu\v{c}aja, \v{s}to je posljedica definija predznaka broja $z$,
\begin{equation*}
    \sgn(z):= 
    \begin{cases}
        \frac{z}{|z|} \quad &\text{ako } z \neq 0,\\
        0 &\text{ako } z = 0
    \end{cases}
\end{equation*}
i \v{c}injenice da je u realnom slu\v{c}aju to diskretna vrijednost, dok u kompleksnom nije. Za vektor $\vec x \in C^N$, $\sgn(\vec x)\in \C^N$ definiramo kao vektor s komponentama $\sgn(x_),\ j \in [N]$.
\begin{thm}
    Za danu matricu $\vec A \in \C^{m \times N}$, vektor $\vec x \in \C^N$ sa nosa\v{c}em $S$ je jedinstveni minimizator od $\norm{\vec z}_1$ uz uvjet $\vec{Az} = \vec{Ax}$ ako je jedna od narednih, ekvivalentnih tvrdnji zadovoljena:
    \begin{enumerate}[label=(\alph*)]
        \item $|\sum_{j \in S} \overline{\sgn(x_j)}v_j| < \norm{\vec v_{\bar S}}$ za sve $\vec v \in \ker \vec A \backslash \{ \vec 0 \}$, 
            \newpage
        \item $\vec A_S$ je injektivna i postoji vektor $\vec h \in \C^m$ takav da
            \begin{multicols}{2}
                \noindent
                \begin{equation*}
                    (\vec A^* \vec h)_j = \sgn(x_j),\ j \in S,
                \end{equation*}
                \begin{equation*}
                    |(\vec A^* \vec h)_l| < 1,\ l \in \bar S.
                \end{equation*}
            \end{multicols}
    \end{enumerate}
\end{thm}
\begin{proof}
    Doka\v{z}imo prvo da $(a)$ implicira da je $\vec x$ jedinstveni minimizator od $\norm{\vec z}_1$ takav da $\vec{Az} = \vec{Ax}$. Za $\vec z \neq \vec x$ takav da $\vec{Az} = \vec {Ax}$ uzmimo $\vec v := \vec x - \vec z \in \ker \vec A \backslash \{\vec 0\}$
    \begin{align*}
        \norm{\vec z}_1 &= \norm{\vec z_S}_1 + \norm{\vec z_{\bar S} }_1 = \norm{(\vec x - \vec z)_S}_1 + \norm{\vec v_{\bar S}}_1  \\
        &> |\langle \vec x - \vec z, \sgn(\vec x)_S \rangle | + |\langle \vec v, \sgn(\vec x)_S \rangle| \geq |\langle \vec x, \sgn(\vec x)_S  \rangle| = \norm{\vec x}_1.
    \end{align*}
    \indent Poka\v{z}imo sada $(b) \implies (a)$. Koriste\'ci \v{c}injenicu da $\vec{Av}_S = - \vec {Av}_{\bar S}$ za $\vec v \in \ker \vec A \backslash \{\vec 0\}$ slijedi,
    \begin{align*}
        |\sum_{j \in S} \overline{\sgn(x_j)v_j}| &= |\langle \vec v_S, \vec A^* \vec h \rangle|  = |\langle \vec{Av}_S, \vec h \rangle| = |\langle \vec{Av}_{\bar S}, \vec h\rangle| \\ 
        &= |\langle \vec v_{\bar S}, \vec{A}^*\vec h \rangle| \leq \max_{l \in \bar S} |(\vec A^* \vec h)_l| \norm{\vec v_{\bar S}}_1 < \norm{\vec v_{\bar S}}_1.
    \end{align*}
    Striktna nejednakost vrijedi jer $\norm{\vec v_{\bar S}} > 0$. U suprotnom bi ne-nul vektor $\vec v \in \ker \vec A$ imao nosa\v{c} u $S$, \v{s}to je kontradikcija s injektivnosti od $\vec A_S$.\\
    \indent Preostaje pokazati $(a) \implies (b)$. Primjetimo da $(a)$ povla\v{c}i $\norm{\vec v_{\bar S}}_1 > 0$ za sve $\vec v \in \ker \vec A \backslash \{\vec 0\}$. Poka\v{z}imo da je $\vec A_S$ injektivna. Pretpostavimo $\vec A_S \vec v_S = \vec 0$ za neki $\vec v_S \neq \vec 0$. Nadopunimo $\vec v_S$ do vektora $\vec v \in \C^N$ tako da stavimo $\vec v_{\bar S} = \vec 0$. Tada je $\vec v \in \ker \vec A \backslash \{\vec 0\}$, \v{s}to je kontradikcija s $\norm{\vec v_{\bar S}}_1 > 0$ za svaki $\vec v \in \ker \vec A \backslash \{\vec 0\}$. Nadalje, primjetimo da je funkcija $\vec v \mapsto |\langle \vec v, \sgn(\vec x)_S \rangle|/ \norm{\vec v_{\bar S}}_1$ neprekidna i da poprima vrijednosti manje od jedan na jedini\v{c}noj kugli u $\ker A$, koja je kompaktan skup. Dakle maksimum $\eta$ zadovoljava $\eta < 1$ i vrijedi 
    \begin{equation*}
        |\langle \vec v, \sgn(\vec x)_S \rangle|  \leq \norm{\vec v_{\bar S}}_1 \quad \text{za sve } \vec v \in \ker \vec A.
    \end{equation*}
    Za $\eta < \nu < 1$ definiramo konveksni skup $\mathcal{C}$ i afin skup $\mathcal{D}$,
    \begin{align*}
        \mathcal{C} &:= \{ \vec z \in \C^N : \norm{\vec z_S}_1 + \nu \norm{\vec z_{\bar S}}_1 \leq \norm{\vec x}_1 \},\\
        \mathcal{D} &:= \{ \vec z \in \C^N : \vec{Az} = \vec{Ax} \}.
    \end{align*}
    Poka\v{z}imo da je $\mathcal{C} \cap \mathcal{D} = \{\vec x\}$. Uzmimo $\vec x \in \mathcal{C} \cap \mathcal{D}$. Za $ \vec z \neq \vec x \in \ker \vec A \backslash \{\vec 0\}$ kontradikcija slijedi iz
    \begin{align*}
        \norm{\vec x}_1 &\geq \norm{\vec z_S}_1 + \nu \norm{\vec z_{\bar S}}_1 = \norm{(\vec x - \vec z)_S}_1 + \nu \norm{\vec v_{\bar S}}_1\\
        &> \norm{(\vec x - \vec v)_S}_1 + \mu \norm{\vec v_{\bar S}}_1 \geq |\langle \vec x - \vec v, \sgn(\vec x)_S \rangle| + |\langle \vec v, \sgn(\vec x)_S \rangle|\\
        &\geq |\langle \vec x, \sgn(\vec x)_S \rangle| = \norm{\vec x}_1.
    \end{align*}
    Dakle, prema teoremu o separaciji konveksnih skupova hiperplohama (vidi TODO), postoji vektor $\vec w \in \C^N$ takav da
    \begin{align}
        \mathcal{C} \subset \{ \vec z \in \C^N: \Re \langle \vec z, \vec w \rangle \leq \norm{\vec x}_1 \}\label{4:22},\\
        \mathcal{D} \subset \{ \vec z \in \C^N: \Re \langle \vec z, \vec w \rangle = \norm{\vec x}_1 \}\label{4:23}.
    \end{align}
    Iz \eqref{4:22} slijedi, 
    \begin{align*}
        \norm{\vec x}_1 &\geq \max_{\norm{\vec z_S + \nu \vec z_{\bar S}}_1 \leq \norm{\vec x}_1} \Re \langle \vec z, \vec x\rangle \\
        &=\max_{\norm{\vec z_S + \nu \vec z_{\bar S}}_1 \leq \norm{\vec x}_1} \Re \bigg( \sum_{j \in S} z_j \overline{w_j} + \sum_{j \in \bar S} \nu z_j \overline{w_j}/ \nu \bigg)\\
        &= \max_{\norm{\vec z_S + \nu \vec z_{\bar S}}_1 \leq \norm{\vec x}_1} \Re \langle \vec z_S + \nu \vec z_{\bar S}, \vec w_{\bar S} + (1/\nu) \vec w_{\bar S} \rangle \\
        &= \norm{\vec x}_1 \norm{\vec w_S + (1/\nu)\vec w_{\bar S}}_{\infty} = \norm{\vec x}_1 \max\{ \norm{\vec w_S}_{\infty}, (1/\nu) \norm{\vec w_{\bar S}}_{\infty} \}.
    \end{align*}
    U slu\v{c}aju $\vec x = \vec 0$, dovoljno je uzeti vektor $\vec h = \vec 0$, stoga neka je $\vec x \neq \vec 0$. Gornja nejednakost daje $\norm{\vec w_S}_{\infty} \leq 1$ i $\norm{\vec w_{\bar S}}_{infty} \leq \nu < 1$. Iz \eqref{4:23} slijedi $\Re \langle \vec x, \vec w\rangle = \norm{\vec x}_1$, tj. $w_j = \sgn(x_j)$ za sve $j \in S$, te $\Re \langle \vec v, \vec w \rangle = 0$ za sve $\vec v \in \ker \vec A$, tj. $\vec w \in (\ker \vec A)^{\perp}$. Po\v{s}to je $(\ker \vec A)^{\perp} = \im \vec A^*$, imamo $\vec w = \vec A^* \vec h$ za neki $\vec h \in \C^m$.
\end{proof}
U realnom slu\v{c}aju obratna tvrdnja tako\dj er vrijedi, dok op\v{c}enito to nije istina. Dati \'cemo jo\v{s} jednu karakteriziciju egzaktne rekonstrukcije $\ell_1$-minimizacijom u realnom slu\v{c}aju. Za vektor $\vec x \in \R^N$, \textit{konveksni konus} definiramo kao
\begin{equation}\label{4:34}
    T(\vec x) = \cone \{ \vec z - \vec x : \vec z \in \R^N,\ \norm{\vec z}_1 \leq \norm{\vec x}\} 
\end{equation}
gdje $\cone$ predstavlja konusnu ljusku (vidi TODO).
\begin{thm}
    Za matricu $\vec A \in \R^{m \times N}$, vektor $\vec x \in \R^N$ je jedinstveni minimizator od $\norm{\vec z}_1$ takav da $\vec{Az} = \vec{Ax}$ ako i samo ako $\ker \vec A \cap T(\vec x) = \{\vec 0\}$.
\end{thm}

\begin{proof}
    Pretpostavimo da je $\ker \vec A \cap T(x) = \{\vec 0\}$. Neka je $\vec x^{\sharp}$ $\ell_1$-minimizator. Imamo, $\norm{\vec x^{\sharp}}_1 \leq \norm{\vec x}_1$ i $\vec A \vec x^{\sharp} = \vec{Ax}$, pa je $\vec v := \vec x^{\sharp} - \vec x \in T(\vec x) \cap \ker \vec A = \{\vec 0\}$. Stoga je $\vec x^{\sharp} = \vec x$. Dakle, $\vec x$ je jedinstveni $\ell_1$-minimizator.\\
    \indent
    Obratno, neka je $\vec x$ jedinstveni $\ell_1$-minimizator. Vektor $\vec v \in T(\vec x) \backslash \{\vec 0\}$ mo\v{z}emo zapisati kao $\vec v = \sum t_j (\vec z_j - \vec x)$ gdje je $t_j \geq 0$ i $\norm{\vec z_j} \leq \norm{\vec x}_1$. Da je $\vec v \in \ker \vec A$, vrijedilo bi $\vec A (\sum t_j^{'} \vec z_j) = \vec{Ax}$ i $\norm{\sum t_j^{'} \vec z_j}_1 \leq \sum t_j^{'} \norm{\vec z_j}_1 \leq \norm{\vec x}_1$. Zbog jedinstvenosti, to bi zna\v{c}ilo da $\sum t_j^{'} \vec z_j = \vec x$ pa bi $\vec v = \vec 0$, \v{s}to je kontradikcija. Dakle, vrijedi $(T(\vec x) \backslash \{\vec 0\}) \cap \ker \vec A = \emptyset$. 
\end{proof}
Ovaj rezultat mo\v{z}emo pro\v{s}iriti i na robusnu rekonstrukciju,
\begin{thm}
    Za $\vec A \in \R^{m \times N}$, neka je $\vec x \in \R^N$ i $\vec y = \vec{Ax} + \vec e \in \R^m$ i $\norm{\vec e}_2 \leq \eta$. Ako je
    \begin{equation*}
        \inf_{\vec v \in T(x),\ \norm{\vec v}_2 = 1} \norm{\vec{Av}}_2 \geq \tau
    \end{equation*}
    za neki $\tau > 0$, tada minimizator $\vec x^{\sharp}$ od $\norm{\vec z}_1$ takav da $\norm{\vec{Az} - \vec y}_2 \leq \eta$ zadovoljava
    \begin{equation}\label{4:35}
        \norm{\vec x - \vec x^{\sharp}}_2 \leq \frac{2 \eta}{\tau}. 
    \end{equation}
\end{thm}
\begin{proof}
    Bez smanjenja op\v{c}enitosti mo\v{z}emo uzeti da je $\vec x^{\sharp} = \vec x$. Iz $\norm{ \vec x^{\sharp}}_1 \leq \norm{\vec x}_1$ slijedi da je $\vec v := (\vec x^{\sharp} - \vec x)/ \norm{\vec x^{\sharp} - \vec x}_2 \in T(x)$. Po\v{s}to je $\norm{v}_2 = 1$ imamo da je $\norm{\vec{Av}}_2 \geq \tau$, tj. $\norm{\vec A (\vec x^{\sharp} - \vec x)}_2 \geq \tau \norm{\vec x^{\sharp} - \vec x}_2$. Nadalje, vrijedi
    
    \begin{equation*}
        \norm{\vec A (\vec x^{\sharp} - \vec x)}_2 \leq \norm{\vec{Ax}^{\sharp} - y}_2 + \norm{\vec{Ax} - \vec y}_2 \leq 2 \eta
    \end{equation*}
    Tvrdnja slijedi kombiniranjem prethodne dvije nejednakosti.
\end{proof}

\chapter[Koherencija][Koherencija]{Koherencija}
Kao \v{s}to smo vidjeli, uspje\v{s}nost rekonstrukcije rijetkog vektora u kontekstu sa\v{z}etog uzorkovanja ovisi o odre\dj enim kvalitetama matrice mjerenja. Jedna od takvih mjera kvalitete je koherencija. Neformalno, \v{s}to je koherencija matrice mjerenja manja, to je rekonstrukcija uspje\v{s}nija.  

\section[Definicija i svojstva][Definicija i svojstva]{Definicija i svojstva}
U cjelom poglavlju podrazumjevamo da su stupci matrice mjerenje $\ell_2$-normalizirani.
\begin{defn}
    Neka je $\vec A \in \C^{m \times N}$ matrica sa $\ell_2$-normaliziranim stupcima $\vec a_1, \vec a_2, \dots, \vec a_N$, tj. $\norm{\vec a_i}_2 = 1$ za sve $i \in [N]$. Koherencija $\mu = \mu(\vec A)$ matrice $\vec A$ definiramo kao
    \begin{equation}\label{5:1}
        \mu := \max_{1 \leq i \neq j \leq N} |\langle \vec a_i, \vec a_j \rangle| .
    \end{equation}
\end{defn}

Nadalje, uvodimo op\v{c}enitiji pojam funckije $\ell_1$-koherencije. Gornja definicija je poseban slu\v{c}aj za $s = 1$.
\begin{defn}
    Neka je matrica $\vec A \in \C^{m \times N}$ sa $\ell_2$-normaliziranim stupcima  $\vec a_1, \vec a_2, \dots, \vec a_N$. Za $s \in [N-1]$, funkcija $\ell_1$-koherencije $\mu_1$ matrice $\vec A$ je definirana kao
    \begin{equation*}
        \mu_1(s) := \max_{i \in [N]} \max \big\{ \sum_{j \in S} |\langle \vec a_i, \vec a_j \rangle|,\ S \subset [N],\ \card(S) = s,\ i \not \in S   \big\} .
    \end{equation*}
\end{defn}
Jasno je da za $1 \leq s \leq N-1$ vrijedi
\begin{equation}\label{5:2}
    \mu \leq \mu_1(s) \leq s \mu
\end{equation}
i op\v{c}enitije za $1 \leq s,\ t \leq N-1$ takve da $s+t \leq N-1$
\begin{equation}\label{5:3}
    \max \{\mu_1(s), \mu_1(t)\}  \leq \mu_1(s+t) \leq \mu_1(s) + \mu_1(t).
\end{equation}
Primjetimo da je $\ell_1$-koherencija pa stoga i koherencija invarijanta na mno\v{z}enje s lijeva unitarnom matricom $\vec U$. Zaista, stupci od $\vec{UA}$ su $\ell_2$-normalizirani vektori $\vec{Ua}_1, \dots, \vec{Ua}_N$ te zadovoljavaju $\langle \vec{Ua}_i, \vec{Ua}_j \rangle = \langle \vec a_i , \vec a_j \rangle$. Nadalje zbog Cauchy-Schwarzove nejednakosti imamo da vrijedi
\begin{equation*}
   \mu \leq 1. 
\end{equation*}
Neka je na matrica $\vec A \in \C^{m \times N}$ takva da $m \geq N$. Tada je $\mu = 0$ ako i samo ako stupci matrice $\vec A$ formiraju ortonormirani sustav. U slu\v{c}aju da je matrica kvadratna, $\mu = 0$ ako i samo ako je $\vec A$ unitarna. U nastavu \'cemo prou\v{c}avati samo matrice kojima je $m < N$. U tom slu\v{c}aju vrijednost koherencije je odozdo ograni\v{c}ena, \v{s}to \'cemo kasnije i pokazati. 

\begin{thm}
    Neka je $\vec A \in \C^{m \times N}$ matrica sa $\ell_2$-normaliziranim stupcima i neka je $s \in [N]$. Za sve $s$-rijetke vektore $\vec x \in \C^N$ vrijedi,
    \begin{equation*}
        \big(1-\mu_1(s-1)\big) \norm{\vec x}_2^2 \leq \norm{\vec{Ax}}_2^2 \leq \big(1+\mu_1(s-1)\big) \norm{\vec x}_2^2
    \end{equation*}
    ili ekvivalentno, za svaki skup $S \subset [N]$ takav da $\card(S) \leq s$, svojstvene vrijednosti matrice $\vec A^*_S \vec A_S$ le\v{z}e u segmentu $[1-\mu_1(s-1),\ 1+\mu(s-1)]$. Posebno, ako je $\mu_1(s-1) < 1$ tada je $\vec A^*_S \vec A_S$ invertibilna.
\end{thm}
\begin{proof}
    Neka je $S \subset [N]$. Po\v{s}to je matrica $\vec A^*_S \vec A_S$ pozitivno semidefinitna, svojstveni vektori koji odgovaraju realnim pozitivnim svojstvenim vrijednostima \v{c}ine ortonormiranu bazu. Ozna\v{c}imo s $\lambda_{min}$ najmanju i s $\lambda_{max}$ najve\v{c}u svojstvenu vrijednost. Po\v{s}to je $\vec{Ax} = \vec A_S \vec x_S$ za svaki $\vec x \in \C^N$ sa nosa\v{c}em na skupu $S$, slijedi da je maksimum od
    \begin{equation*}
        \norm{\vec{Ax}}_2^2 = \langle \vec A_S \vec x_S, \vec A_S \vec x_S  \rangle = \langle \vec A^*_S \vec A_S \vec x_S, \vec x_S \rangle
    \end{equation*}
    po skupu ${\vec x \in \C^N,\ \supp \vec x \subset S,\ \norm{\vec x}_2 = 1}$, jednak $\lambda_{max}$ i minimum jednak $\lambda_{min}$. Ovo pokazuje ekvivalenciju dvije tvrdnje u teoremu. Nadalje, po\v{s}to imamo da je $\norm{\ve a_j}_2 = 1$ za sve $j \in [N]$, svi dijagonalni elementi matrice $\vec A^*_S \vec A_S$ jednaki su jedan. Prema Gershgorinom teoremu (vidi TODO), svojstvene vrijednost od $\vec A^*_S \vec A_S$ sadr\v{z}ane su u uniji diskova s centrom u 1 radijusa
    \begin{equation*}
        r_j := \sum_{l \in S,\ l \neq j} |(\vec A^*_S \vec A_S)_{j,l}| = \sum_{l \in S,\ l \neq j} |\langle \vec a_l, \vec a_j \rangle| \leq \mu_1 (s-1),\quad j \in S.
    \end{equation*}
    \newpage
    Po\v{s}to su svojstvene vrijednost realno, moraju le\v{z}ati u segmentu $[1-\mu_1(s-1, 1+ \mu_1(s-1))]$.
\end{proof}
\begin{cor}\label{kor:5:4}
    Neka je $\vec A \in \C^{m \times N}$ s $\ell_2$-normaliziranim stupcima i neka je $s \geq 1$. Ako  
    \begin{equation*}
        \mu_1(s) + \mu_1(s-1) < 1, 
    \end{equation*}
    onda je, za svaki $S \subset [N]$ takav da $\card(S) \leq 2s$, matrica $\vec A^*_S \vec A_S$ invertibilna i matrica $\vec A_S$ injektivna. Posebno, isti zaklju\v{c}ak vrijedi ako
    \begin{equation*}
        \mu < \frac{1}{2s - 1}  
    \end{equation*}
\end{cor}
\begin{proof}
    Iz \eqref{5:3}, $\mu_1(s) + \mu_1(s-1) < 1$ povla\v{c}i $\mu_1(2s-1) < 1$. Prema prethodnom teoremu, za $S \subset [N]$ takav da $\card(S) = 2s$, najmanja svojstvena vrijednost matrice $\vec A^*_S \vec A_S$ zadovoljava $\lambda_{min} \geq 1 - \mu_1(2s-1)>0$. Dakle, $\vec A^*_S \vec A_S$ je invertibilna. Ako je $\vec A_S \vec z = \vec 0$ tada je i $\vec A^*_S \vec A_S \vec z = \vec 0$ no to implicira $\vec z = \vec 0$. Dakle, $\vec A_S$ je injektivna. Isti zaklju\v{c}ci slijedi iz $\mu_1(s) + \mu_1(s-1) \leq (2s-1)\mu < 1$ ako je $\mu < 1/(2s-1)$
\end{proof}

\section[Matrice male koherencije][Matrice male koherencije]{Matrice male koherencije}
Sada \'cemo prou\v{c}iti ocjene odozdo na koherenciju i na $\ell_1$-koherenciju matrice $\vec A \in \K^{m \times N}$ takve da $m < N$ i gdje je $\K = \R$ ili $\K = \C$.
\begin{defn}
    Sustav $\ell_2$-normaliziranih vektora $(\vec a_1, \dots, \vec a_N)$ iz $\K^m$ nazivamo ekviangularan ako postoji konstana $c \leq 0$ takva da
    \begin{equation*}
        |\langle \vec a_i, \vec a_j \rangle|  = c \quad \text{za sve } i,j \in [N],\ i \neq j.
    \end{equation*}
\end{defn}
\begin{defn}
    Sustav  vektora $(\vec a_1, \dots, \vec a_N)$ iz $\K^m$ zovemo napeti bazni okvir ako postoji konstanta $\lambda > 0$ takva da vrijedi jedan od ekvivalentnih uvjeta:
    \begin{enumerate}[label=(\alph*)]
        \item $\norm{\vec x}_2^2 = \lambda \sum_{j=1}^N |\langle \vec x, \vec a_j \rangle|^2$ za sve $\vec x \in \K^m$,
        \item $\vec{x} = \lambda \sum_{j=1}^N \langle \vec x, \vec a_j\rangle \vec a_j$  za sve $\vec x \in \K^m$,
        \item $\vec{AA}^* = (1/\lambda) \vec I_m$, gdje je $\vec A$ matrica sa stupcima $\vec a_1, \dots \vec a_N$.
    \end{enumerate}
\end{defn}
Sustav $\ell_2$-normaliziranih vektora zove se ekviangularni napeti bazni okvir ako je bazni okvir ujedno ekviangularni sustav vektora i napeti bazni okvir. Takve sustavi vektora posti\v{z}u takozvanu \textit{Welchovu ocjenu}.
\begin{thm}\label{tm:5:7}
    Koherencija matrice $\vec A \in \K^{m \times N}$ s $\ell_2$-normaliziranim stupcima zadovoljava 
    \begin{equation}\label{5:4}
        \mu \geq \sqrt{\frac{N-m}{m(N-1)}}. 
    \end{equation}
    Jednakost vrijedi ako i samo ako stupci $\vec a_1, \dots \vec a_N$ matrice $\vec A$ \v{c}ine ekviangularni napeti bazni okvir.
\end{thm}
\begin{proof}
    $\vec G := \vec A^* \vec A \in \K^{N \times N}$ zovemo \textit{Gramova matrica} sustava vektora $(\vec a_1, \dots, \vec a_N)$. Elementi od $G$ su obika
    \begin{equation*}
        G_{i,j} = \overline{ \langle \vec a_i, \vec a_j \rangle}  =  \langle \vec a_j, \vec a_i \rangle, \quad i,j \in [N].
    \end{equation*}
    Nadalje, definirajmo matricu $\vec H := \vec{AA}^* \in \K^{m \times m}$. Po\v{s}to su stupci od $\vec A$ $\ell_2$-normalizirani, imamo
    \begin{equation}\label{5:5}
        \tr(\vec G) = \sum_{i = 1}^{N} \norm{\vec a_i}_2^2 = N.
    \end{equation}
    Po\v{s}to skalarni produkt
    \begin{equation*}
        \langle \vec U, \vec V \rangle_F := \tr(\vec{UV}^*) = \sum_{i,j = 1}^{n} U_{i,j}\overline{V_{i,j}}
    \end{equation*}
    inducira \textit{Froebeniusovu normu} $\norm{ \cdot }_F$ na $\K^{n \times n}$ (vidi TODO), Cauchy-Schwarzova nejednakost daje
    \begin{equation}\label{5:6}
        \tr(\vec H) = \langle \vec H, \vec I_m \rangle_F \leq \norm{\vec H}_F \norm{\vec I_m}_F = \sqrt{m} \sqrt{\tr(\vec{HH}^*)}.
    \end{equation}
    Nadalje,
    \begin{align}\label{5:7}
        \tr(\vec{HH}^*) &= \tr(\vec{AA}^* \vec{AA}^*) = \tr(\vec A^* \vec{AA}^* \vec A) = tr(\vec{GG}^*) = \sum_{i,j = 1}^N |\langle \vec a_i, \vec a_j \rangle|^2\nonumber\\ 
        &= \sum_{i=1}^N \norm{\vec a_i}_2^2 + \sum_{i,j = 1,\ i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2 = N + \sum_{i,j = 1,\ i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2.
    \end{align}
    Iz \v{c}injenice da $\tr(\vec G) = \tr(\vec H)$, te kombiniranjem \eqref{5:5}, \eqref{5:6} i \eqref{5:7} imamo
    \begin{equation}\label{5:8}
        N^2 \leq m \big( N + \sum_{i,j = 1,\ i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2 \big)
    \end{equation}
    Napokon, uva\v{z}imo da 
    \begin{equation}\label{5:9}
        |\langle \vec a_i, \vec a_j \rangle| \leq \mu \quad \text{za sve }i,j \in [N],\ i \neq j,
    \end{equation}
    pa slijedi,
    \begin{equation*}
        N^2 \leq m \big( N + (N^2 - N)\mu^2 \big), 
    \end{equation*}
    od kuda lako slijedi ocjena iz tvrdnje teorema.
    Nadalje, jednakost u \eqref{5:4} ako vrijede jednakosti u \eqref{5:6} i \eqref{5:9}. Jednakost u \eqref{5:6} daje $\vec H = \lambda \vec I_m$ za neku nenegativnu konstantu $\lambda$, tj. sustav $(\vec a_1, \dots, \vec a_N)$ je napeti bazni okvir. Iz jednakost u \eqref{5:9} slijedi da je taj sustav ekviangularan.
\end{proof}

Welchovu ocjenu mo\v{z}emo pro\v{s}iriti i na funkciju $\ell_1$-koherencije.
\begin{thm}\label{tm:5:8}
    Funkcija $\ell_1$-koherencije matrice $\vec A \in \K^{m \times N}$ s $\ell_2$-normaliziranim stupcima zadovoljava 
    \begin{equation}\label{5:10}
        \mu_1(s) \geq s \sqrt{\frac{N-m}{m(N-1)}}\quad \text{za } s < \sqrt{N - 1}.
    \end{equation}
    Jednakost se posti\v{z}e ako i samo stupci matrice $\vec A$ formiraju ekviangularni napeti bazni okvir.
\end{thm}
Za dokaz biti \'ce nam potrebna sljede\'ca lema,
\begin{lem}\label{lem:5:9}
    Za $k < \sqrt{n}$, ako kona\v{c}ni niz brojeva $(\alpha_1, \alpha_2, \dots, \alpha_n)$ zadovoljava
    \begin{equation*}
        \alpha_1 \geq \alpha_2 \geq \cdots \alpha_n \geq 0 \quad \text{i} \quad \alpha_1^2,\alpha_2^2, \cdots,\alpha_n^2 \geq \frac{n}{k^2} 
    \end{equation*}
    tada
    \begin{equation*}
        \alpha_1 + \alpha_2 + \cdots + \alpha_k \geq 1, 
    \end{equation*}
    gdje se jednakost posti\v{z}e ako i samo ako $\alpha_1 = \cdots = \alpha_n = 1/k$.
\end{lem}
Ideja dokaza je analogna dokazu teorema \ref{tm:2:5}, tj. problem se svodi na maksimizaciju konveksne funkcije (vidi TODO).

\begin{proof}[Dokaz (Teorem \ref{tm:5:8})]
    Iz \eqref{5:8} imamo 
    \begin{equation*}
        \sum_{i,j=1,i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2 \geq \frac{N^2}{m} - N = \frac{N(N-m)}{m},
    \end{equation*}
    odakle slijedi
    \begin{equation*}
        \max_{i \in [N]} \sum_{i,j=1,i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2 \geq \frac{1}{N} \sum_{i,j=1,i \neq j}^N |\langle \vec a_i, \vec a_j \rangle|^2 \geq \frac{N-m}{m} .
    \end{equation*}
    Neka je $i^* \in [N]$ indeks za koji se posti\v{z}e maksimum. Sortirajmo niz $(|\langle \vec a_{i^*}, \vec a_j \rangle|)_{j=1}^{N}$ kao $\beta_1 \geq \beta_2 \geq \cdots \beta_{N-1} \geq 0$, tako da
    \begin{equation*}
        \beta_1^2 + \beta_2^2 + \cdots + \beta_{N-1}^2 \geq \frac{N-m}{m} .
    \end{equation*}
    Primjenom prethodne lemu s $n = N-1,\ k = s$, i $\alpha_l := (\sqrt{m(N-1)/(N-m)}/s)\beta_l$ dobivamo $\alpha_1 + \cdots \alpha_s \geq 1$. Dakle,
    \begin{equation*}
        \mu_1(s) \geq \beta_1 + \beta_2 + \cdots + \beta_s \geq s \sqrt{\frac{N-m}{m(N-1)}}.
    \end{equation*}
    Pretpostavimo sada da u \eqref{5:10} vrijedi jednakost, pa su sve nejednakosti zapravo jednakosti. Jednakost u \eqref{5:8} implicira da su stupci matrice a napeti bazni okvir. Jednakost u prethodnoj lemi implicira da $|\langle \vec a_{i^*}, \vec a_j \rangle| = \sqrt{(N-m)/(m(N-1))}$ za sve $j \in [N]$, takve da $j \neq i^*$. Po\v{s}to indeks $i^*$ mo\v{z}emo proizvoljno odabrati iz $[N]$, slijedi da je sustav stupaca matrice $\vec A$ ekviangularan. Obrat lako slijedi iz teorema \ref{tm:5:7} i \eqref{5:2}.
\end{proof}

U kontekstu sa\v{z}etog uzorkovanja zanimaju $m \times N$ matrice gdje je $N$ puno ve\'ci od $m$. No, pokazati \'cemo da u tom slu\v{c}aju ne mo\v{z}emo posti\'ci Welchovu ocjenu.
\begin{thm}\label{tm:5:10}
    Kardinalitet $N$ ekviangularnog sustava $(\vec a_1, \dots, \vec a_N)$ $\ell_2$-normaliziranih vektora u $\K^m$ zadovoljava
    \begin{equation*}
        N \leq \frac{m(m+1)}{2} \quad \quad \text{za } \K = \R, 
    \end{equation*}
    \begin{equation*}
        N \leq m^2 \quad \quad \text{za } \K = \C.
    \end{equation*}
    Ako vrijedi jednakost onda je sustav $(\vec a_1, \dots, \vec a_N)$ tako\dj er i napeti bazni okvir.
\end{thm}

Za dokaz teorema potrebna nam je sljede\v{c}a tvrdnja,
\begin{lem}\label{lem:5:11}
    Neka je $z \in \C$, matrica 
    \begin{equation*}
        \begin{bmatrix*}
            1 & z & z & \cdots & z \\
            z & 1 & z & \cdots & z \\
            \vdots & \ddots & \ddots & \ddots & \vdots \\ 
            z & \cdots & z & 1 & z \\
            z & \cdots & z & z & 1 
        \end{bmatrix*}
    \end{equation*}
    ima jednostruku svojstvenu vrijednost $1+(n-1)z$ te svojstvenu vrijednost $1-z$ algebarske kratnosti $n-1$.
\end{lem}
\noindentj Za dokaz leme vidi TODO.

\begin{proof}[Dokaz (Teorem \ref{tm:5:10})]
    Ideja je razmatranja sa prostora $\K^m$ prebaciti na potprostor $\mathcal{S}_m$ operatora na $\K^m$. U slu\v{c}aju $\K = \R$, $\mathcal{S}_m$ je prostor simetri\v{c}nih operatora na $\R^m$, a u slu\v{c}aju $\K = \C$, $\mathcal{S}_m$ je cjeli prostor operatora na $\C^m$. Ti su prostori opremljeni Froebeniusovim skalarnim produktom
    \begin{equation}
        \langle \vec P, \vec Q \rangle_F = \tr(\vec{PQ}^*) 
    \end{equation}
    za $\vec P$, $\vec Q \in \mathcal{S}_m$.
    Ozna\v{c}imo sa $\vec P_1, \dots, \vec P_N \in \mathcal{S}_m$ orthogonalne projektore na potprostore razapete sa $\{\vec a_i\}$ za $i=1,2,\dots,N$, definirane sa
    \begin{equation*}
        \vec P_i(\vec v) = \langle \vec v, \vec a_i \rangle \vec a_i
    \end{equation*}
    za $\vec v \in \K^m$. Nadalje, neka je $c:=|\langle \vec a_i, \vec a_j \rangle|$ za $i \neq j$ te neka je $(\vec e_1, \dots, \vec e_N)$ kanonska baza za $\K^m$. Koriste\v{c}i \v{c}injenicu da je $\vec P_i^2 = \vec P_i = \vec P^*$, za $i,j \in [N],\ i \neq j$ ra\v{c}unamo
    \begin{align*}
        \langle \vec P_i, \vec P_i \rangle_F &= \tr(\vec P_i \vec P_i^*) = \tr(\vec P_i) = \sum_{k=1}^{m} \langle \vec P_i(\vec e_k), \vec e_k \rangle_F = \sum_{k=1}^{m} \langle \vec e_k, \vec a_i \rangle \langle \vec a_i, \vec e_k \rangle \\
    &= \sum_{k=1}^m |\langle \vec a_i, \vec e_k \rangle|^2 = \norm{\vec a_i}_2^2 = 1, \\
        \langle \vec P_i, \vec P_j \rangle_F &= \tr(\vec P_i \vec P_j^*) = \tr(\vec P_i \vec P_j) = \sum_{k=1}^m \langle \vec P_i \vec P_j (\vec e_k), \vec e_k \rangle = \sum_{k=1}^m \langle \vec P_j(\vec e_k), \vec P_i(\vec e_k) \rangle \\
        &= \sum_{k=1}^m \langle \vec e_k, \vec a_j \rangle \overline{\langle \vec e_k, \vec a_i \rangle}\langle \vec a_j, \vec a_i \rangle = \overline{\langle \vec a_i, \vec a_j \rangle} \big \langle \sum_{k=1}^m \langle \vec a_i, \vec e_k \rangle \vec e_k, \vec a_j  \big \rangle\\
        &= \overline{\langle \vec a_i, \vec a_j \rangle} \langle \vec a_i, \vec a_j \rangle = |\langle \vec a_i, \vec a_j \rangle|^2 = c^2.
    \end{align*}
    Dakle, Gramova matrica sustava $(\vec P_1, \dots, \vec P_N)$ je $N \times N$ matrica oblika
    \begin{equation*}
        \begin{bmatrix*}
            1 & c^2 & c^2 & \cdots & c^2 \\
            c^2 & 1 & c^2 & \cdots & c^2 \\
            \vdots & \ddots & \ddots & \ddots & \vdots \\ 
            c^2 & \cdots & c^2 & 1 & c^2 \\
            c^2 & \cdots & c^2 & c^2 & 1 
        \end{bmatrix*}
    \end{equation*}
    Iz \v{c}injenice $0 \leq c^2 < 1$ i leme \ref{lem:5:11} slijedi da je ova Gramova matrica invertibilna, \v{s}to zna\v{c}i da je sustav $(\vec P_1, \dots, \vec P_N)$ linearno nezavisan. Taj sustav le\v{z}i u prostoru $\mathcal{S}_m$ koji je dimenzije $m(m+1)/2$ za $\K = \R$ te dimenzije $m^2$ za $\K = \C$. Stoga vrijedi,

    \begin{equation*}
        N \leq \frac{m(m+1)}{2} \quad \quad \text{za } \K = \R,\quad \quad \quad \quad  N \leq m^2 \quad \quad \text{za } \K = \C.
    \end{equation*}
    Pretpostavimo sada da vrijedi jednakost. Tada je sustav $(\vec I_m, \vec P_1, \dots, \vec P_N)$ linearno zavisan, pa je stoga
    \begin{equation*}
        \begin{vmatrix*}
            1 & b & b & \cdots & b \\
            b & 1 & b & \cdots & b \\
            \vdots & \ddots & \ddots & \ddots & \vdots \\ 
            b & \cdots & b & 1 & b \\
            b & \cdots & b & b & 1 
        \end{vmatrix*} = 0, \quad \quad \text{gdje je } b:= \frac{mc^2 - 1}{m - 1}. 
    \end{equation*}
    Po\v{s}to $1-b = m(1-c^2)/(m-1) \neq 0$, lema \ref{lem:5:11} implicira da je $1+(N-1)b = 0$.Slijedi, 
    \begin{equation*}
        c^2 = \frac{N-m}{m(N-1)}.  
    \end{equation*}
    Dakle, pokazali smo da $\ell_2$-normalizirani sustav $(\vec a_1, \dots, \vec a_N)$ posti\v{z}e Welchovu ocjenu a teorem \ref{tm:5:7} implicira da je taj sustav onda ekviangularan napeti okvir.
\end{proof}

Zanimljvo je da u kontekstu prostora $\C^m$ postoje sustavi od $m^2$ ekviangularnih vektora za sve $m$, dok u $\R^m$ sustavi od $m(m+1)/2$ ekviangularnih vektora ne postoje za sve $m$. Poznato je da postoje u slu\v{c}ajevima gdje je $m$ jednak $2,3,7$ i $23$. Pitanje ostalih slu\v{c}ajeva je i dalje otvoreno.
 
\begin{thm}
    Za $m \geq 3$, ako postoji ekviangularni sustav od $m(m+1)/2$ vektora u $\R^m$, tada je $m+2$ nu\v{z}no kvadrat nekog neparnog prirodnog broja.
\end{thm}
\begin{proof}
    Neka je $(\vec a_1, \dots \vec a_N)$ sustav od $N = m(m+1)/2$ ekviangularnih $\ell_2$-normaliziranih vektora. Prema teoremu \ref{tm:5:10} taj je sustav napeti bazni okvir, pa stoga matrica $\vec A$ sa stupcima $\vec a_1, \dots, \vec a_N$ zadovoljava $\vec{AA}^* = \lambda \vec I_m$ za neki $\lambda > 0$. Matrica $\vec G := \vec A^* \vec A$ ima iste ne-nul svojstvene vrijednosti kao i $\vec{AA}^*$, tj. svojstvenu vrijednost $\lambda$ algebarske kratnosti $m$. i svojstvenu vrijednost nula kratnosti $N-m$. Nadalje, po\v{s}to je $\vec G$ Gramova matrica sustava $(\vec a_1, \dots, \vec a_N)$, njezini dijagonalni elementi jednaki su jedinici, dok su svi vandijagonalni elementi jednaki po apsolutnoj vrijednosti nekom broju $c$. Dakle, matrica $\vec B := (\vec G - \vec I_N)/c$ je oblika
    \begin{equation*}
        \begin{bmatrix*}
            0 & b_{1,2} & b_{1,3} & \cdots & b_{1,N} \\
            b_{2,1} & 0 & b_{2,2} & \cdots & b_{2,N} \\
            \vdots & \ddots & \ddots & \ddots & \vdots \\ 
            b_{N-1,1} & \cdots & b_{N-1, N-2} & 0 & b_{N-1,N} \\
            b_{N,1} & \cdots & b_{N,N-2} & b_{N,N-1} & 0 
        \end{bmatrix*} = 0, \quad \quad \text{gdje je } b_{i,j}:= \pm 1,
    \end{equation*}
    i ima $-1/c$ kao svojstvenu kratnosti $N-m$. Stoga je njezin karaktersti\v{c}ni polinom $p_{\vec B}(x) := \sum_{k = 0}^N \beta_k (-x)^k, \beta_N = 1$, s cjelobrojnim koeficijentima $\beta_k$ i poni\v{s}tava se za $-1/c$. Uva\v{z}e\'ci da je
    \begin{equation*}
        c = \sqrt{\frac{N-m}{m(N-1)}} = \sqrt{\frac{(m+1)/2-1}{m(m+1)/2 -1}} = \sqrt{\frac{m-1}{m^2 + m - 2}} = \frac{1}{\sqrt{m+2}}
    \end{equation*}
    imamo $p_{\vec B}(-\sqrt{m+2}) = 0$, tj.
    \begin{equation*}
        \bigg( \sum_{0 \leq k \leq N/2}b_{2k} (m+2)^k \bigg) + \sqrt{m+2} \bigg( \sum_{0 \leq k \leq (N-1)/2} b_{2k+1}(m+2)^k\bigg)  = 0.
    \end{equation*}
    Ozna\v{c}imo gornje cjelobrojne sume sa $\Sigma_1$ i $\Sigma_2$. Dakle, imamo $\Sigma_1^2 = (m+2) \Sigma_2^2$, \v{s}to implicira da je $(m+2)$ kvadrat. Preostaje pokazati da je $n := \sqrt{m+2}$ neparan. Definiramo $N \times N$ matricu $\vec J_N$ kojoj su svi elementi jednaki jedinici. Dimenzija njezine jezgre je $N-1$ pa je stoga u presjeku s $N-m$ dimenzijonalnim svojstvenim potprostorom od $\vec B$ koji odgovara svojstvenoj vrijednosti $-1/c = -n$, po\v{s}to $N-1 + N - m > N$ za $m \geq 3$, tj. $N = m(m+1)/2 > m + 1$. Matrica $\vec C := (\vec B - \vec I_n + \vec J_N)/2$ ima $-(n+1)/2$ kao svojstvenu vrijednost. Dijagonalni elementi su joj nula, a vandijagonalni jednaki su ili nuli ili jedinici. Stoga je $p_{\vec C}(x) := \sum_{k=0}^{N} \gamma (-x)^k, \gamma_N = 1$ s cjelobrojnim koeficijentima i $p_{\vec C}(x)$ poni\v{s}tava se za $x = -(n+1)/2$. Tu zadnju \v{c}injenicu mo\v{z}emo zapisati u obliku
    \begin{equation*}
        (n+1)^N = - \sum_{k=0}^{N-1}2^{N-k}\gamma_k(n+1)^k.
    \end{equation*}
    \newpage \noindent Slijedi da je $(n+1)^N$ paran pa je stoga i $n+1$. Kona\v{c}no imamo da je $n=\sqrt{m+2}$ neparan.
\end{proof}
Naredni teorem daje eksplicitnu konstrukciju $m \times m^2$ kompleksnih matrica s koherencijom $1/\sqrt{m}$, \v{s}to je ujedno i limes Welchove ocjene kada $N$ ide u beskona\v{c}nost.

\begin{thm}
    Za svaki prosti broj $m \geq 5$, postoji eksplicitna $m \times m^2$ kompleksna matrica s koherencijom $\mu = 1/\sqrt{m}$.
\end{thm}
\begin{proof}
    Kroz dokaz $[m]$ identificiramo sa skupom $\Z / m \Z =: \Z_m$. Za $k,l \in \Z_m$ uvodimo operator \textit{translacije} $\vec T_k$ i operator \textit{modulacije} $\vec M_l$ definirane sa
    \begin{equation*}
        (\vec T_k \vec z)_j  = z_{j-k}, \quad \quad (\vec M_l \vec z)_j = e^{2 \pi i l j / m}z_j 
    \end{equation*}
    za $\vec z \in \C^{\Z_m}$ i $j \in \Z_m$. Ti operatori su izometrije prostora $\ell_2(\Z_m)$. Uvedimo takovani \textit{Alltop} $\ell_2$-normalizirani vektor $\vec x \in \C^{\Z_m}$ definiran sa
    \begin{equation*}
        x_j := \frac{1}{\sqrt{m}}e^{2\pi i j^3/m}, \quad j \in \Z_m .
    \end{equation*}
    Eksplicitna $m \times m^2$ matrica iz tvrdnje teorema dana je kao matrica sa stupcima $\vec M_l \vec T_k \vec x$ za $k,l \in \Z_m$, tj. matrica oblika
    \begin{equation*}
        \begin{bmatrix*}
            \vec M_1 \vec T_1 \vec x & \cdots & \vec M_1 \vec T_m \vec x & \vec M_2 \vec T_1 \vec x & \cdots & \vec M_m \vec T_m \vec x 
        \end{bmatrix*} 
    \end{equation*}
    Ra\v{c}unamo skalarni produkt dva stupca indeksirana sa $(k,l)$ i $(k', l')$
    \begin{align*}
        \langle \vec M_l \vec T_k \vec x, \vec M_{l'} \vec T_{k'} \vec x \rangle &= \sum_{j \in \Z_m} (\vec M_{l} \vec T_{k} \vec x)_j \overline{(\vec M_{l'} \vec T_{k'} \vec x)_j} \\
        &= \sum_{j \in \Z_m} e^{2 \pi ilj/m}x_{j-k} e^{-2 \pi il'j/m}\overline{x_{j-k'}}\\ 
        &= \frac{1}{m} \sum_{j \in \Z_m} e^{2 \pi i(l-l')j/m} e^{2 \pi i((j-k)^3 - (j - k')^3)/m}.
    \end{align*}
    Ozna\v{c}imo $a := l - l'$ i $b := k - k'$, $(a,b) \neq (0,0)$ i promijenimo indeks sumacije za $h = j - k'$
    \begin{align*}
        |\langle \vec M_l \vec T_k \vec x, \vec M_{l'} \vec T_{k'} \vec x \rangle| &= \frac{1}{m} \big |   e^{2 \pi iak'/m}  \sum_{h \in \Z_m} e^{2 \pi iah/m}e^{2 \pi i ((h-b)^3 - h^3)/m}  \big | \\
        &= \frac{1}{m} \big |   \sum_{h \in \Z_m} e^{2 \pi iah/m}e^{2 \pi i (-3bh^2 + 3b^2h - b^3)/m}  \big | \\
        &= \frac{1}{m} \big |   \sum_{h \in \Z_m} e^{2 \pi i (-3bh^2 + (a+3b^2)h)/m}  \big |
    \end{align*}
    Neka je $c := -3b$ i $d := a + 3b^2$,
    \begin{align*}
        |\langle \vec M_l \vec T_k \vec x, \vec M_{l'} \vec T_{k'} \vec x \rangle|^2 &= \frac{1}{m^2} \sum_{h \in \Z_m} e^{2 \pi i (ch^2 + dh)/m} \sum_{h' \in \Z_m} e^{-2 \pi i (ch'^2 + dh')/m}\\
        &= \frac{1}{m^2} \sum_{h,h'} e^{2 \pi i (h-h')(c(h+h') + d)/m} \\ 
        &= \frac{1}{m^2} \sum_{h',h'' \in \Z_m} e^{2 \pi i h''(c(h''+2h') + d)/m} \\
        &= \frac{1}{m^2} \sum_{h'' \in \Z_m} e^{2 \pi i h''(ch''+d)/m} \big ( \sum_{h' \in \Z_m} e^{4 \pi ich''h'/m} \big).
    \end{align*}
    Primjetimo, za svaki $h'' \in \Z_m$ imamo
    \begin{equation*}
          \sum_{h' \in \Z_m} e^{4 \pi ich''h'/m} =  
          \begin{cases}
              m \quad &\text{ako } 2ch'' = 0 \mod m, \\
              0 \quad &\text{ako } 2ch'' \neq 0 \mod m.
          \end{cases}
    \end{equation*}
    Pogledajmo dva slu\v{c}aja:
    \begin{enumerate}
        \item $c = 0 \mod m$: \\
            Po\v{s}to je $c = -3b$ i $3 \neq 0 \mod m$, imamo $b = 0$, pa stoga $d = a +3b^2 \neq 0 \mod m$ i 
            \begin{equation*}
                |\langle \vec M_l \vec T_k \vec x, \vec M_{l'} \vec T_{k'} \vec x \rangle|^2 = \frac{1}{m} \sum_{h'' \in \Z_m} e^{2 \pi idh''/m} = 0.
            \end{equation*}

        \item $c \neq 0 \mod m$: \\
            Po\v{s}to $2 \neq 0 \mod m$, jednakost $2ch'' = 0$ vrijedi samo kada je $h'' = 0 \mod m$, pa stoga
            \begin{equation*}
                |\langle \vec M_l \vec T_k \vec x, \vec M_{l'} \vec T_{k'} \vec x \rangle|^2 = \frac{1}{m} 
            \end{equation*}
    \end{enumerate}
    Dakle, koherencija matrice je $1/\sqrt{m}$.
\end{proof}

\section[Analiza OMP algoritma][Analiza OMP algoritma]{Analiza OMP algoritma}
Pokazati \'cemo da mala koherencija osigurava rekonstrukciju rijetkih vektora OMP algortmom.
\begin{thm}
    Neka je $\vec A \in \C^{m \times N}$ matrica sa $\ell_2$-normaliziranim stupcima. Ako je 
    \begin{equation}\label{5:11}
        \mu_1(s) + \mu_1(s-1) < 1, 
    \end{equation}
    onda se svaki $s$-rijedak vektor $\vec x \in \C^N$ mo\v{z}e rekonstruirati iz vektora mjerenja $\vec y = \vec{Ax}$ u najvi\v{s}e $s$ iteracija OMP algoritma.
\end{thm}
\begin{proof}
    Neka su $\vec a_1, \dots \vec a_N$ $\ell_2$-normalizirani stupci matrice $\vec A$. Prema propoziciji \ref{prop:3:5} dovoljno je dokazati da je za svaki $S \subset [N]$ takav da $\card(S) = s$ matrica $\vec A_S$ injektivna te da vrijedi
    \begin{equation}\label{5:12}
        \max_{j \in S} |\langle \vec r,\vec a_j \rangle|  > \max_{l \in \bar S} |\langle \vec r, \vec a_l \rangle|
    \end{equation}
    za sve ne-nul vektore $\vec r \in \{\vec{Az},\ \supp(\vec z) \subset S\}$. Neka je $\vec r := \sum_{i \in S}r_i \vec a_i$ i neka je $k \in S$ takav da $|r_k| = \max_{i \in S} |r_i| > 0$. Za $l \in \bar S$ imamo,
    \begin{align*}
        |\langle \vec r, \vec a_l \rangle| &= \big | \sum_{i \in S}r_i \langle \vec a_i, \vec a_l \rangle \big | \leq \sum_{i \in S}|r_i||\langle \vec a_i, \vec a_l \rangle| \leq |r_k| \mu_1(s) \\
        |\langle \vec r, \vec a_k \rangle| &= \big | \sum_{i \in S}r_i \langle \vec a_i, \vec a_k \rangle \big |  \geq |r_k| |\langle \vec a_k, \vec a_k \rangle| - \sum_{i \in S, i \neq k}|r_i||\langle \vec a_i, \vec a_k \rangle| \\
        &\geq |r_k| - |r_k|\mu_1(s-1).
    \end{align*}
    Dakle, \eqref{5:12} vrijedi jer \eqref{5:11} implicira $1-\mu_1(s-1) > \mu_1(s)$. Injektivnost od $\vec A_S$ slijedi iz korolara \ref{kor:5:4}.
\end{proof}

% KRAJ
% Na kraju diplomkog rada stavlja se  bibliografija
% Najprije definiramo nacin prikazivanja bibliografije, u ovom slucaju verzija amsplain stila
\bibliographystyle{babamspl} % babamspl ili babplain

% U datoteku diplomski.bib se stavljaju bibliografske reference
% Bibliografske reference u bib formatu se mogu dobiti iz MathSciNet baze, Google Scholara, ArXiva, ...
\bibliography{diplomski}

\pagestyle{empty} % ne zelimo brojanje sljedecih stranica

% I na koncu idu sazeci na hrvatskom i engleskom

\begin{sazetak}
\end{sazetak}

\begin{summary}
\end{summary}

% te zivotopis

\begin{cv}
\end{cv}

\end{document}
