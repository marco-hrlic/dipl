% Predlozak za pisanje diplomskog rada na PMF-MO
% Opcenita uputstva za LaTeX se mogu npr. naci na 
% http://web.math.hr/nastava/rp3, http://web.math.hr/nastava/s4-prof/latex.pdf
% NE PREPORUCA se "Ne baÅ¡ tako kratak uvod u TEX", buduci se radi o vrlo starom prirucniku
% koji nije pogodan za moderne verzije LaTEXa.
% Originalna verzija "The not so short..." na http://tobi.oetiker.ch/lshort/lshort.pdf 
% je obnovljena i daje bolji uvid u moderne verzije LaTeXa

% Stil je optimiziran za kreiranje pdf dokumenta (npr. pomocu pdflatex-a, XeLaTeX-a)

\documentclass[a4paper,twoside,12pt]{memoir} % jednostrano: promijeniti twoside u oneside

% Paket inputenc omogucava direktno unosenje hrvatskih dijakritickih znakova 
% opcija utf8 za unicode (unix, linux, mac)
% opcija cp1250 za windowse
\usepackage[utf8]{inputenc}  % ukoliko se koristi XeLaTeX onda je \usepackage{xunicode}\usepackage{xltxtra}
\usepackage{mathrsfs} 
% Stil za diplomski, unutra je ukljucena podrska za hrvatski jezik
\usepackage{diplomski}
% bibliografija na hrvatskom
\usepackage[languagenames,fixlanguage,croatian]{babelbib} % zahtijeva datoteku croatian.bdf
% hiperlinkovi 
\usepackage[pdftex]{hyperref} 
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}
\usepackage{enumitem}
\usepackage{multicol}

%\newcommand{\vect}[1]{\boldsymbol{\mathrm{#1}}}
\newcommand{\vect}[1]{\mathbf{#1}}
\renewcommand{\vec}{\vect}
\newcommand{\card}{\text{\normalfont{card}}}
\newcommand{\supp}{\text{\normalfont{supp}}}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\norms}[1]{\left\lVert#1\right\rVert}
\newcommand{\rank}{\text\normalfont{rank}}
%\newcommand{\argmin}{\text{\normalfont{arg}}\min}
%\newcommand{\argmax}{\text{\normalfont{arg}}\max}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\sgn}{\text{\normalfont{sgn}}}
\renewcommand{\Re}{\text{\normalfont{Re}}}
\renewcommand{\Im}{\text{\normalfont{Im}}}

\newenvironment{alg}[1]
{
    \bigskip
    \begin{tcolorbox}[arc=0mm,boxrule=1.2pt,colframe=black,colback=white,detach title, before upper={\medskip\begin{center}\textbf{#1}\end{center}\hline\newline\medskip},frame hidden]
    \medskip
}
{
    \medskip
\end{tcolorbox}
    \bigskip
}


% Odabir familije fontova:
% koristenjem XeLaTeX-a mogu se koristiti svi fontovi instalirani na racunalu, npr
% \defaultfontfeatures{Mapping=tex-text}
% \setmainfont[Ligatures={Common}]{Hoefler Text}
% ili
% \newcommand{\nas}[1]{\fontspec{Adobe Garamond Pro}\fontsize{24pt}{24pt}\color{Chocolate}\selectfont #1}
% i onda \nas{Naslov ...}
%\usepackage{lmodern} % times new roman 
\usepackage[T1]{fontenc}
%\usepackage{newtxtext,newtxmath}
\usepackage{lmodern}
\usepackage{mathtools}

\usepackage{lineno}
%\linenumbers

% Paket graphicx sluzi za manipuliranje grafikom 
\usepackage[pdftex]{graphicx} % ukoliko se koristi XeLaTeX onda je \usepackage[xetex]{graphicx}
\usepackage[most]{tcolorbox}
% Paket amsmath je vec ukljucen
% Dodatno definirane matematicke okoline:
% teorem (okolina: thm), lema (okolina: lem), korolar (okolina: cor),
% propozicija (okolina: prop), definicija (okolina: defn), napomena (okolina: rem),
% slutnja (okolina: conj), primjer (okolina: exa), dokaz (okolina: proof)
% Definirane su naredbe za ispisivanje skupova N, Z, Q, R i C
% Definirane su naredbe za funkcije koje se u hrvatskoj notaciji oznacavaju drukcije 
% nego u americkoj: tg, ctg, ... (\tgh za tangens hiperbolni)
% Takodjer su definirane naredbe za Ker i Im (da bi se razlikovala od naredbe za imaginarni dio kompleksnog
% broja, naredba se zove \slika).

\pagestyle{headings}
% uz paket fancyhdr mogu se lako kreirati fancy zaglavlja i podnozja

% Podaci koje treba unijeti
\title{Sa\v{z}eto uzorkovanje}
\author{Marco Hrli\'c}
\advisor{Prof. dr. sc. Damir Baki\'c}  % obavezno s titulom (prof. dr. sc ili doc. dr. sc.)
\date{2019.}  % oblika mjesec, godina

% Moguce je unijeti i posvetu
% Ukoliko nema posvete, dovoljno je iskomentirati/izbrisati sljedeci redak 
\dedication{Albini}

\begin{document}
\setlength\abovedisplayskip{10pt}
\setlength\belowdisplayskip{10pt}
\setlength\abovedisplayshortskip{10pt}
\setlength\belowdisplayshortskip{10pt}

\nocite{*}

% Naredna frontmatter generira naslovnu stranicu, stranicu za potpise povjerenstva, eventualnu posvetu i sadrzaj
% Moze se iskomentirati ukoliko nije u pitanju konacna verzija
\frontmatter

% Tekst diplomskog ...

% Diplomski rad treba poceti s uvodnim poglavljem  
\begin{intro}
...
\end{intro}

\chapter[Rijetka rje\v{s}enja][Rijetka rje\v{s}enja]{Rijetka rje\v{s}enja}\label{chapter_rijetka_rijesenja}	
% ukoliko naslov nije jako dugacak dovoljno je samo \chapter{Naslov poglavlja} 

\section[Rijetsko i sa\v{z}etost vektora][Rijetsko i sa\v{z}etost vektora]{Rijetsko i sa\v{z}etost vektora}
%\subsection{Naslov podsekcije}
Uvedimo potrebnu notaciju. Neka je $[N]$ oznaka za skup $\{1,2,...,N\}$ gdje je $N\in\N$. Sa $\card(S)$ ozna\v{c}ujemo kardinalitet skupa $S$. Nadalje, $\bar{S}$ je komplement od $S$ u $[N]$, tj. $\bar{S}=[N]\backslash S$.

\begin{defn}
    Nosa\v{c} vektora $\vec{x} \in \C^{N}$ je skup indeksa njegovih ne-nul elemenata, tj.
    $$\supp(\vec{x}):=\{j\in[N]:x_j \neq 0 \}$$
\end{defn}

\noindent Za vektor $\vec{x}\in\C^{N}$ ka\v{z}emo da je $s$-rijedak ako vrijedi $$\|\vec{x}\|_0 := \card(\supp(\vec{x})) \leq s$$
Primjetimo,
$$\|\vec{x}\|_p^p := \sum_{j=1}^N|x_j|^p \xrightarrow{p\rightarrow 0} \sum_{j=1}^N\mathbf{1}_{\{x_j \neq 0\}} = \card(\{j \in [N]:x_j \neq 0\}) = \|\vec{x}\|_0$$
Gdje smo koristili da je $\mathbf{1}_{\{x_j \neq 0\}} = 1$  ako je $x_j \neq 0$ te $\mathbf{1}_{\{x_j \neq 0\}} = 0$  ako je $x_j = 0$. Drugim rije\v{c}ima, $\|\vec{x}\|_0$ je limes $p$-te potencije $\ell_p$-kvazinorme vektora $\vec{x}$ kada $p$ te\v{z}i k nuli. Kvazinorma definira se jednako kao standardna $\ell_p$-norma, jedino \v{s}to nejednakost trokuta oslabimo, tj. 
$$\|\vec{x}+\vec{y}\|\leq C(\|\vec{x}\|+\|\vec{y}\|)$$ 
za neku konstantu $C \geq 1$.
Funkciju $\|\cdot\|_0$ \v{c}esto nazivamo $\ell_0$-norma vektora $x$, iako  ona nije niti norma niti kvazinorma. U samoj praksi, te\v{s}ko je tra\v{z}iti rijetkost vektora, pa je stoga prirodno zahtjevati slabiji uvjet \textit{kompresibilnosti}.  
\begin{defn}\label{greska_naj_s_aprox}
    $\ell_p$-gre\v{s}ku najbolje $s$-rijetke aproksimacije vektora $\vec{x}\in\C^{N}$ definiramo sa 
    $$\sigma_s(\vec{x})_p := \inf\big\{\|\vec{x}-\vec{z}\|_p,\ \vec{z} \in \C^{N} \ \text{je s-rijedak}\big\}$$
\end{defn}
\indent Primjetimo da se infimum posti\v{z}e za svaki $s$-rijedak vektor $\vec{z} \in \C^{N}$ koji ima ne-nul elemente koji su jednaki sa $s$ najve\'cih komponenti vektora $\vec{x}$. Iako takav $\vec{z} \in \C^{N}$ nije jedinstven, on posti\v{z}e infimum za svaki $p > 0$. Neformalno, mogli bi re\'ci da je vektor $\vec{x} \in \C^{N}$ \textit{kompresibilan} ako gre\v{s}ka njegove najbolje $s$-rijetke aproksimacije brzo konvergira u $s$. Da bi to formalno iskazali, od koristi \'ce biti ocjena na $\sigma_s(\cdot)_p$. Po\v{s}to nam za to ne\'ce biti va\v{z}an poredak elemenata vektora $\vec{x}$, uvodimo sljede\'cu definiciju koja \'ce nam olaksati ra\v{c}un.

\begin{defn}
    Nerastu\'ci poredak vektora $\vec{x} \in \C^{N}$ je vektor $\vec{x}^* \in \R^{\N}$ takav da
    $$x^*_1 \geq x^*_2 \geq x^*_3 \geq \dots \geq 0$$
    te postoji permutacije $\pi : [N]\rightarrow[N]$ takva da $x^*_j=|x_{\pi(j)}|$ za sve $j\in [N]$.
\end{defn}
\begin{prop}\label{osnovna_ocjena_lp_greske}
    Za svaki $q > p > 0$ i za svaki $\vec{x}\in \C^{N}$ vrijedi
    $$\sigma_s(\vec{x})_q \leq \frac{1}{s^{1/p - 1/q}}\|\vec{x}\|_p.$$
\end{prop}
\begin{proof}
    Neka je $\vec{x}^* \in \R^N$ nerastu\'ci poredak vektora $\vec{x}\in\C^N$. Tada slijedi,
    \begin{equation*}
    \begin{split} 
        \sigma_s(\vec{x})_q^q &= \sum_{j=s+1}^{N}(x_j^*)^q=\sum_{j=s+1}^{N}(x_j^*)^p(x_j^*)^{q-p} \leq (x_s^*)^{q-p} \sum_{j=s+1}^{N}(x_j^*)^p \\ & \leq \bigg(\frac{1}{s}\sum_{j=1}^{s}(x_j^*)^p\bigg)^{\frac{q-p}{p}}\bigg( \sum_{j=s+1}^N(x_j^*)^p\bigg) \leq \bigg( \frac{1}{s} \|\vec{x}\|_p^p \bigg)^{\frac{q-p}{p}}\|\vec{x}\|_p^p \\ & = \frac{1}{s^{q/p-1}}\|\vec{x}\|_p^q
    \end{split}
    \end{equation*}
    Prva nejednakost slijedi iz \v{c}injenice da je $x_j^* \leq x_s^*$ za svaki $j \geq s+1$. Druga nejednakost je tako\dj er posljedica nerasta komponenti od $\vec{x}^*$. Potenciranjem obje strane s $1/q$ slijedi tvrdnja.
\end{proof}
Primjetimo da ako je $\vec{x}$ iz jedini\v{c}ne $\ell_p$-kugle za neki mali $p>0$, onda prethodna propozicija garantira kovergenciju od $\sigma_s(\vec{x})_q$ u $s$, gdje $\ell_p$-kuglu definiramo kao
$$B_p^N := \big\{ \vec{z} \in \C^N : \|\vec{z}\|_p \leq 1\big\}$$
Vratimo se sada ocjeni iz propozicije \ref{osnovna_ocjena_lp_greske}. Sljede\'ci teorem daje najmanju konstantu $c_{p,q}$ takvu da vrijedi $\sigma_s(\vec{x})_q\leq c_{p,q}s^{-1/p+1/q}\|\vec{x}\|_p$ te zapravo predstavlja ja\v{c}u tvrdnju.
\begin{thm}
    Za svaki $q > p > 0$ i za svaki $\vec{x}\in\C^N$ vrijedi
    \begin{equation*}
    \sigma_s(\vec{x})_q \leq \frac{c_{p,q}}{s^{1/p - 1/q}}\|\vec{x}\|_p
    \end{equation*}
    gdje je
    $$
    c_{p,q} := \bigg[ \bigg(\frac{p}{q}\bigg)^{p/q}\bigg( 1-\frac{p}{q}^{1-p/q}\bigg) \bigg]^{1/p}\leq1.
    $$
\end{thm}
Istaknimo za \v{c}esti odabir $p=1$ i $q=2$
\begin{equation*}
    \sigma_s(\vec{x})_2 \leq \frac{1}{2\sqrt{s}}\|\vec{x}\|_1
\end{equation*}
\begin{proof}
    Neka je $\vec{x}^*$ nerastu\'ci poredak vektora $\vec{x}\in\C^N$ i $\alpha_j := (x_j^*)^p$. Dokazati \'cemo ekvivaltenu tvrdnju
    \begin{equation}\label{ocjena_ekv_tvrdnja}
    \begin{rcases}
{\alpha_1 \geq \alpha_2 \geq \cdots \geq \alpha_N \geq 0} \\
{\alpha_1 + \alpha_2 + \cdots + \alpha_N \leq 1} 
\end{rcases}\implies \alpha_{s+1}^{q/p} + \alpha_{s+2}^{q/p} + \cdots + \alpha_{s+N}^{q/p} \leq \frac{c^q_{q}}{s^{q/p-1}}
    \end{equation}
    Stoga, za $r:=q/p>1$, problem se svodi na maksimizaciju konveksne funkcije
    $$
    f(\alpha_1, \alpha_2, \dots, \alpha_N) := \alpha_{s+1}^r + \alpha_{s+2}^r + \cdots +\alpha_{N}^r
    $$
    na konveksnom mnogokutu
    $$
    \mathcal{C} := \big\{ (\alpha_1, \cdots, \alpha_N)\in \R^N :  \alpha_1 \geq \alpha_2 \geq \cdots \geq \alpha_N \geq 0 i  \alpha_1 + \alpha_2 + \cdots + \alpha_N \leq 1\big\}
    $$
    Prema teoremu (todo) $f$ posti\v{z}e maksimum na nekom od vrhova mnogokuta $\mathcal{C}$, a vrhovi od $\mathcal{C}$ su dani kao sjeci\v{s}ta  $N$ hiperplohi koje dobijemo tako da u \eqref{ocjena_ekv_tvrdnja} $N$ nejednakosti pretvorimo u jednakosti. Mogu\v{c}nosti su:
    \begin{enumerate}
        \item $\alpha_1=\cdots=\alpha_N \ \implies\  f(\alpha_1, \alpha_2, \dots, \alpha_N) = 0$.
        \item $\alpha_1+\cdots+\alpha_N=1$ i $\alpha_1=\cdots=\alpha_k>\alpha_{k+1}=\cdots=\alpha_N=0$ za neki \\ $1\leq k \leq s \  \implies \  f(\alpha_1, \alpha_2, \dots, \alpha_N) = 0$
        \item $\alpha_1+\cdots+\alpha_N=1$ i $\alpha_1=\cdots=\alpha_k>\alpha_{k+1}=\cdots=\alpha_N=0$ za neki\\ $s+1\leq k \leq N \  \implies \  \alpha_1=\cdots\alpha_k=1/k$ te $f(\alpha_1, \alpha_2, \dots, \alpha_N) = (k-s)/k^r$
    \end{enumerate}
    Dakle, slijedi da 
    $$
    \max\limits_{(\alpha_1,\dots,\alpha_N)\in\mathcal{C}} f(\alpha_1, \alpha_2, \dots, \alpha_N) = \max\limits_{s+1\leq k \leq N} \frac{k-s}{k^r}
    $$
    Shvatimo sada $k$ kao realnu varijablu i zamjetimo da $g(k):=(k-s)/k^r$ raste do kriti\v{c}ne to\v{c}ke $k^*=(r/(r-1))s$ nakon koje opada.
    $$
    \max\limits_{(\alpha_1,\dots,\alpha_N)\in\mathcal{C}} f(\alpha_1, \alpha_2, \dots, \alpha_N) \leq g(k^*) = \frac{1}{r}\bigg( 1- \frac{1}{r}\bigg)^{r-1}\frac{1}{s^r-1}=c^q_{p,q}\frac{1}{s^{q/p}-1}
    $$
\end{proof}
\indent Alternativni na\v{c}in na koji bi mogli definirati pojam \textit{kompresibilnosti} za vektor $\vec{x}\in\C^N$ je da zahtjevamo da je broj
$$\card(\{j\in[N]:|x_j|\geq t\})$$
tj. broj njegovih zna\v{c}ajnih ne-nul komponenti dovoljno mali. Ovaj pristup vodi na definiciju slabih $\ell_p$-prostora.
\begin{defn}
Za $p>0$, slabi $\ell_p$-prostor s oznakom $w\ell_p^N$ definiramo kao prostor $\C^N$ sa kvazinormom
\begin{equation}\label{slaba_kvazinorma}
    \|\vec{x}\|_{p, \infty}:=\inf\bigg\{ M \geq 0: \card (\{j\in [N]: |x_j|\geq t \})\leq \frac{M^P}{t^p},\ \forall t>0    \bigg\}
\end{equation}
\end{defn}
\noindent
Da bi pokazali da je \eqref{slaba_kvazinorma} zapravo kvazinorma, potreban nam je sljede\'ci rezultat.
\begin{prop}
    Neka su $\vec{x}^1,\dots\vec{x}^k\in\C^N$. Tada za svaki $p>0$ vrijedi 
    \begin{equation*}
    \|\vec{x}^1+\dots+\vec{x}^k\|_{p,\infty} \leq k^{\max\{1, 1/p\}}(\|\vec{x}^1\|_{p, \infty} + \cdots + \|\vec{x}^k\|_{p, \infty})
    \end{equation*}
\end{prop}
\begin{proof}
    Neka je $t>0$. Ako je $|x_j^1+\cdots+x_j^k|\geq t$ za neki $j\in [N]$, tada imamo da je $|x_j^i|\geq t/k$ za neki $i \in [k]$. Dakle, vrijedi
    \begin{equation*}
        \big\{ j\in [N]:|x_j^1+\cdots+x_j^k| \geq t \big\} \subset \bigcup\limits_{i\in [k]} \big \{ j \in [N] : |x_j^i| \geq t/k \big \}
    \end{equation*}
    pa je stoga
    \begin{align*}
        \card\big( \big\{ j\in [N] : |x_j^1+\cdots+k_j^k| \geq t \big\}\big)&\leq\sum\limits_{i\in [k]}\frac{\|\vec{x}^i\|^p_{p, \infty}}{(t/k)^p} \\ 
                                                                            &= \frac{k^p(\|\vec{x}^1\|^p_{p, \infty}+\cdots + \|\vec{x}^k\|^p_{p, \infty})}{t^p}
    \end{align*}
    Prema definiciji slabe $\ell_p$-kvazinorme \eqref{slaba_kvazinorma} vektora $\vec{x}^1+\cdots+\vec{x}^k$ dobivamo
    \begin{equation*}
        \|\vec{x}^1+\cdots+\vec{x}^k\|_{p, \infty}\leq k\big(\|\vec{x}^1\|^p_{p,\infty}+ \cdots +\|\vec{x}^k\|^p_{p,\infty}\big) 
    \end{equation*}
    Ako je $p \leq 1$, uspore\dj uju\'ci $\ell_p$ i $\ell_1$ norme na $\R^k$ slijedi
    \begin{equation*}
        \big(\|\vec{x}^1\|^p_{p,\infty}+ \cdots +\|\vec{x}^k\|^p_{p,\infty}\big)^{1/p} \leq k^{1/p-1}\big(\|\vec{x}^1\|_{p,\infty}+ \cdots +\|\vec{x}^k\|_{p,\infty}\big)
    \end{equation*}
    te ako je $p \geq 1$ slijedi
    \begin{equation*}
        \big(\|\vec{x}^1\|^p_{p,\infty}+ \cdots +\|\vec{x}^k\|^p_{p,\infty}\big)^{1/p} \leq \|\vec{x}^1\|_{p,\infty}+ \cdots +\|\vec{x}^k\|_{p,\infty}.
    \end{equation*}
    Tvrdnja slijedi kombiniranjem dobivenih ocjena.
\end{proof}

\noindent
Uzmimo $\vec{x}, \vec{y} \in \C^N$ i neka je $\lambda \in \C$ proizvoljan.

\begin{enumerate}
    \item Neka je $\|\vec{x}\|_{p, \infty}=0$. Iz \eqref{slaba_kvazinorma} slijedi $ \card(\{j \in [N]: |x_j| \geq t\}) = 0$ za svaki $t > 0$ pa je stoga broj ne-nul komponenti on $\vec{x}$ jednak nuli, tj. $\vec{x}=0$ 
    \item Ako je $\lambda$ nula, $\|\lambda \vec{x}\| = | \lambda | \| \vec{x} \|$ vrijedi trivijalno. Za $\lambda \neq 0$, imamo \\
        $\card(\{ j \in [N]: |\alpha x_j| \geq t \}) = \card(\{ j \in [N]: |x_j| \geq t/|\alpha| \})\leq (\alpha M)^p/t^p$ za svaki $t>0$. Dakle, opet $\|\lambda \vec{x}\| = | \lambda | \| \vec{x} \|$.
    \item $\|\vec{x}+\vec{y}\|\leq C(\|\vec{x}\|+\|\vec{y}\|)$ je sada direktna posljedica prethodne propozicije.
\end{enumerate}

\noindent sljede\'ca propozicija daje alternativni izraz za slabu $\ell_p$-kvazinormu.
\begin{prop}\label{slaba_kvazinorma_2}
    Za $p>0$, vrijedi
    \begin{equation*}
        \|\vec{x}\|_{p, \infty} = \max \limits_{k \in [N]}k^{1/p}x_k^{*}
    \end{equation*}
    gdje je $\vec{x}^* \in \R^N$ nerastu\'ci poredak vektora $\vec{x}\in \C^N$.
\end{prop}
\begin{proof}
    Primjetimo prvo da iz \eqref{slaba_kvazinorma} slijedi da je $\|\vec{x}\|_{p, \infty}=\|\vec{x}^*\|_{p, \infty}$, pa zapravo pokazujemo da je $\|\vec{x}\|:= \max_{k \in [N]}k^{1/p}x_k^* = \|\vec{x}^*\|$. Nadalje, za $t>0$ vrijedi da je $\{j \in [N]: x^*_j \geq t\}=[k]$ za neki $k \in [N]$ ili je $\{j \in [N]: x^*_j \geq t\}=\emptyset$. U prvom slu\v{c}aju $t \leq x^*_k \leq \|\vec{x}\|/k^{1/p}$ pa je $\card(\{j \in [N]:x_j^* \geq t \}) = k \leq \|\vec{x}\|/k^{1/p}$. U drugom slu\v{c}aju ista nejednakost vrijedi trivijalno. Iz definicije slabe $\ell_p$-kvazinorme \eqref{slaba_kvazinorma} sada dobivamo $\|\vec{x}^*\|_{p, \infty} \leq \|\vec{x}\|$. Pretpostavimo da je $\|\vec{x}^*\|_{p, \infty} < \|\vec{x}\|$. Tada postoji $\varepsilon > 0$ takav da $(1+ \varepsilon)\|\vec{x}^*\|_{p, \infty} \leq \|\vec{x}\|$. Slijedi da je $(1 + \varepsilon)\|\vec{x}^*\| \leq  k^{1/p}x^*_k$ za neki $k \in [N]$ pa stoga
    \begin{equation*}
        [k] \subseteq \big\{ j \in [N] : (1 + \varepsilon)\|\vec{x}^*\|_{p, \infty}/k^{1/p} \leq x_j^* \big\}
    \end{equation*}
    Ponovo iz \eqref{slaba_kvazinorma} imamo
    \begin{equation*}
        k \leq \frac{\|\vec{x}^*\|^p_{p, \infty}}{\big( (1 + \varepsilon)\|\vec{x}^*\|_{p, \infty}k^{1/p}\big)^p}=\frac{k}{(1 + \varepsilon)^p}
    \end{equation*}
    Kontradikcija, dakle mora vrijediti $\|\vec{x}\| = \|\vec{x}^*\|_{p, \infty}$.
\end{proof}
\noindent Sada lagano mo\v{z}emo usporediti slabi i jaku $\ell_p$ normu,
\begin{prop}
    Za svaki $p > 0$ i za svaki $\vec{x} \in \C^N$,
    \begin{equation*}
        \|\vec{x}\|_{p, \infty} \leq \|\vec{x}\|_p
    \end{equation*}
\end{prop}
\begin{proof}
    Neka je $k \in [N]$,
    \begin{equation*}
        \|\vec{x}\|_p^p = \sum_{j=1}^{N}(x_j^*)^p \geq \sum_{j=1}^{k}(x_j^*)^p \geq k(x_k^*)^p
    \end{equation*}
    Tvrdnja slijedi potenciranjem na $1/p$ i uzimaju\'ci maksimum po $k$ i primjenom prethodne propozicije.
\end{proof}
Koriste\'ci propoziciju \eqref{slaba_kvazinorma_2} mo\v{z}emo dobiti verziju ocjene iz propozicije \eqref{osnovna_ocjena_lp_greske} sa slabom $\ell_p$ normom.
\begin{prop}
    Za svaki $q>p>0$ i $\vec{x} \in \C^N$, vrijedi
    \begin{equation*}
        \sigma_s(\vec{x})_q \leq \frac{d_{p,q}}{s^{1/p-1/q}}\|\vec{x}\|_{p, \infty}
    \end{equation*}
    gdje je
    \begin{equation*}
        d_{p,q} := \big( \frac{p}{q-p} \big)^{1/q}.
    \end{equation*}
\end{prop}
\begin{proof}
    Bez smanjenja op\v{c}enitosti mo\v{z}emo pretpostaviti da je $\norm{\vec{x}}_{p,\infty} \leq 1$, pa je $x_k^* \leq 1/k^{1/p}$ za svaki $k \in [N]$. Tada vrijedi,
    \begin{equation*}
    \sigma_s(\vec{x})^q_q = \sum_{k=s+1}^{N} (x_k^*)^q \leq \sum_{k=s+1}^N \frac{1}{k^{q/p}} \leq \int_s^N \frac{1}{t^{q/p}} dt = - \frac{1}{q/p-1} \frac{1}{t^{q/p-1}}\bigg\rvert^{t=N}_{t=s} \leq \frac{p}{q-p} \frac{1}{s^{q/p-1}}.
    \end{equation*}
    Potenciranjem sa $1/q$ slijedi tvrdnja.
\end{proof}
Prethodna propozicija daje da su vektori $\vec{x} \in \C^N$ koji su kompresibilni u smislu $\norm{\vec{x}}_{p, \infty} \leq 1$ za mali $p>0$, tako\dj er kompresibilni u smislu da gre\v{s}ka njihove najbolje $s$-rijetke aproksimacije brzo konvergira sa $s$. Iska\v{z}imo jo\v{s} jedan tehni\v{c}ki rezultat,
\begin{lem}
    Neka su $\vec{x}, \vec{y} \in \C^N$. Tada vrijedi,
    \begin{equation} \label{nerastuci_poredak_ocjena_1}
        \norm{\vec{x}^* - \vec{y}^*}_{\infty} \leq \norm{\vec{x} - \vec{y}}_{\infty}
    \end{equation}
    Nadalje, za $s \in [N]$,
    \begin{equation}\label{nerastuci_poredak_ocjena_2}
        |\sigma_s(\vec{x})_1 - \sigma(\vec{y})_1| \leq \norm{\vec{x} - \vec{y}}_1
    \end{equation}
    i za $k>s$,
    \begin{equation}\label{nerastuci_poredak_ocjena_3}
        (k-s)x_k^* \leq \norm{\vec{x} - \vec{y}}_1 + \sigma_s(\vec{y})_1
    \end{equation}
\end{lem}
\begin{proof}
    Za $j \in [N]$, skup indeksa $j$ najve\'cih komponenti vektora $\vec{x}$ ima ne-trivijalni presjek sa skupom od $N-j+1$ najmanjih komponenti vektora $\vec{y}$. Izaberimo indeks $l$ iz tog presjeka. Tada vrijedi, 
    \begin{equation*}
        x_j^* \leq |x_l| \leq |y_l| + \norm{\vec{x} - \vec{y}}_{\infty} \leq z_j^* + \norm{\vec{x} - \vec{y}}_{\infty}
    \end{equation*}
    Zamjenom uloga od $\vec{x}$ i $\vec{y}$ slijedi \eqref{nerastuci_poredak_ocjena_1}.
    Neka je $\vec{v} \in \C^N$ najbolja $s$-rijetka aproksimacija vektora $\vec{y}$. Tada
    \begin{equation*}
        \sigma_s(\vec{x})_1 \leq \norm{\vec{x} - \vec{v}}_1 \leq \norm{\vec{x} - \vec{y}}_1 + \norm{\vec{y} - \vec{v}}_1 = \norm{\vec{x} - \vec{y}}_1 + \sigma_s(\vec{y})_1 
    \end{equation*}
    Ponovno, zbog simetrije slijedi \eqref{nerastuci_poredak_ocjena_2}. Napokon, ocjena \eqref{nerastuci_poredak_ocjena_3} slijedi iz \eqref{nerastuci_poredak_ocjena_2} te iz \v{c}injenice
    \begin{equation*}
        (k-s)x_k^* \leq \sum_{j=s+1}^{k}x_j^* \leq \sum_{j \geq s+1} x_j^* = \sigma_s(\vec{x})_1.
    \end{equation*}
\end{proof}

\section[Minimalni broj mjerenja][Minimalni broj mjerenja]{Minimalni broj mjerenja}
Problem sa\v{z}etog uzorkovanja sastoji se od rekonstrukcije $s$-rijetkog vektora $\vec{x} \in \C^N$ iz sustava
\[\vec{y} = \vec{A}\vec{x}\]
Matricu $\vec{A} \in \C^{m\times N}$ nazivamo \textit{matrica mjerenja}. Ako je $m < N$, za ovakav sustav linearnih jednad\v{z}bi ka\v{z}emo da je \textit{neodre\dj en}. Iako iz klasi\v{c}ne teorije linearne algebre ovakvi sustavi imaju beskona\v{c}no mnogo rije\v{s}enja, pokazati \'ce se da je dodatna pretpostavka rijetkosti vektora $x$ dovoljno za jedinstvenost rje\v{s}enja. U ovom poglavlju istra\v{z}iti \'cemo koji je minimalni broj mjerenja, tj. $m$ broj redaka matrice $\vec{A}$, koji garantira rekonstrukciju $s$-rijetkog vektora $\vec{x}$. Zapravo, postoje dva pristupa ovom problemu. Mo\v{z}emo zahtjevati da problem mjerenja rekonstruira sve $s$-rijetke vektore $\vec{x} \in \C^N$ istodobno ili mo\v{z}emo tra\v{z}iti rekonstrukciju specifi\v{c}nog, tj. predodre\dj enog vektora $\vec{x} \in \C^N$. Taj pristup \v{c}ini se neprirodan, no pokazuje se da je on va\v{z}an u prou\v{c}avanju problema gdje matricu $\vec{A}$ biramo nasumi\v{c}no. \\ 
\indent Poka\v{z}imo da su za danu rijetkost $s$, matricu $\vec{A} \in \C^{m \times N}$ i $s$-rijedak vektor $\vec{x} \in \C^N$, naredne tvrdnje ekvivaltentne: 
\begin{enumerate}
    \item Vektor $\vec{x}$ je jedinstveno $s$-rijetko rje\v{s}enje sustava $\vec{A}\vec{z}=\vec{y}$ gdje je $\vec{y} = \vec{Ax}$, tj. $\{\vec{z} \in \C^N : \vec{A}\vec{z}= \vec{A}\vec{x},\ \norm{\vec{z}}_0 \leq s\} = \{\vec{x}\}$
    \item Vektor $\vec{x}$ je jedinstveno rje\v{s}enje problema minimizacije
        \begin{equation}\label{problem_minimizacije}
            \min\limits_{\vec{z} \in \C^N} \norm{\vec{z}}_0\quad \text{uz uvjet}\ \vec{Az} = \vec y \tag{$P_{0}$}
        \end{equation}
\end{enumerate}
Ako je $\vec{x} \in \C^N$ jedinstveno $s$-rijetko rje\v{s}enje od $\vec{Az} = \vec y$ takvo da je $\vec y = \vec{Ax}$, onda rje\v{s}enje $x^{\sharp}$ od \eqref{problem_minimizacije} je $s$-rijetko i zadovoljava $\vec{Ax} = \vec y$ pa je $\vec x^\sharp = \vec x$. Drugi smjer slijedi trivijalno.


\subsection[Rekonstrukcija svih rijetkih vektora][Rekonstrukcija svih rijetkih vektora]{Rekonstrukcija svih rijetkih vektora}
Neka je $\vec{A} \in \C^{m \times N}$ i $S \subset [N]$, sa $\vec A_S$ ozna\v{c}ujemo matricu formiranu od stupaca od $\vec A$ indeksiranih sa $S$. Sli\v{c}no, sa $\vec x_S$ ozna\v{c}ujemo ili vektor iz $\C^{S}$ koji se sastoji od komponenti vektora $\vec x$ indeksiranih po $S$, tj. $(\vec x_S)_l = x_l$ za sve $l \in S$, ili vektor iz $\C^N$ koji se podudara s $\vec x$ na komponentama indeksiranim u $S$ i jednak je nula na indeksima koji nisu u $S$, tj. $(\vec x_S)_l = x_l$ za $l \in S$ i $(\vec x_S)_l =0$ za $ l \notin S$. Iz konteksta \'ce uvijek biti jasno na koju definiciju se misli.

\begin{thm} \label{rekonstrukcija_tm1}
    Neka je $\vec A \in \C^{m \times N}$. Ekvivalentno je:
    \begin{enumerate}[label=(\alph*)]
        \item Svaki $s$-rijedak vektor $\vec x \in \C^N$ je jedinstveno rje\v{s}enje od $\vec{Ax}=\vec{Az}$, tj. ako je $\vec{Ax}=\vec{Az}$ i ako su $\vec x$, $\vec z$ oboje $s$-rijetki tada $\vec x = \vec z$.
        \item Jezgra od $\vec A$ ne sadr\v{z}i niti jedan $2s$-rijedak vektor osim nul-vektora, tj. $\ker \vec A \cap \{\vec z \in \C^N: \norm{\vec z}_0 \leq 2s\} = \{\vec 0\}$
        \item Za svaki $S \subset [N]$ takav da $\card(S) \leq 2s$, podmatrica $\vec A_S$ je injektivna kao preslikavanje sa $\C^S$ u $\C^m$.
        \item Svaki skup od $2s$ stupaca matrice $\vec A$ je linearno nezavisan skup.
    \end{enumerate}
\end{thm}
\begin{proof}
    \begin{itemize}
        \item[]$(b)\implies(a)$. Neka su $\vec x$ i $\vec z$ $s$-rijetki vektori takvi da $\vec{Ax} = \vec{Az}$. Tada je $\vec x - \vec z$ $2s$-rijedak i $\vec A(\vec x - \vec z) = \vec 0$. Po\v{s}to $\ker \vec A$ ne sadr\v{z}i $2s$-rijetke vektore osim nul-vektora, mora vrijediti $\vec x = \vec z$.
        \item[] $(a)\implies(b)$. Obratno, pretpostavimo da za svaki $s$-rijetki vektor $\vec x \in \C^N$ vrijedi $\{\vec z \in \C^N : \vec{Az} = \vec{Ax}, \norm{\vec z}_0 \leq s\} = \{\vec x\}$. Neka je $\vec v \in \ker \vec A$, $2s$-rijedak. Tada $\vec v$ mo\v{z}emo rastaviti kao $\vec v = \vec x - \vec z$ gdje su $\vec x$ i $\vec z$ $s$-rijetki takvi da $\supp(\vec x)\cap \supp(\vec z) = \emptyset$. Imamo da je $\vec{Ax}=\vec{Az}$ pa prema pretpostavci vrijedi $\vec{x}=\vec{z}$. Po\v{s}to su nosa\v{c}i od $\vec x$ i $\vec z$ disjunktni, mora vrijediti $\vec x = \vec z = \vec 0$ pa je stoga i $\vec v = 0$.
        \item[] $(b)\implies(c)$. Pretpostavimo suprotno, $\ker \vec A \cap \{\vec z \in \C^N: \norm{\vec z}_0 \leq 2s\} = \{\vec 0\}$ i da postoji $S \in [N]$ takav da je $\card(S) \leq 2s$ te da $\vec A_s$ nije injektivna. To zna\v{c}i da postoji vektor $\vec x \in \C^{\card(S)} \backslash \{\vec 0\}$ takav da je $\vec A_S \vec x = \vec 0$. Definiramo vektor $\tilde{\vec{x}}\in \C^N$ sa 
            \begin{equation*}
                \tilde{x}_j = 
                \begin{cases}
                    x_j \quad & \text{za}\ j \in S \\
                    0 \quad & \text{za}\  j \in \bar S \\
                \end{cases}
            \end{equation*}
            Dakle, imamo $\vec x \neq \vec 0$, $\norm{\vec x}_0 \leq 2s$ i vrijedi $\vec{Ax}=0$, tj. $\vec x \in \ker \vec A$. Kontradikcija s $(b)$.
        \item[]$(c)\implies(d)$. Odaberimo $2s$  stupaca od $\vec A$. Skup indeksa tih stupaca ozna\v{c}imo sa $S$. Prema $(c)$, matrica $\vec A_S$ je injektivna, a to zna\v{c}i da su njeni stupci linearno nezavisni, pa su stoga i $2s$ odabranih stupaca matrice $\vec A$ linearno nezavisni.
        \item[]$(d)\implies(b)$. Pretpostavimo da jezgra od $\vec A$ sadr\v{z}i $2s$-rijedak ne-nul vektor $\vec x \in \C^N$. Neka je $S$ skup indeksa ne-nul elemenata vektora $\vec x$. To zna\v{c}i da je $\vec A_S \vec x_S = 0$, i $\vec x_S \neq \vec 0$. Dakle $\vec A_S$ nije injektivna, pa stoga i skup stupaca od $\vec A$ indeksiranih sa $S$ nije linearno nezavisan, \v{s}to je kontradikcija sa $(d)$.
\end{itemize}
\end{proof}

Uo\v{c}imo da ako je mogu\v{c}e rekonstruirati svaki $s$-rijedak vektor $\vec x \in \C^N$ iz vektora mjerenja $\vec y = \vec{Ax} \in \C^m$, tada vrijedi $(a)$. Prema pro\v{s}lom teoremu tada vrijedi i tvrdnja $(d)$ pa je stoga $\rank(\vec A) \geq 2s$. Tako\dj er vrijedi da je $\rank(\vec A) \leq m$ pa imamo 
\begin{equation*}
    m \geq 2s.    
\end{equation*}
To zna\v{c}i da je potrebno barem $2s$ mjerenja da bi rekonstruirali svaki $s$-rijedak vektor. Pokazati \'cemo da je, makar u teoriji, dovoljno to\v{c}no $2s$ mjerenja.

\begin{thm}
    Za svaki $N \geq 2s$, postoji matrica mjerenja $\vec A \in \C^{2s \times N}$ takva da se svaki $s$-rijedak vektor $\vec x \in \C^N$ mo\v{z}e rekonstruirati iz vektora mjerenja $\vec y = \vec{Ax} \in \C^m$ kao rje\v{s}enje problema minimizacije \eqref{problem_minimizacije}.
\end{thm}
\begin{proof}
    Fiksirajmo $t_N>\cdots t_2 > t_1 > 0$ i neka je $\vec A \in \C^{2s \times N}$ dana sa
    \begin{equation}\label{vandermont_matrica}
        \vec A = 
        \begin{bmatrix}
            1 & 1 & \cdots & 1 \\ 
            t_1 & t_2 & \cdots & t_N \\
            \vdots & \vdots & \cdots & \vdots \\
            t_1^{2s-1} & t_2^{2s-1} & \cdots & t_N^{2s-1} \\
        \end{bmatrix}
    \end{equation}
    Nadalje, neka je $S=\{j_1 < \cdots < j_{2s}\}$ skup indeksa. Matrica $\vec A_S \in \C^{2s \times 2s}$ je transponirana \textit{Vandermontova matrica}. Prema (TODO) slijedi
    \begin{equation*}
        \det(\vec{A}_S) = \prod_{k < l} (t_{j_l} - t_{j_k})>0.
    \end{equation*}
    To zna\v{c}i da je matrica $\vec A$ invertibilna, pa posebno i injektivna. Tada je zadovoljena tvrdnja $(c)$ teorema \eqref{rekonstrukcija_tm1}, pa je po istom teoremu zadovoljena i tvrdnja $(a)$, tj. svaki $s$-rijedak vektor $\vec x \in \C^N$ zadovoljava $\vec{Az}=\vec{Ax}$. Stoga je taj vektor mogu\'ce jedinstveno rekonstruirati putem minimizacije \eqref{problem_minimizacije}.
\end{proof}
\indent Zapravo, mnogo matrica zadovoljava uvjet $(c)$ iz teorema \eqref{rekonstrukcija_tm1}. Na primjer, potencije od $t_1,\dots,t_N$ u \eqref{vandermont_matrica} ne moraju biti uzastopne. Nadalje, brojevi $t_1,\dots,t_N$ ne moraju biti pozitivni, niti realni sve dok vrijedi $\det(\vec A_S) \neq 0$. Posebno, mo\v{z}emo uzeti $t_l = e^{2\pi i (l-1)/N}$ za $l \in [N]$, teorem (TODO) garantira da parcijalna Fourierova matrica
\begin{equation*}
   \vec A = 
   \begin{bmatrix*}
       1 & 1 & 1 & \cdots & 1 \\
       1 & e^{2 \pi i/ N} & e^{2 \pi i2/ N} & \cdots & e^{2 \pi i(N-1)/ N} \\ 
       \vdots & \vdots & \vdots & \vdots & \vdots \\ 
       1 & e^{2 \pi i(2s-1)/ N} & e^{2 \pi i(2s-1)2/ N} & \cdots & e^{2 \pi i(2s-1)(N-1)/ N} \\ 
   \end{bmatrix*}
\end{equation*}
rekonstruira svaki $s$-rijedak vektor $\vec x \in \C^N$ iz $\vec y = \vec{Ax} \in \C^{2s}$.
Zapravo mo\v{z}e se pokazati da skup $(2s) \times N$ matrica takvih da $\det(\vec A_S) = 0$ za neki $S \subset [N]$ i $\card(S) \leq 2s$ ima Lebesgueovu mjeru nula, pa stoga gotovo sve $(2s) \times N$ matrice rekonstruiraju svaki $s$-rijedak vektor $\vec x \in \C^N$ iz $\vec y = \vec{Ax} \in \C^{2s}$. Me\dj utim u praksi nije isplativo rje\v{s}avati problem minimizacije \eqref{problem_minimizacije}, \v{s}to \'cemo kasnije i pokazati.




\subsection[Rekonstrukcija zadanog rijetkog vektora][Rekonstrukcija zadanog rijetkog vektora]{Rekonstrukcija zadanog rijetkog vektora}
Promatramo problem gdje je $s$-rijedak vektor $\vec x \in \C^N$ unaprijed zadan i poznat, a matricu $\vec A \in \C^{m \times N}$ \v{z}elimo odabrati tako da ona garantira rekonstrukciju vektora $\vec x$ iz mjerenja $\vec y = \vec{Ax} \in \C^m$. Isprva, ovaka pristup izgleda neprirodan zbog \v{c}injenice da je vektor $\vec x$ apriorno poznat. Ideja je da \'ce uvjeti rekonstrukcije vrijediti za gotovo sve $(s+1) \times N$ matrice, \v{s}to podupire \v{c}injenicu da se u praksi matrice mjerenja \v{c}esto odabiru na nasumi\v{c}an na\v{c}in.
\begin{thm}
Za svaki $N \geq s + 1$ i za dani $s$-rijedak vektor $\vec x \in \C^N$, postoji matrica mjerenja $\vec A \in \C^{(s+1) \times N}$, takva da se vektor $\vec x$ mo\v{z}e rekonstruirati iz mjerenja $\vec y = \vec{Ax} \in \C^m$ kao rje\v{s}enje minimizacije \eqref{problem_minimizacije}.
\end{thm}
\begin{proof}
    Neka je $\vec A \in \C^{(s+1) \times N}$ matrica za koju se $s$-rijedak vektor $\vec x$ ne mo\v{z}e rekonstruirati iz $\vec y = \vec{Ax}$ putem minimizacije \eqref{problem_minimizacije}. To zna\v{c}i da postoji vektor $\vec z \in \C^N$ razli\v{c}it od $\vec x$, takav da $S=\supp(\vec z)=\{j_1, \dots, j_s\}$, $\card(S) \leq s$ (ako je $\norm{\vec z}_0 < s$, u $S$ dodamo proizvoljne elemente $j_l \in [N]$) i $\vec{Az}=\vec{Ax}$. Ako je $\supp(\vec x) \subset S$, tada iz $\big( \vec A (\vec z - \vec x)  \big)_{[s]}=0$ slijedi da $\vec A_{[s], S}$ nije invertibilna, tj.
    \begin{equation*}
        f(a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N}) := \det(\vec A_{[s], S}) = 0.
    \end{equation*}
    Ako $\supp(\vec x) \not\subset S$ tada je dimenzija prostora $V:=\{ \vec u \in \C^N: \supp(\vec u ) \subset S \} + \C \vec x$ jednaka $s+1$, i linearno preslikavanje $G:V \rightarrow \C^{s+1}$, $\vec v \mapsto \vec{Av}$ nije invertibilno, po\v{s}to je $G(\vec z - \vec x)=0$. Matrica linearnog preslikavanja $G$ u bazi $(\vec e_{j_1}, \dots, \vec e_{j_s}, \vec x)$ prostora V, je oblika
    \begin{equation*}
        B_{\vec x, S}:=
        \begin{bmatrix*}
            a_{1, j_1} & \cdots & a_{1,j_s} & \sum_{j \in \supp(\vec x)}x_j a_{1,j} \\
            \vdots & \ddots & \vdots & \vdots \\
            a_{s+1, j_1} & \cdots & a_{s+1,j_s} & \sum_{j \in \supp(\vec x)}x_j a_{s+1,j}
        \end{bmatrix*}
    \end{equation*}
    i imamo
    \begin{equation*}
        g_S(a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N}) := \det (B_{\vec x, S})=0.
    \end{equation*}
    Dakle, vrijedi
    \begin{equation*}
        (a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N}) \in f^{-1}(\{0\}) \cup \bigcup \limits_{\card(S)=s}g_S^{-1}(\{0\}).
    \end{equation*}
    Primjetimo da su skupovi $f^{-1}(\{0\})$ i $g^{-1}_S(\{0\})$ Lebesgueove mjere nula iz razloga \v{s}to su $f$ i $g_S$ polinomi u varijablama $(a_{1,1}, \dots a_{1,N}, \dots, a_{m,1}, \dots, a_{m,N})$. Dakle, elemente matrice $\vec A$ moramo izabrati izvan skupa mjere nula, da bi osigurali rekonstrukciju vekotora $\vec x$ iz $\vec y = \vec{Ax}$.
\end{proof}

\section[NP-slo\v{z}enost $\mathbf{\ell_0}$-minimizacije][NP-slo\v{z}enost $\ell_0$-minimizacije]{NP-slo\v{z}enost $\ell_0$-minimizacije}
Kao \v{s}to smo najavili, pokazati \'cemo da je u praksi neisplativno rje\v{s}avati problem $\ell_0$-minimizacije u svrhu rekonstrukcije vektora $\vec x$ iz mjerenja $\vec y = \vec{Ax}$. Prisjetimo se, problem koji rje\v{s}avamo je oblika, 
\begin{equation*}
    \min_{\vec z \in \C^N} \norm{\vec z}_0 \quad \text{uz uvjet } \vec{Az}=\vec y. 
\end{equation*}
Po\v{s}to je minimizator najvise $s$-rijedak, najjednostavniji algoritam za rje\v{s}avanje ovog problema je rje\v{s}iti sve pravokutne sustave $\vec A_S \vec u = \vec y$ ili sve kvadratne sustave oblika $\vec A_S^* \vec A_S \vec u = \vec A_S^* \vec y$ za svaki $\vec u \in \C^S$ gdje S ide po svim poskupovima od [N], veli\v{c}ine $s$. No ispada da broj podskupova $N \choose s$, \v{s}to za male probleme sa $N = 1000$ i $s=10$, iznosi ${1000 \choose 10} \geq (\frac{1000}{10})^{10}=10^{20}$. Kada bi jedan $10 \times 10$ sustav mogli rje\v{s}iti u $10^{-10}$ sekundi, trebalo bi nam vi\v{s}e od 300 godina da sve rje\v{s}imo. Sada \'cemo pokazati za\v{s}to je zapravo op\'cenitiji problem
\begin{equation}
\min_{\vec z \in \C^N}\ \norm{\vec z}_0 \quad \text{uz uvjet }\norm{\vec{Az}- \vec{y}}_2 \leq \eta \tag{$P_{0, \eta}$}\label{problem_minimizacije_generalni}
\end{equation}
NP-te\v{z}ak.\\
\indent Uvedimo prvo potrebne pojmove iz kompleksnosti algoritama. Za algoritam ka\v{z}emo da je \textit{polinomijalnog-vremena} ako je broj koraka do rje\v{s}enja ograni\v{c}en polinomom u varijabli veli\v{c}ine ulaza. Nadalje, uvedimo neformalne definicije klasa problema odlu\v{c}ivanja:
\begin{itemize}
    \item $\mathfrak{P}$: Svi problemi odlu\v{c}ivanja za koje postoji algoritam polinomijalnog vremena koji daje rje\v{s}enje.
    \item $\mathfrak{NP}$: Svi problemi odlu\v{c}ivanja za koje postoji algoritam polinomijalnog vremena koji provjerava to\v{c}nost rje\v{s}enja.
    \item $\mathfrak{NP}$-te\v{s}ki: Svi problemi (ne nu\v{z}no problemi odre\dj ivanja) za koje se algoritam za rje\v{s}enje mo\v{z}e u polinomijalnom vremenu transformirati u algoritam rje\v{s}enja za bilo koji $\mathfrak{NP}$ problem.
    \item $\mathfrak{NP}$-potpuni: Svi problemi koji su istovremeno $\mathfrak{NP}$ i $\mathfrak{NP}$-te\v{s}ki.
\end{itemize}
Pitanje je li $\mathfrak{P}$ strogo sadr\v{z}ano u  $\mathfrak{NP}$ do dan danas nije odgovoreno. No, vjeruje se da postoje problemi za koje ne postoji algoritam rje\v{s}enja polinomijalnog vremena, ali postoji algoritam koji \'ce provjeriti to\v{c}nost rje\v{s}enja u polinomijalnom vremenu.
Najpoznatiji $\mathfrak{NP}$-potpun problem je problem putuju\'ceg prodava\v{c}a. No, iskoristiti \'cemo  problem egzaktnog pokriva\v{c}a tro\v{c}lanim skupovima da bi pokazali da je problem \eqref{problem_minimizacije_generalni} $\mathfrak{NP}$-te\v{z}ak.

\subsection[Egzaktni pokriva\v{c} tro\v{c}lanim skupovima][Egzaktni pokriva\v{c} tro\v{c}lanim skupovima]{Egzaktni pokriva\v{c} tro\v{c}lanim skupovima}
Za danu kolekciju $\{\mathcal{C}_i;\ i \in [N]\}$ tro\v{c}lanih podskupova od $[m]$, postoji li egzaktni pokriva\v{c} skupa $[m]$, tj. postoji li $J \subset [N]$ takav da $\cup_{j \in J}\mathcal{C}_j=[m]$, gdje je $\mathcal{C}_j \cap \mathcal{C}_k = \emptyset$ za svaki $j,k \in J$ razli\v{c}iti? Poznato je da je taj problem $\mathfrak{NP}$-potpun (vidi TODO).
\begin{thm}
    Za svaki $\eta \geq 0,\ \vec A \in \C^{m \times N}$ i $\vec y \in \C^m$, problem minimizacije \eqref{problem_minimizacije_generalni} je $\mathfrak{NP}$-potpun.
\end{thm}
\begin{proof}
    Zbog linearnosti problema \eqref{problem_minimizacije_generalni}, mo\v{z}emo uzeti da je $\eta < 1$. Pokazati \'cemo da se problem egzaktnog pokriva\v{c} mo\v{z}e u polinomijalnom vremenu reducirati na problem $\ell_0$-minimizacije. Neka je $\{\mathcal{C}_i;\ i \in [N]\}$ kolekcija tro\v{c}anih podskupova $[m]$. Definirajmo vektora $\vec a_1, \vec a_2,\dots \vec a_N \in \C^m$
    \begin{equation*}
        (\vec a_i)_j = 
        \begin{cases*}
            1\ \text{za } j \in \mathcal{C}_i,\\
            0\ \text{za } j \not\in \mathcal{C}_i
        \end{cases*}
    \end{equation*}
    Definiramo matricu $\vec A \in \C^{m \times N}$ i vektor $\vec y \in \C^m$ sa
    \begin{equation*}
        \vec A = [\vec a_1\ \vec a_2\ \cdots \ \vec a_N], \qquad \vec y = [1,1, \dots, 1]^T.
    \end{equation*}
    Po\v{s}to je $N \leq {m \choose 3}$, to mo\v{z}emo napraviti u polinomijalnom vremenu. Ako $\vec z \in \C^N$ zadovoljava $\norm{\vec{Az}-y}_2 \leq \eta$, tada su svih $m$ komponenti od $\vec{Az}$ udaljeljene od $1$ za najvi\v{s}e $\eta$, pa su te komponente razli\v{c}ite od nula, jer smo $\eta$ uzeli manji od $1$. Dakle, vrijedi $\norm{\vec{Az}}_0 = m$. Ali po\v{s}to svaki od vektora $\vec a_i$ imam to\v{c}no tri ne-nul komponente, vektor $\vec{Az}=\sum_{j=1}^N z_j \vec a_j$ ima najvi\v{s}e $r \norm{\vec z}_0$ ne-nul elemenata, tj. $\norm{\vec{Az}}_0 \leq 3 \norm{\vec{z}}_0$. Dakle, za svaki vektor $\vec z \in \C^N$ koji zadovoljava $\norm{\vec{Az}-\vec y}_2 \leq \eta$ vrijedi $\norm{\vec z}_0 \geq m/3$. Neka je sada $\vec x \in \C^N$ rje\v{s}enje $\ell_0$-minimizacije \eqref{problem_minimizacije_generalni}. Imamo dva slu\v{c}aj za normu vektora $\vec x$:
    \begin{enumerate}
        \item Ako je $\norm{\vec{x}}_0 = m/3$ tada je $\{\mathcal{C}_j;\ j \in \supp(\vec x)\}$ egzaktni pokriva\v{c} skupa $[m]$ jer ina\v{c}e bi neke od $m$ komponenti od $\vec{Ax}$ bile jednake od nula.
        \item Ako je $\norm{\vec{x}}_0 > m/3$ tada ne mo\v{z}e postojati egzaktni pokriva\v{c} $\{\mathcal{C}_j;\ j \in J\}$ jer bi u suprotnom vektor $\vec z \in \C^N$ definiran tako da je $z_j = 1$ ako je $j \in J$i $z_j = 0$ ako je $j \not \in J$, zadovoljavao $\vec{Az}=\vec y$ i $\norm{\vec z}_0=m/3$, \v{s}to je kontradikcija s minimalnosti vektora $\vec x$.
    \end{enumerate}
    Dakle, rje\v{s}avanjem problem $\ell_0$-minimizacije, mo\v{z}emo rje\v{s}iti problem egzaktnog pokriva\v{c}a tro\v{c}lanim skupovima, pa je stoga i sam problem $\ell_0$-minimizacije $\mathfrak{NP}$-potpun.
\end{proof}
\v{C}ini se da prethodni teorem predstavlja ozbiljnu zapreku u prakti\v{c}nom rje\v{s}avanju problema sa\v{z}etog uzorkovanja. No primjetimo, teorem tvrdi da je algoritam koji rje\v{s}ava problem $\ell_0$-minimizacije, za sve mogu\'ce matrie $\vec A$ i vektore $\vec y$ barem klase $\mathfrak{NP}$. Naravno, u samoj praksi nije nu\v{z}no zahtjevati rekonstrukciju za sve takve matrice i vektore. Naime, pokazat \'cemo da postoje algoritmi koji uspje\v{s}no rekonstruiraju $\vec x$ iz $\vec y$ za posebno dizajnirane matrice $\vec A$.




\chapter[Osnovni algoritmi sa\v{z}etog uzorkovanja][Osnovni algoritmi sa\v{z}etog uzorkovanja]{Osnovni algoritmi sa\v{z}etog uzorkovanja}\label{chapter_algoritmi}
Algoritmi za rje\v{s}avanje problema sa\v{z}etog uzorkovanja, koje \'cemo predstaviti, podijeljeni su u tri kategorije: optimizacije, greedy metode i grani\v{c}ne metode. U ovom poglavlju dati \'cemo samo pregled najpopularnijih algoritama, dok \'cemo formalnu analizu nekih od njih ostaviti za kasnije, nakon \v{s}to razvijemo potrebne teorijske alate.
\section[Optimizacijske metode][Optimizacijske metode]{Optimizacijske metode}
Op\v{c}eniti problem optimizacije je oblika
\begin{equation*}
    \min_{\vec x \in \R^N} F_0(\vec x)\quad\text{uz uvjet }F_i(\vec x) \leq b_i,\ i \in [n]
\end{equation*}
gdje $F_0:\R^N \rightarrow \R$ zovemo \textit{funkcija cilja}, a funkcije $F_1, \dots, F_n: \R^N \rightarrow \R$ zovemo \textit{funkcije ograni\v{c}enja}. Ako su $F_0, F_1, \dots, F_n$ konveksne funkcije, tada ovaj problem zovem \textit{problem konveksne optimizacije}. Ako su te funkcije linearne, tada je to \textit{problem linearnog programiranja}. Primjetimo da je problem rekonstrukcije rijetkog vektora \eqref{problem_minimizacije}, zapravo problem minimizacije. No, na\v{z}alost taj problem nije konveksan i kao \v{s}to smo u prethodnom poglavlju pokazali, op\v{c}enito je $\mathfrak{NP}$-te\v{z}ak. Prisjetimo se da $\norm{\vec z}_q^q$ konvergira k $\norm{\vec z}_0$ za $q \rightarrow 0^+$, pa je prirodno  \eqref{problem_minimizacije} aproksimirati problemom
\begin{equation}
    \min \norm{\vec z}_q\quad\text{uz uvjet }\vec{Az}=\vec y\tag{$P_{q}$}\label{problem_minimizacije_aprox}
\end{equation}
Poka\v{z}e se da za $q > 1$, \v{c}ak $1$-rijetki vektori nisu rje\v{s}enja od \eqref{problem_minimizacije_aprox}. Dok za $0 < q < 1$, \eqref{problem_minimizacije_aprox} ponovno nije konveksan i dalje je op\v{c}enito $\mathfrak{NP}$-te\v{z}ak. Za $q=1$, problem postaje konveksan
\begin{equation}
    \min \norm{\vec z}_1 \quad \text{uz uvjet }\vec{Az}=\vec y.\tag{$P_{1}$}\label{problem_minimizacije_l1}
\end{equation}
To je zapravo konveksna relaksacija problema \eqref{problem_minimizacije} i zovemo ga $\ell_1$\textit{-minimizacija} ili \textit{BP} algoritam (eng. \textit{basis pursuit}).

\begin{alg}{$\ell_1$-minimizacija (BP)}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$. \\
    \textit{Problem:}
        \begin{equation}
            \vec x^{\sharp} = \argmin \norm{\vec z}_1 \quad \text{uz uvjet }\vec{Az}=\vec y\tag{$\ell_1-min$}\label{algoritam_l1_minimizacija}
        \end{equation} \\
        \textit{Izlaz:} vektor $\vec x^{\sharp}$
\end{alg}

\noindent Poka\v{z}imo sada da su $\ell_1$-minimizatori rijetki vektori u realnom slu\v{c}aju.
\begin{thm}
    Neka je $\vec A \in \R^{m \times N}$ matrica mjerenja sa stupcima $\vec a_1, \dots, \vec a_N$. Ako je $\vec x^{\sharp}$ minimizator od
    \begin{equation*}
        \min_{\vec z \in \R^N} \norm{\vec z}_1\quad \text{uz uvjet } \vec{Az}=\vec y,
    \end{equation*}
    tada je skup $\{\vec a_j,\ j \in \supp(\vec x^{\sharp})\}$ linearno nezavisan i vrijedi
    \begin{equation*}
        \norm{\vec{x}^{\sharp}}_0 = \card(\supp(\vec x^{\sharp})) \leq m. 
    \end{equation*}
\end{thm}
\begin{proof}
    Pretpostavimo suprotno, tj. da je skup $\{\vec a_j,\ j \in \supp(\vec x^{\sharp})\}$ linearno zavisan. Neka je $S= \supp(\vec x^{\sharp})$. To zna\v{c}i da postoji ne-nul vektor $\vec v \in \R^N$ sa nosa\v{c}em na $S$ takav da $\vec{Av} = \vec 0$. Tada za svaki $t \not= 0$
    \begin{equation*}
        \norm{\vec x^{\sharp}}_1 < \norm{\vec x^{\sharp} + t \vec v}_1 = \sum_{j \in S}|x_j^{\sharp} + tv_j| = \sum_{j \in S}\sgn(x_j^{\sharp}+tv_j)(x_j^{\sharp}+tv_j)
    \end{equation*}
    Ako je $|t|$ dovoljno mali, tj. $|t| < \min_{j \in S}|x_j^{\sharp}|/ \norm{\vec v}_{\infty}$ onda vrijedi
    \begin{equation*}
        \sgn(x_j^{\sharp}+tv_j) = \sgn(x_j^{\sharp})\quad \text{za svaki }j \in S.
    \end{equation*}
    Dakle, za $0<|t|<\min_{j \in S}|x_j^{\sharp}|/ \norm{\vec v}_{\infty}$ slijedi
    \begin{equation*}
    \begin{split}
        \norm{\vec x^{\sharp}}_1 & <  \sum_{j \in S} \sgn(x_j^{\sharp})(x_j^{\sharp}+tv_j)=\sum_{j \in S} \sgn(x_j^{\sharp})(x_j^{\sharp})+t \sum_{j \in S}\sgn(x_j^{\sharp})v_j \\ &= \norm{\vec x^{\sharp}}_1 + t \sum_{j \in S}\sgn(x_j^{\sharp})v_j.
    \end{split}
    \end{equation*}
    No, to je kontradikcija jer $t \not = 0$ mo\v{z}emo odabrati dovoljno mali tako da je \\ $t \sum_{j \in S}\sgn(x_j^{\sharp})v_j \leq 0$.
\end{proof}

\indent
U realnom slu\v{c}aju, \eqref{problem_minimizacije_l1} mo\v{z}emo reinterpretirati kao problem linearnog programiranja, tako da uvedemo pomo\v{c}ne varijable $\vec z^+,\ \vec z^- \in \R^N$ definirane sa
\begin{multicols}{2}
    \noindent
    \begin{equation*} 
        z_j^+ = 
        \begin{cases}
            z_j\ & \text{za } z_j > 0, \\
            0\ & \text{za } z_j \leq 0
        \end{cases}
    \end{equation*}
    \begin{equation*} 
        z_j^- = 
        \begin{cases}
            0\ & \text{za } z_j > 0, \\
            -z_j\ & \text{za } z_j \leq 0
        \end{cases}
    \end{equation*}
\end{multicols}
za svaki $j \in [N]$. Tada je problem \eqref{problem_minimizacije_l1} ekvivaltan problemu
\begin{equation}
    \min_{\vec z^+,\vec z^- \\ \in \R^N} \sum_{j=1}^{N}(z_j^+ + z_j^-) \quad \text{uz uvjet }
    \begin{bmatrix*}
        \vec A & -\vec A
    \end{bmatrix*}
    \begin{bmatrix*}
        \vec z^+ \\ \vec z^-
    \end{bmatrix*}
    = \vec y, \quad
    \begin{bmatrix*}
        \vec z^+ \\ \vec z^-
    \end{bmatrix*}
    \geq 0.\tag{$P_1'$}
\end{equation}
Isto ne vrijedi za kompleksni slu\v{c}aj. Tu \v{c}injenicu pokazati \'cemo na op\'{c}enitijim problemu,
\begin{equation}\label{problem_minimizacije_l1_kvadraticni}
    \min \norm{\vec z}_1 \quad \text{uz uvjet } \norm{\vec{Az}-y}_2 \leq \eta.\tag{$P_{1,\eta}$}
\end{equation}
Taj problem je zapravo pogodniji za praksu, po\v{s}to vektor $\vec y \in \C^m$ ne mo\v{z}emo izmjeriti s beskona\v{c}nom to\v{c}no\v{s}\'cu, ve\'c uz neku gre\v{s}ku $\vec e \in \C^m$ pa je stoga
\begin{equation*}
    \vec y = \vec{Ax} + \vec e. 
\end{equation*}
Takvoj gre\v{s}ki \v{c}esto mo\v{z}emo ocjeniti $\ell_2$-normu, po\v{s}to ona ima interpretaciju energije,
\begin{equation*}
    \norm{\vec e}_2 \leq \eta, \quad \text{za neki } \eta > 0.
\end{equation*}
Za dani vektor $\vec z \in \C^N$, neka su $\vec u,\ \vec v \in \R^N$ njegovi realni i imaginarni djelovi te neka je $\vec c \in \R^N$ takav d je $c_j \geq |z_j| = \sqrt{u_j^2+v_j^2}$ za sve $j \in [N]$. Problem \eqref{problem_minimizacije_l1_kvadraticni} je tada ekvivaltan problemu
\begin{equation}
\begin{split}
    \min_{\vec c, \vec u, \vec v \in \R^N}\sum_{j=1}^N c_j\quad \text{uz uvjete }& 
    \norms{
        \begin{bmatrix*}
            \Re(\vec A) & -\Im(\vec A) \\
            \Im(\vec A) & \Re(\vec A)
        \end{bmatrix*}
        \begin{bmatrix*}
           \vec u \\ \vec v 
        \end{bmatrix*}
        -
        \begin{bmatrix*}
            \Re(\vec y) \\ \Im(\vec y) 
    \end{bmatrix*} }_2 \leq \eta \\
    &\sqrt{u_j^2 + v_j^2} \leq c_j,\quad \forall j \in [N].
\end{split}\tag{$P_{1, \eta}'$}\label{problem_minimizacije_l1_kvadraticni_drugi_oblik}
\end{equation}
Ovo je \textit{problem konike drugog reda}. Primjetimo da za $\eta=0$ dobivamo formulaciju problema \eqref{problem_minimizacije_l1} za kompleksni slu\v{c}aj u takvom obliku.
\\\indent Princip rje\v{s}avanja \eqref{problem_minimizacije_l1_kvadraticni} zove se \textit{kvadrati\v{c}no ograni\v{c}ena $\ell_1$-minimizacija} ili \textit{$\ell$-minimizacija osjetljiva na \v{s}um} (eng. \textit{quadratically constrainted basis pursuit}).
\begin{alg}{Kvadrati\v{c}no ograni\v{c}ena $\ell_1$-minimizacija}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, razina \v{s}uma $\eta$. \\
    \textit{Problem:}
        \begin{equation}
            \vec x^{\sharp} = \argmin \norm{\vec z}_1 \quad \text{uz uvjet }\norm{\vec{Az}-y}_2 \leq \eta\tag{$\ell_1-min_{\eta}$}\label{algoritam_l1_minimizacija_kvadraticna}
        \end{equation} \\
        \textit{Izlaz:} vektor $\vec x^{\sharp}$
\end{alg}
Rje\v{s}enje $\vec x^{\sharp}$ povezano je s rje\v{s}enjem problema $\ell_1$-minimizacije sa ugra\dj enim uklanjanjem \v{s}uma
\begin{equation}\label{problem_minimizacije_l1_sum}
    \min_{\vec z \in \C^N} \lambda\norm{\vec z}_1 + \norm{\vec{Az}-\vec y}_2^2
\end{equation}
za neki $\lambda \geq 0$. Tako\dj er povezano je s rje\v{s}enjem \textit{LASSO} problema, za neki $\tau \geq 0$,
\begin{equation}\label{lasso}
    \min_{\vec z \in \C^N} \norm{\vec{Az}-\vec y}_2\quad \text{uz uvjet } \norm{\vec z}_1\leq \tau
\end{equation}
To upravo tvrdi naredna propozicija.
\begin{prop}
    \begin{enumerate}[label=(\alph*)]
        \item Ako je $\vec x$ minimizator problema \eqref{problem_minimizacije_l1_sum} sa $\lambda > 0$, onda postoji $\eta = \eta_{\vec x} \geq 0$ takva da je $\vec x$ minizator kvadrati\v{c}no ograni\v{c}ene $\ell_1$-minimizacije \eqref{problem_minimizacije_l1_kvadraticni}.
        \item Ako je $\vec x$ jedinstveni minimizator problema \eqref{problem_minimizacije_l1_kvadraticni} sa $\eta \geq 0$, onda postoji $\tau = \tau_{\vec x} \geq 0$ takav da je $\vec x$ minimizator LASSO problema \eqref{lasso}.
        \item Ako je $\vec x$ minimizator LASSO problema \eqref{lasso}, onda postoji $\lambda = \lambda_{\vec x} \geq  0$ takva da je $\vec x$ minimizator problema \eqref{problem_minimizacije_l1_sum}.
    \end{enumerate}
\end{prop}
\begin{proof}
    \begin{enumerate}[label=(\alph*)]
        \item Neka je $\eta := \norm{\vec{Ax}- \vec y}_2$ i $\vec z \in \C^N$ takav da je $\norm{\vec{Az-y}}_2 \leq \eta$. Po\v{s}to je prema pretpostavci $\vec x$ minimizator od \eqref{problem_minimizacije_l1_sum} slijedi,
            \begin{equation*}
                \lambda \norm{\vec x}_1 + \norm{\vec{Ax} -\vec{y}}_2^2 \leq \lambda \norm{\vec z}_1 + \norm{\vec{Az} - \vec{y}}_2^2 \leq \lambda \norm{\vec z}_1 + \norm{\vec{Ax}-\vec y}_2^2.
            \end{equation*}
            Dakle slijedi da je $\norm{\vec x}_1 \leq \norm{\vec{y}}_1$, pa je $\vec x$ minimizator problema \eqref{problem_minimizacije_l1_kvadraticni}

        \item Neka je $\eta := \norm{\vec x}_1$ i neka je $\vec z \in \C^N\backslash\{\vec x\}$ takav da je $\norm{\vec z}_1 \leq \tau$. Po\v{s}to je $\vec x$ jedinstveni minimizator od \eqref{problem_minimizacije_l1_kvadraticni} to zna\v{c}i da $\vec z$ ne mo\v{z}e zadovoljavati uvjet iz \eqref{problem_minimizacije_l1_kvadraticni}, pa stoga $\norm{\vec{Az}- \vec{y}}_2 > \eta \geq \norm{\vec{Ax}-\vec y}_2$. Dakle, $\vec x$ je jedinstveni minimizator \textit{LASSO} problema.
        \item Za dokaz ove tvrdnje potrebni su alati konveksne analize, vidi (TODO).
    \end{enumerate}
\end{proof}

\section[Greedy metode][Greedy metode]{Greedy metode}
Upoznati \'cemo se sa dva iterativna greedy algoritma koji se \v{c}esto koriste u kontekstu sa\v{z}etog uzorkovanja. Prvo algoritam koji \'cemo prou\v{c}iti zove se \textit{OMP} (skra\v{c}enica od eng. \textit{orthogonal matching pursuit}).
\begin{alg}{OMP}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$. \\
    \textit{Inicijalizacija:} $S^0 = \emptyset$, $\vec x^0 = \vec 0$ \\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{align}
            &S^{n+1} = S^n \cup \{j_{n+1}\},\quad j_{n+1} := \argmax\limits_{j \in [N]}\{|(\vec A^*(\vec y - \vec{Ax}^n))_j|\},\tag{$OMP_1$}\label{omp_1}
        \\
            &\vec x^{n+1} = \argmin\limits_{\vec z \in \C^N}\{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset S^{n+1}\}.\tag{$OMP_2$}\label{omp_2}
        \end{align} \\
        \textit{Izlaz:} $\bar{n}$-rijedak vektor $\vec x^{\sharp}=\vec{x}^{\bar{n}}$.
\end{alg}

Numeri\v{c}ki najskuplja operacija ovog algoritma je \eqref{omp_2}. Situacije se mo\v{z}e popraviti kori\v{s}tenjem $QR$ dekompozicije matrice $\vec{A}_{S_n}$. Tada se mogu iskoristiti efikasni algoritmi za a\v{z}uriranje $QR$ dekompozicije kada se u matricu doda novi stupac. Nadalje, za dodatna ubrzanja mogu se iskoristiti i algoritmi za brzo matrica-vektor mno\v{z}enje bazirani na brzoj Fourierovoj transformaciji (vidi TODO).
\newline\indent
Indeks $j_{n+1}$ bira se tako da se reducira $\ell_2$-norma reziduala $\vec{y} - \vec{Ax}^n$ \v{s}to je vi\v{s}e mogu\'ce. Sljede\'ca lema opravdava za\v{s}to je smisleno $j$ odabrati takav da maksimizira vrijednost $|{(\vec{A}^*(\vec{y}-\vec A \vec x^n))_j}|$.
\begin{lem}
    Neka je $\vec A \in \C^{m \times N}$ sa $\ell_2$-normaliziranim stupcima. Ako su $S \subset [N]$, $\vec v \in \C^N$ sa nosa\v{c}em na $S$, $j \in [N]$, te ako vrijedi
    \begin{equation*}
        \vec w := \argmin_{\vec z \in \C^N} \{ \norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset S \cup \{j\} \},
    \end{equation*}
    tada
    \begin{equation*}
        \norm{\vec y - \vec{Aw}}_2^2 \leq \norm{\vec y - \vec{Av}}_2^2 - |{(\vec{A}^*(\vec y - \vec{Av}))_j|^2.
    \end{equation*}
\end{lem}
\begin{proof}
    Po\v{s}to svaki vektor oblika $\vec v + t \vec e_j,\ t \in \C$ ima nosa\v{c} u $S \cup \{j\}$ vrijedi,
    \begin{equation*}
        \norm{\vec y - \vec{Aw}}_2^2 \leq \min_{t \in \C} \norm{\vec y - \vec{A}(\vec v + t \vec e_j)}_2^2
    \end{equation*}
    Stavimo da je $t = \rho e^{i \theta}$, gdje je $\rho \geq 0$ i $\theta \in [0,2 \pi)$. Imamo,
    \begin{align*}
        \norm{\vec y - \vec{A}(\vec v + t \vec e_j)}_2^2 &= \norm{\vec y - \vec{Av} - t \vec{A}\vec{e}_j}_2^2\\
        &= \norm{\vec y - \vec{Av}}_2^2 + |t|^2 \norm{\vec{Ae}_j}_2^2 - 2 \Re(\bar{t}\langle \vec{y} - \vec{Av}, \vec{Ae}_j \rangle)\\
        &= \norm{\vec y - \vec{Av}}_2^2 + \rho^2 - 2 \Re(\rho e^{-i \theta}(\vec{A}^*(\vec y - \vec{Av}))_j)\\
        & \geq \norm{\vec y - \vec{Av}}_2^2 + \rho^2 - 2 \rho |{(\vec{A}^*(\vec y - \vec{Av}))_j}|^2
    \end{align*}
    gdje jednakost vrijedi za pogodno odabrani $\theta$. Kao kvadratni polinom u varijabli $\rho$, zadnji izraz poprima minimum za $\rho = |{(\vec{A}^*(\vec y - \vec{Av}))_j}|$.
\end{proof}

Korak \eqref{omp_2} mo\v{s}e se prikazati u obliku
\begin{equation*}
    \vec{x}_{S^{n+1}}^{n+1} = \vec{A}_{S^{n+1}}^{\dagger}\vec y,
\end{equation*}
gdje je $\vec{x}_{S^{n+1}}^{n+1}$ restrikcija od $\vec x^{n+1}$ na svoj nosa\v{c} $S^{n+1}$ i gdje je $\vec A^{\dagger}_{S^{n+1}}$ pseudo-inverz od $\vec{A}_{S^{n+1}}$ (vidi TODO). Drugim rje\v{c}ima to zna\v{c}i da je $\vec z = \vec x ^{n+1}_{S^{n+1}}$ rje\v{s}enje sustava $\vec A^*_{S^{n+1}} \vec A_{S^{n+1}} \vec z = \vec A^*_{S^{n+1}} \vec y$. Ta \v{c}injenica je korisna i u drugim algoritmima koji imaju korak sli\v{c}an \eqref{omp_2}.
\begin{lem}\label{omp_lema_1}
    Neka je $S \subset [N]$ i
    \begin{equation*}
        \vec v := \argmin_{\vec z \in \C^N} \{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset S\},
    \end{equation*}
    tada je
    \begin{equation}\label{omp_ortogonalnost}
        (\vec A^*(\vec y - \vec{Av}))_S = \vec 0.
    \end{equation}
\end{lem}
\begin{proof}
    Prema definiciji vektora $\vec v$, vektor $\vec{Av}$ je orthogonalna projekcija vektora $\vec y$ na prostor $\{\vec{Az},\ \supp(\vec{z} \subset S)\}$, pa je karakteriziran relacijom ortogonalnosti
    \begin{equation*}
        \langle \vec y - \vec{Av}, \vec{Az} \rangle = 0 \quad \text{za sve }\vec z \in \C^N \ \text{takve da } \supp(\vec z) \subset S.
    \end{equation*}
    Dakle, imamo da vrijedi $\langle \vec A^*( \vec y - \vec{Av}), \vec{z} \rangle = 0$ za sve $\vec z \in \C^N,\ \supp(\vec z) \subset S$, \v{s}to vrijedi ako i samo ako vrijedi \eqref{omp_ortogonalnost}.
\end{proof}

Prirodan uvjet zaustavljanja OMP-a je kada se postigne $\norm{\vec{y} - \vec{Ax}^{\bar{n}}} \leq \varepsilon$ ili $\norm{\vec A^*(\vec y - \vec{Ax}^{\bar{n}})_{\infty}} \leq \varepsilon$ za neku toleranciju $\varepsilon > 0$. Ako nam je dostupna estimacija rijetkosti $s$ rje\v{s}enja $\vec x$, tada je razumno stati kada je $\bar{n} = s$. Sljede\'ci rezultat govori o uvjetim za uspje\v{s}nu rekonstrukciju $s$-rijetkog vektora u $s$ iteracija OMP algoritma.

\begin{prop}
    Neka je $\vec A \in \C^{m \times N}$, svaki ne-nul vektor $\vec x \in \C^N$ sa nosa\v{c}emo na skupu $S$, kardinaliteta $s$ mo\v{z}e se rekonstruirati iz $\vec y = \vec{Ax}$ u najvi\v{s}e $s$ iteracija OMP algoritma ako i samo ako je matrica $\vec A_S$ injektivna i 
    \begin{equation}\label{uvjet_rekon_omp}
        \max_{j \in S}|(\vec A^* \vec r)_j| > \max_{l \in \bar{S}}|(\vec A^* \vec r)_l|
    \end{equation}
    za sve ne-nul $\vec r \in \{\vec{Az},\ \supp(\vec z) \subset S\}$.
\end{prop}
% TODO nije mi bas jasno ovo
\begin{proof}
    Pretpostavimo da OMP algoritam rekonstruira sve vektore sa nosa\v{c}emo na skupu $S$ u najvi\v{s}e $s = \card(S)$ iteracija. Neka su $\vec v, \vec w$ sa nosa\v{c}em na $S$, takvi da je $\vec{Av}=\vec{Aw}$. Zbog pretpostavke, $\vec v$ i $\vec w$ moraju biti jednaki, a to zna\v{c}i da je matrica $\vec A_S$ injektivna. Nadalje, ako je $\vec y = \vec{Ax}$ za neki $\vec x \in \C^N$ sa $\supp(\vec x)=S$, indeks $l \in \bar S$ ne mo\v{z}e biti izabran u prvoj iteraciji, po\v{s}to indeks izabran u prvoj iteraciji ostaje uvijek u nosa\v{c}u, a po pretpostavci OMP rekonstruira $\vec x$ iz $\vec y = \vec{Ax}$ u to\v{c}no $s$ iteracija. Dakle za $n=0$ iz \eqref{omp_1} imamo da je $\max_{j \in S}|(\vec A^*y)_j| > |(\vec A^*y)_l|$ za svaki $l \in \bar{S}$, pa stoga vrijedi $\max_{j \in S}|(\vec A^*y)_j| > \max_{l \in \bar{S}}|(\vec A^*y)_l|$ za sve ne-nul $\vec y \in \{\vec{Az},\ \supp(\vec z) \subset S\}$. \\
    \indent
    Obratno, pretpostavimo da je $\vec{Ax}^1 \neq y,\dots,\vec{Ax}^{s-1} \neq y$ jer u suprotnom nemamo \v{s}to dokazivati. Pokazati \'cemo da $S^n \subset S,\ \card(S^n)=n$ za $0 \leq n \leq s$. To \'ce implicirati $S^s = S$. Nadalje, \eqref{omp_2} daje $\vec{Ax}^s = \vec y$ a iz injektivnosti od $\vec{A}_S$ slijedi $\vec x_s = \vec{x}$. Dakle, neka je $0 \leq n \leq s-1$. Ako je $S^n \subset S$, to povla\v{c}i da je $\vec r^n := \vec y - \vec{Ax}^n \in \{\vec{Az},\ \supp(\vec z) \subset S\}$, pa prema \eqref{uvjet_rekon_omp} indeks $j_{n+1}$ le\v{z}i u S, pa $S^{n+1} = S \cup \{j_{n+1}\} \subset S$. Ovo induktivno pokazuje da je $S^n$ podskup od $S$ za svaki $0 \leq n \leq s$. Nadalje, neka je $1 \leq n \leq s-1$. Lema \eqref{omp_lema_1} daje $(\vec{A}^* \vec r^n)_{S^n} = \vec 0$. Stoga, iz \eqref{omp_1} vidimo da indeks $j_{n+1}$ ne le\v{z}i u $S^{n}$, jer bi u protivnom $\vec A^* \vec r^n = \vec 0$, a po \eqref{uvjet_rekon_omp} $\vec r^n = \vec 0$. Dakle, $\card(S^n)=n$.
\end{proof}
\indent
Slabost OMP algoritma le\v{z}i u \v{c}injenici da ako krivi indeks u\dj e u nosa\v{c}, on ostaje u nosa\v{c}u u svim sljede\'cim iteracijama. Stoga $s$ iteracija algoritma nije dovoljno za rekonstrukciju vektora koji je $s$-rijedak. Mogu\'ce rje\v{s}enje je pove\'cati broj iteracija. Naredni algoritam, CoSaMP (eng. \textit{compressive sampling matching pursuit algorithm}), koristi druga\v{c}iju strategiju kada nam je dostupna estimacija rijetkosti $s$. Uvedimo oznake $H_s(\vec z)$ za najbolju $s$-rijetku aproksimaciju vekotra $\vec z \in \C^N$ i $L_s(\vec z)$ za nosa\v{c} od $H_s(\vec z)$, tj.
\begin{align}
    &L_s(\vec z) := \text{skup indeksa $s$ najve\'cih komponeneti vekora } \vec z \in \C^N \\
    &H_s(\vec z) := \vec z_{L_s(\vec z)}.
\end{align}
Nelinearni operator $H_s$ zovemo \textit{hard thresholding} operator reda $s$. Za dani vektor $\vec z \in \C^N$ on pu\v{s}ta $s$ apsolutno najve\'cih komponeneti a ostale postavi na nulu. Primjetimo da to nije nu\v{z}no jedinstveno definiramo. Da bi zaobi\v{s}li taj problem, skup indeksa $L_s(\vec z)$ biramo iz svih mogu\'cih kandidata leksikografskim poredkom.

\begin{alg}{CoSaMP}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Inicijalizacija:} $s$-rijedak vektor $\vec x^0$ (npr. $\vec x^0 = \vec 0$).\\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{align*}
            U^{n+1} & = \supp(\vec x^n)\cup L_{2s}(\vec A^*(\vec y - \vec{Ax}^n))  \tag{$CoSaMP_1$}\label{cosamp_1}\\
            \vec u^{n+1} & = \argmin_{\vec z \in \C^N}\{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset U^{n+1}\}  \tag{$CoSaMP_2$}\label{cosamp_2}\\
            \vec x^{n+1} & = H_s(\vec u^{n+1})  \tag{$CoSaMP_3$}\label{cosamp_3}
        \end{align*}
        \textit{Izlaz:} $\bar{n}$-rijedak vektor $\vec x^{\sharp}=\vec{x}^{\bar{n}}$.
\end{alg}



\section[Grani\v{c}ne metode][Grani\v{c}ne metode]{Grani\v{c}ne metode}
Algoritmi predstavljeni u ovom poglavlju tako\dj er koriste \textit{hard thresholding} operator $H_s$. Prvi algoritam, BT (eng. \textit{basic thresholding}), sastoji se od odre\dj ivanja nosa\v{c}a $s$-rijetkog vektora $\vec x \in \C^N$, koji se rekonstruira iz $\vec y = \vec{Ax} \in \C^m$, kao indeksi $s$ najve\'cih komponenti vektora $\vec A^* \vec y$, te tra\v{z}enja vektora koji najbolje aproksimira mjerenje $\vec y$

\begin{alg}{BT}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Problem:}
        \begin{align*}
            S^{\sharp} &= L_s(\vec A^* \vec y),\tag{$BT_1$}\label{bt_1}\\
            \vec x^{\sharp} &= \argmin_{\vec z \in \C^N}\{\norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset S^{\sharp}\}.\tag{$BT_2$}\label{bt_2}\\
        \end{align*}
        \textit{Izlaz:} $s$-rijedak vektor $\vec x^{\sharp}$.
\end{alg}

\noindent Dovoljni i nu\v{z}i uvjeti rekonstrukcije jednostavnim BT algoritmom, sli\v{c}ni su uvjetu \eqref{uvjet_rekon_omp}.
\begin{prop}
    BT algoritam rekonstruira vektor $\vec x \in \C^N$ sa nosa\v{c}em na $S$, iz $\vec y = \vec{Ax}$ ako i samo ako
    \begin{equation}\label{bt_uvjet}
        \min_{j \in S}|(\vec A^* \vec y)_j| > \max_{l \in \bar{S}} |(\vec A^* \vec y)_l| .
    \end{equation}
\end{prop}
\begin{proof}
    Vektor $\vec x$ mo\v{z}e se rekonstruirati ako i samo ako skup indeksa $S^{\sharp}$ u \eqref{bt_1} jednak skupu $S$. A to vrijedi ako i samo ako je element vektora $\vec A^* \vec y$ s indeksom iz $S$, ve\'ci od svakog elementa vektora $\vec A^* \vec y$ s indeksom u $\bar{S}$.
\end{proof}
\indent
IHT (eng. \textit{iterative hard thresholding}) algoritam rje\v{s}ava kvadratni sustav $\vec A^* \vec A \vec z= \vec A^* \vec y$ umjesto $\vec{Az}=\vec y$. To mo\v{z}emo interpretirati kao rje\v{s}avanje problema fiksne to\v{c}ke $\vec z = (\vec{I}- \vec A^* \vec A ) \vec z + \vec A^* \vec y$. Prirodno je gledati iteracije oblika $\vec x^{n+1} = (\vec{I}- \vec A^* \vec A) \vec x^n + \vec A^* \vec y$. Po\v{s}to tra\v{z}imo $s$-rijetko rje\v{s}enje u svakoj iteraciji uzimamo samo $s$ apsolutno najve\'cih komponenti od $(\vec{I} - \vec A^* \vec A ) \vec x^n + \vec A^* \vec y$.

\begin{alg}{IHT}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Inicijalizacija:} $s$-rijedak vektor $\vec x^0$ (npr. $\vec x^0 = \vec 0$).\\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{equation}
            x^{n+1} = H_s(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n).\tag{$IHT$}\label{iht}\\
        \end{equation}
        \textit{Izlaz:} $s$-rijedak vektor $\vec x^{\sharp}=\vec x^{\bar n}$.
\end{alg}

Primjetimo da IHT algoritam ne koristi orthogonalne projekcije, \v{s}to je njegova prednost. No, ako smo spremi platiti cjenu projekcija, ima smisla gledati vektor koji ima isti nosa\v{c} kao $\vec x^{n+1}$ koji najbolje aproksimira mjerenje. Upravo je to strategija HTP (eng. \textit{hard thresholding pursuit}) algoritma.

\begin{alg}{HTP}
    \textit{Ulaz:} Matrica mjerenja $\vec A$, vektor mjerenja $\vec y$, rijetkost $s$ \\
    \textit{Inicijalizacija:} $s$-rijedak vektor $\vec x^0$ (npr. $\vec x^0 = \vec 0$).\\
    \textit{Iteracija:} Zaustavi kada $n = \bar{n}$:
        \begin{align*}
            S^{n+1} &= L_s(\vec x^n + \vec A^* (\vec y - \vec{Ax}^n),\tag{$HTP_1$}\label{htp_1}\\
            \vec x^{n+1} &= \argmin_{\vec z \in \C^N}\{ \norm{\vec y - \vec{Az}}_2,\ \supp(\vec z) \subset S^{n+1} \}.\tag{$HTP_2$}\label{htp_2}
        \end{align*}
        \textit{Izlaz:} $s$-rijedak vektor $\vec x^{\sharp}=\vec x^{\bar n}$.
\end{alg}





\chapter[$\ell_1$-minimizacija][$\ell_1$-minimizacija]{$\ell_1$-minimizacija}
Prisjetimo se, problem sa\v{z}etog uzorkovanja sastoji se od rekonstrukcije $s$-rijetkog vektora $\vec x \in \C^N$ iz mjerenja $\vec y = \vec{Ax} \in \C^m$, gdje je $m < N$. Prirodno se name\'ce problem $\ell_0$-minimizacije,
\noindent
\begin{equation}
\min_{\vec z \in \C^N} \norm{\vec z}_0\quad \text{uz uvjet }\vec{Az} = \vec{y}\tag{$P_0$}
\end{equation}
U poglavlju \eqref{chapter_rijetka_rijesenja} vidjeli smo da je taj problem op\'cenito $\mathfrak{NP}$-te\v{z}ak. U poglavlju \eqref{chapter_algoritmi} pokazali smo nekoliko u\v{c}inkovitih strategija za rje\v{s}avanje problema sa\v{z}etog uzorkovanja. U ovom poglavlju fokusirati \'cemo se na strategiju $\ell_1$-minimizacije
\begin{equation}
    \min_{\vec z \in \C^N} \norm{\vec z}_1 \quad \text{uz uvjet }\vec{Az}=\vec y.\tag{$P_1$}
\end{equation}
Prou\v{c}iti \'{c}emo uvjete na matricu $\vec A$ koji osiguravaju egzaktnu ili aproksimativnu rekonstrukciju vektora $\vec x$.

\section[Svojstvo nul-prostora][Svojstvo nul-prostora]{Svojstvo nul-prostora}
Argumenti u ovom potpoglavlje vrijede u oba kontekstu realnih i u konteksu kompleksnih prostora. Stoga \'cemo rezultate prvo iznjeti za polje $\K$, koje mo\v{z}e $\R$ ili $\C$. Nakon toga uspostaviti \'cemo ekvivalentnost realnog i kompleksnog svojstva nul-prostora.

\begin{defn}
    Za matricu $\vec A \in \K^{m \times N}$ ka\v{z}emo da zadovoljava \textit{svojstvo nul-prostora} za skup $S \subset [N]$ ako
    \begin{equation}\label{svojstvo_nul_prostora}
        \norm{\vec v_S}_1 < \norm{\vec v_{\bar{S}}}_1  \quad \text{za svaki }\vec v \in \ker \vec A \backslash \{\vec 0\}.
    \end{equation}
    Nadalje, ka\v{z}emo da $\vec A$ zadovoljava svojstvo nul-prostora reda $s$ ako zadovoljava gornju nejednakost za svaki $S \subset [N]$ takav da $\card(S) \leq s$.
\end{defn}

Primjetimo da za vektor $\vec v \in \ker \vec A \backslash \{ \vec 0\}$ svojstvo nul-prostora vrijedi za svaki $S \subset [N]$ takav da $\card(S) \leq s$, \v{c}im vrijedi za skup indeksa $s$ apsolutno najve\'cih komponenti vektora $\vec v$. \\
\indent Postoje dvije dodatne formulaciju svojsta nul-prostora. Prvu dobijemo tako da gornjoj nejednakosti dodamo $\norm{\vec v_s}_1$ s obje strane. Tada imamo
\begin{equation}\label{svojstvo_nul_prostora_form_1}
    2 \norm{\vec v_S}_1 < \norm{\vec v}_1 \quad \text{za svaki } \vec v \in \ker \vec A \backslash \{\vec 0\}.
\end{equation}
Drugu dobijemo tako da u skup $S$ stavimo $s$ apsolutno najve\'cih  komponenti vektora $\vec v$ i ovaj put nejednakosti dodamo $\norm{\vec v_{\bar S}}_1$ s obje strane. Tada imamo
\begin{equation}\label{svojstvo_nul_prostora_form_2}
    \norm{\vec v}_1 < 2 \sigma_s(\vec v)_1 \quad \text{za sve } \vec v \in \ker \vec A \backslash \{\vec 0\}.
\end{equation}
Prisjetimo se definicije \ref{greska_naj_s_aprox} $\ell_p$-gre\v{s}ke najbolje $s$-rijetke aproksimacija vektora $\vec x \in \K^N$,
\begin{equation*}
    \sigma_s(\vec x)_p = \inf_{\norm{\vec z} \leq s} \norm{\vec x - \vec z}_p.
\end{equation*}
Sljede\'ci teorem govori o veci svojstva nul-prostora i egzaktne rekonstrukcije rijetkog vektora putem $\ell_1$-minimizacije.
\begin{thm}\label{bp_tm1}
    Za $\vec A \in \K^{m \times N}$, svaki vektor $\vec x \in \K^N$ sa nosa\v{c}em na $S$ je jedinstveno rje\v{s}enje od \eqref{problem_minimizacije_l1} sa $\vec y = \vec {Ax}$ ako i samo ako $\vec A$ zadovoljava svojstvo nul-prostora za skup $S$.
\end{thm}
\begin{proof}
    Neka je skup indeksa $S$ fiksan. Pretpostavimo da je svaki vektor $\vec x \in \K^N$ s nosa\v{c}em na $S$ jedinstveni minimizator od $\norm{\vec z}_1$ takav da $\vec {Az} = \vec {Ax}$. Stoga za svaki $\vec v \in \ker \vec A \backslash \{\vec 0\}$, vektor $\vec v_S$ je jedinstveni minimizator od $\norm{\vec z}_1$ takav da $\vec{Az} = \vec{Av}_S$. Ali imamo $\vec A (-\vec v_{\bar S}) = \vec A \vec v_S$ i $-\vec v_{\bar S} \neq \vec v_S$ jer je $\vec v \neq \vec 0$ i $ \vec 0 = \vec{Av} = \vec A (\vec v_S + \vec v_{\bar S})$. Dakle, mora vrijediti $\norm{\vec v_S}_1 < \norm{\vec v_{\bar S}}_1$.
\indent
Obratno, pretpostavimo da $\vec A$ zadovoljava svojstvo nul-prostora za skup $S$. Tada za vektor $\vec x \in \K^N$ sa nosa\v{c}em na $S$ i za $\vec z \in \K^N$, $\vec z \neq \vec x$ takvi da $\vec{Az}=\vec{Ax}$, ozna\v{c}imo vektor $\vec v := \vec x - \vec z \in \ker \vec A \backslash \{\vec 0\}$. Imamo,
\begin{equation*}
    \norm{\vec x} \leq \norm{\vec x - \vec z_S}_1 + \norm{\vec z_S}_1 = \norm{\vec v_S}_1 + \norm{\vec z_S}_1 < \norm{\vec v_{\bar S}}_1 + \norm{\vec z_S}_1 = \norm{- \vec z_{\bar S}}_1 + \norm{\vec z_S}_1 = \norm{\vec z}_1
\end{equation*}
Dakle, vektor $\vec x$ je minimizator od \eqref{problem_minimizacije_l1}.
\end{proof}

\indent
Variranjem skupa $S$, sljede\'ci rezultat sljedi direktno iz prethodnog teorema. 

\begin{thm}\label{svojstvo_nul_prostora_tm}
    Za matricu $\vec A \in \K^{m \times N}$, svaki $s$-rijedak vektor $\vec x \in \K^N$ je jedinstveno rje\v{s}enje problema \eqref{problem_minimizacije_l1} uz $\vec y = \vec{Ax}$ ako i samo ako $\vec A$ zadovoljava svojstvo nul-prostora reda $s$.
\end{thm}

Primjetimo da prethodni teorem tvrdi da za svaki $\vec y = \vec{Ax}$, gdje je $\vec x$ $s$-rijedak, $\ell_1$-minimizacija \eqref{problem_minimizacije_l1} zapravo rje\v{s}ava problem $\ell_0$-minimizacije \eqref{problem_minimizacije} kada vrijedi svojstvo nul-prostora reda $s$. Zaista, pretpostavimo da se svaki $s$-rijedak vektor $\vec{x}$ mo\v{z}e rekonstruirati $\ell_1$-minimizacijom iz $\vec y = \vec{Ax}$. Neka je $\vec z$ minimizator $\ell_0$ problema \eqref{problem_minimizacije} sa $\vec y = \vec{Ax}$, tada je $\norm{\vec z}_0 \leq \norm{\vec x}_0$ pa je $\vec z$ tako\dj er $s$-rijedak. No, svaki $s$-rijedak vektor je jedinstveni $\ell_1$-minimizator, slijedi da je $\vec x = \vec z$.
\newline \indent
Za algoritam rekonstrukcije po\v{z}eljno je da zadr\v{z}i mogu\v{c}nost rekonstrukcije ako su neka od mjerenja reskaliraju, ispermutiraju ili dodaju nova. $\ell_1$-minimizacija ima takvo svojstvo. Formalno, gore opisane promijene zapravo predstavljaju zamjenu matrice $\vec A$ matricama $\vec{\hat A}$ i $\vec{\tilde A}$
\begin{align*}
    & \vec{\hat A} := \vec{GA}, \quad \text{gdje je }\vec{G}\text{ neka invertibilna }m \times m \text{ matrica},\\
    & \vec{\tilde A} := 
    \begin{bmatrix*}
        \vec A \\ \vec B
    \end{bmatrix*}
    , \quad \text{gdje je }\vec{B}\text{ neka }m' \times N \text{ matrica}.
\end{align*}
Primjetimo da je $\ker \vec{\hat A} = \ker \vec A$ i $\ker \tilde A \subset \ker \vec A$, pa svojstvo nul-prostora vrijedi i za matrice $\vec{\hat A}$ i $\vec{\tilde A}$.
\newline
\newline
\indent
Za kraj prou\v{c}iti \'cemo utjecaj polja $\K$. Razlika izme\dj u $\ker_{\R}\vec A$ i\\ $\ker_{\C} \vec A = \ker_{\R} \vec A + i \ker_{\R} \vec A$ vodi u slu\v{c}aju da je $\K=\R$ na realno svojstvo nul-prostora, 
\begin{equation}\label{svojstvo_nul_prostora_realno}
    \sum_{j \in S}|v_j| < \sum_{l \in \bar{S}}|v_l| \quad \text{za svaki } \vec v \in \ker_{\R} \vec A,\ \vec v \neq \vec 0,  
\end{equation}
a u slu\v{c}aju da je $\K = \C$, na kompleksno svojsto nul-prostora,
\begin{equation}\label{svojstvo_nul_prostora_kompleksno}
    \sum_{j in S}\sqrt{v_j^2 + w_j^2} < \sum_{l \in \bar{S}}\sqrt{v_j^2 + w_j^2}\quad \text{za svaki } \vec v, \vec w \in \ker_{\R} \vec A,\ \ ( \vec v, \vec w ) \neq \vec (\vec 0,\vec 0).  
\end{equation}

\noindent
Zapravo, pokazati \'cemo da su svojstva nul-prostora me\dj usobno ekvivalentna u realnom i kompleksnom slu\v{c}aju. Zato mo\v{z}emo re\'ci da realna matrica mjerenja egzaktno rekonstruira sve rijetke vektore $\ell_1$-minimizacijom. 
\begin{thm}
    Neka je $\vec A \in \R^{m \times N}$, tada je realno svojstvo nul-prostora \eqref{svojstvo_nul_prostora_realno} za skup $S$ ekvivalentno je kompleksnom svojstvu nul-prostora \eqref{svojstvo_nul_prostora_kompleksno} za isti skup $S$.
\end{thm}
\begin{proof}
    Primjetimo \eqref{svojstvo_nul_prostora_realno} slijedi direktno iz \eqref{svojstvo_nul_prostora_kompleksno} za $\vec w = \vec 0$. Uzmimo sada $\vec v, \vec w \in \ker_{\R}\vec A$, takvi da $(\vec v, \vec w) \neq (\vec 0, \vec 0)$. Ako su $\vec v$ i $\vec w$ linearno zavisni. tj. $\vec v = \alpha \vec w$ za neki $\alpha \in \R \backslash \{ \vec 0 \}$ onda je 
   \begin{align*}
       \sum_{j \in S} \sqrt{v_j^2+w_j^2} &=  \sum_{j \in S} \sqrt{(1+\alpha^2)w_j^2}=\sqrt{1+\alpha^2}\sum_{j \in S} \sqrt{w_j^2}\\
       &< \sqrt{1+\alpha^2}\sum_{j \in \bar S} \sqrt{w_j^2} = \sum_{j \in \bar S} \sqrt{(1+\alpha^2)w_j^2} = \sum_{j \in \bar S} \sqrt{v_j^2+w_j^2}
   \end{align*} 
   Pretpostavimo sada da su $\vec v$ i $\vec w$ linearno nezavisni i definirajmo $\vec u := \cos \theta \vec v + \cos \theta \vec v \in \ker_{\R} \vec A \backslash \{\vec 0\}$. Tada za svaki $\theta \in \R$,
   \begin{equation}\label{svojstvo_nul_prostora_r_c_nejed1}
       \sum_{j \in S} |\cos \theta v_j + \sin \theta w_j| < \sum_{l \in \bar S} |\cos \theta v_l + \sin \theta w_l|.
   \end{equation}
   Za svaki $k \in [N]$, neka je $\theta_k \in [-\pi, \pi]$ takav da
   \begin{equation*}
       v_k = \sqrt{v_k^2 + w_k^2}\cos{\theta_k}, \quad w_k = \sqrt{v_k^2 + w_k^2}\sin{\theta_k}
   \end{equation*}
   Iz \eqref{svojstvo_nul_prostora_r_c_nejed1} slijedi,
   \begin{equation*}
       \sum_{j \in S}\sqrt{v_j^2+w_j^2}|\cos(\theta - \theta_j)|<\sum_{l \in \bar S}\sqrt{v_l^2+w_l^2}|\cos(\theta - \theta_l)|
   \end{equation*}
   Integriranjem po $\theta \in [-\pi,\pi]$ dobijemo
   \begin{equation*}
       \sum_{j \in S}\sqrt{v_j^2+w_j^2}\int_{-\pi}^{\pi}  |\cos(\theta - \theta_j)| d \theta<\sum_{l \in \bar S}\sqrt{v_l^2+w_l^2}\int_{-\pi}^{\pi}  |\cos(\theta - \theta_l)|
   \end{equation*}
    No lako se provjeri da je
    \begin{equation*}
         \int_{-\pi}^{\pi}  |\cos(\theta - \theta_j)| d \theta = 4
    \end{equation*}
    tj. da je pozitivan i neovisan o $\theta' \in [-\pi, \pi]$.
\end{proof}

\subsection[Nekonveksna minimizacija][Nekonveksna minimizacija]{Nekonveksna minimizacija}
Prisjetimo se, $\ell_0$ norma vektora $\vec z \in \C^N$ aproksimirana je $q$-tom potencijom svoje $\ell_q$-kvazinorme,
\begin{equation*}
\|\vec{z}\|_p^p := \sum_{j=1}^N|x_j|^p \xrightarrow{p\rightarrow 0} \sum_{j=1}^N\mathbf{1}_{\{z_j \neq 0\}} = \|\vec{z}\|_0
\end{equation*}
To sugestira da $\ell_0$-minimizaciju \eqref{problem_minimizacije} zamjenimo sa
\begin{equation}
    \min_{\vec z \in \C^N} \norm{\vec z}_q \quad \text{uz uvjet }\vec{Az} = \vec{y}.\tag{$P_q$}
\end{equation}
Za $0 < q < 1$ taj je problem nekonveksan i $\mathfrak{NP}$-te\v{z}ak. No, \v{z}elimo teoretski potvrditi ideju da \eqref{problem_minimizacije_aprox} dobro aproksimira \eqref{problem_minimizacije} za male $q$.
Sljede\'ci teorem daje analogon svojstva nul-prostora za $0<q<1$. Dokaz je tako\dj er analogan dokazu teorema \ref{svojstvo_nul_prostora_tm} te se koristi \v{c}injenica da za $\ell_q$-kvazinorma zadovoljava nejednakost trokuta.  
\begin{thm}\label{svojstvo_nul_prostora_tm_2}
    Za matricu $\vec A \in \C^{m \times N}$ i $0<q<1$, svaki $s$-rijedak vektor $\vec x \in \C^N$ je jedinstveno rje\v{s}enje problema \eqref{problem_minimizacije_aprox} uz $\vec y = \vec{Ax}$ ako i samo ako 
    \begin{equation*}
        \norm{\vec v_S}_q < \norm{\vec v_{\bar{S}}}_q  \quad \text{za svaki }\vec v \in \ker \vec A \backslash \{\vec 0\}.
    \end{equation*}
\end{thm}
Sada mo\v{z}emo dokazivati da rekonstrukcija $\ell_q$-minimizacijom implicira rekonstrukciju $\ell_p$-minimizacijom za $o<p<q<1$.
\begin{thm}
    Za matricu $\vec A \in \C^{m \times N}$ i $0<p<q<1$, ako je svaki $s$-rijedak vektor $\vec x \in \C^N$ jedinstveno rje\v{s}enje problema \eqref{problem_minimizacije_aprox} uz $\vec y = \vec{Ax}$  onda je $\vec x$ tako\dj er i rje\v{s}enje problema $(P_p)$ za $\vec y = \vec{Ax}$.
\end{thm}
\begin{proof}
    Prema teoremu \ref{svojstvo_nul_prostora_tm_2} dovoljno je pokazati da vrijedi
    \begin{equation}\label{svojstvo_nul_prostora_tm_3_nejed}
        \sum_{j \in S} |v_j|^p < \sum_{l \in \bar S}|v_l|^p,
    \end{equation}
    ako je $\vec v \in \ker \vec A \backslash \{\vec 0\}$, $S$ skup indeksa od $s$ apsolutno najve\'cih komponeneti od $\vec v$ i ako ista nejednakost vrijedi za $q$.
    Dakle, pretpostavimo da \eqref{svojstvo_nul_prostora_tm_3_nejed} vrijedi za q. Tada je nu\v{z}no $\vec v_{\bar S} \neq \vec 0$ po\v{s}to je $S$ skup indeksa od $s$ apsolutno najve\'cih komponeneti ne-nul vektora $\vec v$. Stoga \eqref{svojstvo_nul_prostora_tm_3_nejed} mo\v{z}emo napisati u obliku
    \begin{equation}\label{svojstvo_nul_prostora_tm_3_nejed_2}
        \sum_{j \in S} \frac{1}{\sum_{l \in \bar S}(|v_l|/|v_j|)^p} < 1.  
    \end{equation}
    Primjetimo da $|v_l|/|v_j| \leq 1$ za $l \in \bar S$ i $j \in S$. Stoga je lijeva strana \eqref{svojstvo_nul_prostora_tm_3_nejed_2} nepadaju\'ca funkcija u varijabli $0<p \leq 1$. Pa stoga njena vrijednost u $p < q$ ne prelazi njezinu vrijednost u $q$, koji je manji od 1 po pretpostavci.
\end{proof}

\section[Stabilnost][Stabilnost]{Stabilnost}
Signali u praksi gotovo nikad nisu idealno rijetki. U najboljem slu\'caju blizu su rijetkim vektorima. Stoga, \v{z}elimo da metode sa\v{z}etog uzorkovanja rekonstruiraju vektor $\vec x \in \C^N$ sa gre\v{s}kom koja je kontrolirana udaljenosti vektora $\vec x$ do $s$-rijetkih vektora. Za algoritme koji imaju to svojsto ka\v{z}emo da su \textit{stabilni} s obzirom na defekte rijetkosti. Pokazati \'cemo da je $\ell_1$-minimizacija \eqref{problem_minimizacije_l1} stabilna pod ja\v{c}im svojstvom nul-prostora.

\begin{defn}
    Matrica $\vec A \in \C^{m \times N}$ zadovoljava \textit{svojstvo stabilnog nul-prostora} sa konstantom $0<\rho<1$ za skup $S \subset [N]$ ako
    \begin{equation*}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 \quad \text{za svaki }\vec v \in \ker \vec A.
    \end{equation*}
    Nadalje, ka\v{z}emo da $\vec A$ zadovoljava \textit{svojstvo stabilnog nul-prostora reda} $s$ sa konstantom $0<\rho<1$ ako zadovoljava zadovoljava gornju nejednakost za svaki $S \subset [N]$ takav da $\card(S)=s$.
\end{defn}

\begin{thm}\label{stabilnost_tm_1}
    Ako matrica $\vec A \in \C^{m \times N}$ zadovoljava svojstvo stabilnog nul-prostora reda $s$ sa konstantom $0<\rho<1$, tada za svaki $\vec x \in \C^N$, rje\v{s}enje $\vec x^{\sharp}$ problema \eqref{problem_minimizacije_l1} sa $\vec y = \vec{Ax}$ aproksimira vektor $\vec x$ s $\ell_1$-gre\v{s}kom
    \begin{equation}\label{stabilnost_tm_1_nejed}
        \norm{\vec x - \vec x ^{\sharp}} \leq \frac{2(1+\rho)}{(1-\rho)}\sigma_s(\vec x)_1.
    \end{equation}
\end{thm}
\noindent
Sada vi\v{s}e nemamo jedinstvenost $\ell_1$-minimizatora. Prethodni teorem biti \'ce direktna posljedica ja\v{c}e tvrdnje,
\begin{thm}\label{stabilnost_tm_2}
    Ako matrica $\vec A \in \C^{m \times N}$ zadovoljava svojstvo stabilnog nul-prostora sa konstantom $0<\rho<1$ za skup $S$ ako i samo ako
    \begin{equation}\label{stabilnost_tm_2_nejed}
        \norm{\vec z - \vec x}_1  \leq \frac{1+\rho}{1-\rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1) 
    \end{equation}
    za sve vektore $\vec x, \vec z \in \C^N$ za $\vec{Az} = \vec{Ax}$.
\end{thm}

Poka\v{z}imo kako teorem \ref{stabilnost_tm_1} slijedi iz \ref{stabilnost_tm_2}:
Neka je $S$ skup $s$ apsolutno najve\'cih komponeneti vekotora $\vec x$, tako da $\norm{\vec x_{\bar S}}=\sigma_s(\vec x)_1$. Ako je $\vec x^{\sharp}$ minimizator problema \eqref{problem_minimizacije_l1}, tada vrijedi $\norm{\vec x^{\sharp}}_1 \leq \norm{\vec x}_1$ i $\vec{Ax}^{\sharp}= \vec{Ax}$. Dakle, desnu strana \eqref{stabilnost_tm_2_nejed} za $\vec z = \vec x^{\sharp}$ mo\v{z}emo ocjeniti desnom stranom \eqref{stabilnost_tm_1_nejed}.\\
\indent
Prije dokaza teorema \ref{stabilnost_tm_2} poka\v{z}imo jo\v{s} jedan koristan rezultat.
\begin{lem}\label{stabilnost_lema_1}
    Za $S \subset [N]$ i vektore $\vec x, \vec z \in \C^N$ vrijedi,
    \begin{equation*}
        \norm{(\vec x - \vec z)_{\bar S}}_1 \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \norm{(\vec x - \vec z)_S}_1 + 2 \norm{\vec x_{\bar S}}_1
    \end{equation*}
\end{lem}
\begin{proof}
    Imamo,
    \begin{align*}
        \norm{\vec x}_1 &= \norm{\vec x_{\bar S}}_1 + \norm{\vec{x}_S}_1 \leq \norm{\vec x_{\bar S}}_1 + \norm{(\vec x - \vec z)_S}_1 + \norm{\vec z_{S}}_1\\
        \norm{(\vec x - \vec z)_{\bar S}}_1 &\leq \norm{\vec x_{\bar S}}_1 + \norm{\vec z_{\bar S}}_1.
    \end{align*}
    Sumiranjem ove dvije nejednakosti, slijedi
    \begin{equation*}
        \norm{\vec x}_1 + \norm{(\vec x - \vec z)_{\bar S}}_1 \leq 2 \norm{\vec x_{\bar S}}_1 +  \norm{(\vec x - \vec z)_S}_1 + \norm{\vec z}_1.
    \end{equation*}
\end{proof}

\begin{proof}[Dokaz (Teorem \ref{stabilnost_tm_2})]
    Pretpostavimo da matrica $\vec A$ zadovoljava \eqref{stabilnost_tm_2_nejed} za sve vektore $\vec x, \vec z \in \C^N$ uz $\vec{Az} = \vec{Ax}$. Za dani vektor $\vec{v} \in \ker \vec A$, po\v{s}to je $\vec{Av}_{\bar S} = \vec A(-\vec v_S)$ mo\v{z}emo primjeniti \eqref{stabilnost_tm_2_nejed} sa $\vec x = - \vec v_S$ i $\vec z = \vec v_{\bar S}$. Slijedi,
    \begin{equation*}
        \norm{\vec v}_1 \leq \frac{1+\rho}{1-\rho}(\norm{\vec v_{\bar S}}_1 - \norm{\vec v_S}_1). 
    \end{equation*}
    To mo\v{z}emo zapisati kao 
    \begin{equation*}
        (1-\rho)(\norm{\vec v_S}_1 + \norm{\vec v_{\bar S}}_1)  \leq  (1+\rho)(\norm{\vec v_{\bar S}}_1 + \norm{\vec v_S}_1).
    \end{equation*}
    Jednostavnom manipulacijom slijedi
    \begin{equation*}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1 
    \end{equation*}
    \indent
    Obratno, neka matrica $\vec A$ zadovoljava svojstvo stabilnog nul-prostora s konstantom $0<\rho<1$ za skup $S$. Neka su $\vec x, \vec z \in \C^N$ takvi da $\vec{Az} = \vec{Ax}$, po\v{s}to je $\vec v := \vec z - \vec x \in \ker \vec A$, svojstvo stabilnog nul-prostora daje
    \begin{equation}\label{stabilnost_tm_3_nejed_1}
        \norm{\vec v_S}_1 \leq \rho \norm{\vec v_{\bar S}}_1.
    \end{equation}
    Nadalje, iz lema \ref{stabilnost_lema_1} slijedi
    \begin{equation}\label{stabilnost_tm_3_nejed_2}
        \norm{\vec v_{\bar S}}_1  \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \norm{\vec v_S}_1 + 2 \norm{\vec x_{\bar S}}_1.
    \end{equation}
    Substituiramo \eqref{stabilnost_tm_3_nejed_1} u \eqref{stabilnost_tm_3_nejed_2},
    \begin{equation*}
        \norm{\vec v_{\bar S}}_1  \leq \norm{\vec z}_1 - \norm{\vec x}_1 + \rho\norm{\vec v_{\bar S}}_1 + 2 \norm{\vec x_{\bar S}}_1.
    \end{equation*}
    Po\v{s}to je $\rho < 1$,
    \begin{equation*}
        \norm{\vec v_{\bar S}}_1 \leq \frac{1}{1 - \rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2\norm{\vec x_{\bar S}}_1).  
    \end{equation*}
    Ponovno iskoristimo \eqref{stabilnost_tm_3_nejed_1},
    \begin{equation*}
        \norm{\vec v}_1 = \norm{\vec v_{\bar S}}_1 +  \norm{\vec v_{S}}_1  \leq (1 + \rho) \norm{\vec v_{\bar S}}_1 \leq \frac{1+\rho}{1-\rho}(\norm{\vec z}_1 - \norm{\vec x}_1 + 2 \norm{\vec x_{\bar S}}_1).
    \end{equation*}
\end{proof}




% KRAJ
% Na kraju diplomkog rada stavlja se  bibliografija
% Najprije definiramo nacin prikazivanja bibliografije, u ovom slucaju verzija amsplain stila
\bibliographystyle{babamspl} % babamspl ili babplain

% U datoteku diplomski.bib se stavljaju bibliografske reference
% Bibliografske reference u bib formatu se mogu dobiti iz MathSciNet baze, Google Scholara, ArXiva, ...
\bibliography{diplomski}

\pagestyle{empty} % ne zelimo brojanje sljedecih stranica

% I na koncu idu sazeci na hrvatskom i engleskom

\begin{sazetak}
\end{sazetak}

\begin{summary}
\end{summary}

% te zivotopis

\begin{cv}
\end{cv}

\end{document}
